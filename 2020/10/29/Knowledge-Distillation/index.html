<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Knowledge Distillation | Viva La Vida</title><meta name="keywords" content="知识蒸馏"><meta name="author" content="BROWALLIA"><meta name="copyright" content="BROWALLIA"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="知识蒸馏是一种模型压缩常见方法，用于模型压缩指的是在teacher-student框架中，将复杂、学习能力强的网络学到的特征表示“知识”蒸馏出来，传递给参数量小、学习能力弱的网络。蒸馏可以提供student在one-shot label上学不到的soft label信息，这些里面包含了类别间信息，以及student小网络学不到而teacher网络可以学到的特征表示‘知识’，所以一般可以提高s">
<meta property="og:type" content="article">
<meta property="og:title" content="Knowledge Distillation">
<meta property="og:url" content="https://www.browallia.top/2020/10/29/Knowledge-Distillation/index.html">
<meta property="og:site_name" content="Viva La Vida">
<meta property="og:description" content="知识蒸馏是一种模型压缩常见方法，用于模型压缩指的是在teacher-student框架中，将复杂、学习能力强的网络学到的特征表示“知识”蒸馏出来，传递给参数量小、学习能力弱的网络。蒸馏可以提供student在one-shot label上学不到的soft label信息，这些里面包含了类别间信息，以及student小网络学不到而teacher网络可以学到的特征表示‘知识’，所以一般可以提高s">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gitee.com/browallia/tuchuang/raw/master/img/KD.jpg">
<meta property="article:published_time" content="2020-10-29T07:16:52.000Z">
<meta property="article:modified_time" content="2020-12-13T16:57:12.575Z">
<meta property="article:author" content="BROWALLIA">
<meta property="article:tag" content="知识蒸馏">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/browallia/tuchuang/raw/master/img/KD.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://www.browallia.top/2020/10/29/Knowledge-Distillation/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin="crossorigin"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="content"/><meta name="baidu-site-verification" content="content"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6df37eb9bea6fc6acfb352324998b672";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-D99NED65DC"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-D99NED65DC');
</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.2.0',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-12-14 00:57:12'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {
  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }

  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Viva La Vida" type="application/atom+xml">
</head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/dbs.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">24</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">14</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> POI</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> Me</span></a></div></div></div></div><div id="body-wrap"><div id="web_bg"></div><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Distilling-the-Knowledge-in-a-Neural-Network"><span class="toc-number">1.</span> <span class="toc-text">Distilling the Knowledge in a Neural Network</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#FITNETS-HINTS-FOR-THIN-DEEP-NETS"><span class="toc-number">2.</span> <span class="toc-text">FITNETS: HINTS FOR THIN DEEP NETS</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Paying-More-Attention-to-Attention-Improving-the-Performance-of-Convolutional-Neural-Networks-via-Attention-Transfer"><span class="toc-number">3.</span> <span class="toc-text">Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#A-Gift-from-Knowledge-Distillation-Fast-Optimization-Network-Minimization-and-Transfer-Learning"><span class="toc-number">4.</span> <span class="toc-text">A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Paraphrasing-Complex-Network-Network-Compression-via-Factor-Transfer"><span class="toc-number">5.</span> <span class="toc-text">Paraphrasing Complex Network: Network Compression via Factor Transfer</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Learning-Deep-Representations-with-Probabilistic-Knowledge-Transfer"><span class="toc-number">6.</span> <span class="toc-text">Learning Deep Representations with Probabilistic Knowledge Transfer</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Knowledge-Transfer-via-Distillation-of-Activation-Boundaries-Formed-by-Hidden-Neurons"><span class="toc-number">7.</span> <span class="toc-text">Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Correlation-Congruence-for-Knowledge-Distillation"><span class="toc-number">8.</span> <span class="toc-text">Correlation Congruence for Knowledge Distillation</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#like-what-you-like-knowledge-distill-via-neuron-selectivity-transfer"><span class="toc-number">9.</span> <span class="toc-text">like what you like: knowledge distill via neuron selectivity transfer</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Relational-Knowledge-Disitllation"><span class="toc-number">10.</span> <span class="toc-text">Relational Knowledge Disitllation</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Similarity-Preserving-Knowledge-Distillation"><span class="toc-number">11.</span> <span class="toc-text">Similarity-Preserving Knowledge Distillation</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Variational-Information-Distillation-for-Knowledge-Transfer"><span class="toc-number">12.</span> <span class="toc-text">Variational Information Distillation for Knowledge Transfer</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">13.</span> <span class="toc-text">总结</span></a></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://gitee.com/browallia/tuchuang/raw/master/img/KD.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Viva La Vida</a></span><span id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> POI</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> Me</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">Knowledge Distillation</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-10-29T07:16:52.000Z" title="发表于 2020-10-29 15:16:52">2020-10-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2020-12-13T16:57:12.575Z" title="更新于 2020-12-14 00:57:12">2020-12-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>17分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><a id="more"></a>
<p> ﻿知识蒸馏是一种模型压缩常见方法，用于模型压缩指的是在teacher-student框架中，将复杂、学习能力强的网络学到的特征表示“知识”蒸馏出来，传递给参数量小、学习能力弱的网络。蒸馏可以提供student在one-shot label上学不到的soft label信息，这些里面包含了类别间信息，以及student小网络学不到而teacher网络可以学到的特征表示‘知识’，所以一般可以提高student网络的精度。 </p>
<p>本文对一些KD方法的benchmark的论文进行一些方法总结。代码分析主要来自<a target="_blank" rel="noopener" href="https://github.com/HobbitLong/RepDistiller">github</a>。</p>
<h1 id="Distilling-the-Knowledge-in-a-Neural-Network"><a href="#Distilling-the-Knowledge-in-a-Neural-Network" class="headerlink" title="Distilling the Knowledge in a Neural Network"></a><strong>Distilling the Knowledge in a Neural Network</strong></h1><p>NIPS 2014 [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1503.02531">https://arxiv.org/abs/1503.02531</a>]</p>
<p>​    这篇论文是知识蒸馏的开山之作，简单来说，KD就是将一个大模型(Teacher)或者多个模型的ensemble学习到的知识迁移到更轻量的模型(Student)上继续学习，从而提高student的性能。</p>
<p>​    本文提出的方法是用大模型的类别概率(logits)作为”soft target”对student模型进行训练。相比于hard target，soft target包含更多的信息熵并且训练中的梯度方差会小很多，所以student模型可以用更小的数据进行训练并且收敛速度会有提升。比如MNIST任务中，２看起来可能会像3/7会有信息包含在soft target中，但是经过softmax之后会接近于0，丢失一些信息。</p>
<p>​    作者使用logits(最后一个softmax之前的输入)作为student模型的学习目标，即将teacher的logits和student的logits平方差最小化。在文中作者作者提出了Softmax_T</p>
<script type="math/tex; mode=display">
q_i = \frac{exp(z_i/T)}{\sum_jexp(z_j/T)}</script><p>​    如果将T取1，这个公式就是softmax，根据logit输出各个类别的概率。如果T接近于0，则最大的值会越近1，其它值会接近0，近似于onehot编码。如果T越大，则输出的结果的分布越平缓，相当于平滑的一个作用，可以是概率分布更集中，起到保留相似信息的作用。如果T等于无穷，就是一个均匀分布。 </p>
<p>​    在蒸馏任务中，一般采用两个损失函数，第一个是和soft-target的cross-entropy，第二个是和正确label的cross-entropy，但一般第二个损失函数需要设置一个较低的权重。</p>
<p>​    经过简化的目标函数为</p>
<script type="math/tex; mode=display">
\frac{\delta C}{\delta z_i} = \frac{1}{NT^2}(z_i-v_i)</script><p>其中vi是大模型有温度的softmax输出，zi是小模型有温度的softmax输出。</p>
<p><strong>代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DistillKL</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Distilling the Knowledge in a Neural Network&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, T</span>):</span></span><br><span class="line">        super(DistillKL, self).__init__()</span><br><span class="line">        self.T = T</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, y_s, y_t</span>):</span></span><br><span class="line">        p_s = F.log_softmax(y_s/self.T, dim=<span class="number">1</span>)</span><br><span class="line">        p_t = F.softmax(y_t/self.T, dim=<span class="number">1</span>)</span><br><span class="line">        loss = F.kl_div(p_s, p_t, size_average=<span class="literal">False</span>) * (self.T**<span class="number">2</span>) / y_s.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h1 id="FITNETS-HINTS-FOR-THIN-DEEP-NETS"><a href="#FITNETS-HINTS-FOR-THIN-DEEP-NETS" class="headerlink" title="FITNETS: HINTS FOR THIN DEEP NETS"></a>FITNETS: HINTS FOR THIN DEEP NETS</h1><p>ICLR2015 [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1412.6550">https://arxiv.org/abs/1412.6550</a>]</p>
<p>​    本文指出之前的工作都是利用student或者一组模型的ensemble压缩为深度宽度相·当或者较浅和较宽的student网络，没有有效的利用深度。</p>
<p>​    文章提出了FitNets方法去压缩较宽和较深的网络，即student网络为thiner and deeper Network(比teacher更深)。主要方法是用teacher的中间隐藏层引导student网络进行学习，期望student网络能够学习一个中间的表现形式能够对teacher网络的中间层进行预测。</p>
<p>​    <strong>实现</strong></p>
<p>​    选取teacher网络中间的隐藏层作为hint layer，同样选取student网络中间的隐藏层作为guide layer向hintt layer学习。</p>
<script type="math/tex; mode=display">
L_{HT}(W_{Guided}, W_r) = 1/2||u_h(X;W_{Hint}-r(v_g(X;W_{Guided});Wr)||^2</script><p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201030171516.png" alt></p>
<p>​    <strong>训练策略</strong></p>
<p>​    teacher网络的前 $h$层作为$W_{Hint}$ ，student网络的前$g$层作为 $W_{Guided}$,$W_{Hint}$指导$W_{Guided}$训练，然后初始化$W_r$适配层的参数。</p>
<p>​    第一阶段用teacher网络的hint层预训练student网络的guided层</p>
<p>​    第二阶段是对整个网络做知识蒸馏</p>
<p>​    代码实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HintLoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Fitnets: hints for thin deep nets, ICLR 2015&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(HintLoss, self).__init__()</span><br><span class="line">        self.crit = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, f_s, f_t</span>):</span></span><br><span class="line">        loss = self.crit(f_s, f_t)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h1 id="Paying-More-Attention-to-Attention-Improving-the-Performance-of-Convolutional-Neural-Networks-via-Attention-Transfer"><a href="#Paying-More-Attention-to-Attention-Improving-the-Performance-of-Convolutional-Neural-Networks-via-Attention-Transfer" class="headerlink" title="Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer"></a>Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer</h1><p>ICLR 2017[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1612.03928">https://arxiv.org/abs/1612.03928</a>]</p>
<p>Attention Transfer传递了teacher网络的attention信息给student网络，CNN的attention一般分为两种，spatial-attention,channel-attention。本文利用的是spatial-attention.所谓spatial-attention即一种热力图，用来解码出输入图像空间区域对输出贡献大小。文章提出了两种可利用的spatial-attention,基于响应图的和基于梯度图的。</p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201102155032.png" alt></p>
<p><strong>activation-based AT</strong></p>
<p>基于特征图取出某层输出特征图张量A，定义一个映射F:</p>
<script type="math/tex; mode=display">
F:R^{CXHXW->R^{HXW}}</script><p>文中提出了三种映射方式：</p>
<ol>
<li>各通道绝对值相加$F_{sum}(A) = \sum^C_{i=1}|A_i|$</li>
<li>各通道绝对值p次幂相加$F_{sum}^p(A)  =\sum^C_{i=1}|A_i|^p$</li>
<li>各通道绝对值p次幂最大值$F_{max}^p(A)=max_{i=1,C}|A_i|^p$</li>
</ol>
<p>不同level的attention map的activation在不同的位置，由浅到深从眼睛到眼睛鼻子再到全脸。</p>
<p>​    <strong>网络的架构</strong></p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201102165737.png" alt></p>
<script type="math/tex; mode=display">
L_{AT} = L(W_S, x) + \frac{\beta}{2}\sum_{j\epsilon x}||\frac{Q_S^j}{||Q_S^j||_2}-\frac{Q_T^j}{||Q_T^j||_2}||_p</script><p><strong>gradient-based AT</strong></p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201102170624.png" alt></p>
<p><strong>代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks</span></span><br><span class="line"><span class="string">    via Attention Transfer</span></span><br><span class="line"><span class="string">    code: https://github.com/szagoruyko/attention-transfer&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, p=<span class="number">2</span></span>):</span></span><br><span class="line">        super(Attention, self).__init__()</span><br><span class="line">        self.p = p</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, g_s, g_t</span>):</span></span><br><span class="line">        <span class="keyword">return</span> [self.at_loss(f_s, f_t) <span class="keyword">for</span> f_s, f_t <span class="keyword">in</span> zip(g_s, g_t)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">at_loss</span>(<span class="params">self, f_s, f_t</span>):</span></span><br><span class="line">        s_H, t_H = f_s.shape[<span class="number">2</span>], f_t.shape[<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> s_H &gt; t_H:</span><br><span class="line">            f_s = F.adaptive_avg_pool2d(f_s, (t_H, t_H))</span><br><span class="line">        <span class="keyword">elif</span> s_H &lt; t_H:</span><br><span class="line">            f_t = F.adaptive_avg_pool2d(f_t, (s_H, s_H))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">return</span> (self.at(f_s) - self.at(f_t)).pow(<span class="number">2</span>).mean()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">at</span>(<span class="params">self, f</span>):</span></span><br><span class="line">        <span class="keyword">return</span> F.normalize(f.pow(self.p).mean(<span class="number">1</span>).view(f.size(<span class="number">0</span>), <span class="number">-1</span>))</span><br></pre></td></tr></table></figure>
<h1 id="A-Gift-from-Knowledge-Distillation-Fast-Optimization-Network-Minimization-and-Transfer-Learning"><a href="#A-Gift-from-Knowledge-Distillation-Fast-Optimization-Network-Minimization-and-Transfer-Learning" class="headerlink" title="A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning"></a>A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning</h1><p>CVPR 2017[<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf">https://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf</a>]</p>
<p>这篇文章的蒸馏方法不是通过Teacher直接输出的feature map作为student网络的训练目标，而是将teacher网络层间的关系作为student网络学习的target。</p>
<p><strong>FSP Metrix</strong></p>
<p>文章提出的描述层与层之间关系的方法FSP matrix，是某层特征图与另外一层特征图之间的偏心协方差矩阵（即没有减去均值的协方差矩阵）。如F1层特征图配置（H,W,M）M为通道数。F2层特征图配置（H,W,N）。得到一个M * N的矩阵G。G（i，j）为F1第i通道与F2第j通道的elemet-wise乘积之和：</p>
<script type="math/tex; mode=display">
G_{i,j}(x;W) = \sum^h_{s=1}\sum^w_{t=1}\frac{F^1_{s,t,i}(x;W)*F^2_{s,t,j}(x;W)}{h*w}</script><p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201102174536.png" alt></p>
<p>网络结构</p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201102193717.png" alt></p>
<p>最后的loss</p>
<script type="math/tex; mode=display">
L_{FSP}(W_t,W_s) = \frac{1}{N}\sum_x\sum^n_{i=1}\lambda_i*||G_i^T(x;W_t)-G_i^S(x;W_a)||_2^2</script><p><strong>代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FSP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;A Gift from Knowledge Distillation:</span></span><br><span class="line"><span class="string">    Fast Optimization, Network Minimization and Transfer Learning&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, s_shapes, t_shapes</span>):</span></span><br><span class="line">        super(FSP, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> len(s_shapes) == len(t_shapes), <span class="string">&#x27;unequal length of feat list&#x27;</span></span><br><span class="line">        s_c = [s[<span class="number">1</span>] <span class="keyword">for</span> s <span class="keyword">in</span> s_shapes]</span><br><span class="line">        t_c = [t[<span class="number">1</span>] <span class="keyword">for</span> t <span class="keyword">in</span> t_shapes]</span><br><span class="line">        <span class="keyword">if</span> np.any(np.asarray(s_c) != np.asarray(t_c)):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;num of channels not equal (error in FSP)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, g_s, g_t</span>):</span></span><br><span class="line">        s_fsp = self.compute_fsp(g_s)</span><br><span class="line">        t_fsp = self.compute_fsp(g_t)</span><br><span class="line">        loss_group = [self.compute_loss(s, t) <span class="keyword">for</span> s, t <span class="keyword">in</span> zip(s_fsp, t_fsp)]</span><br><span class="line">        <span class="keyword">return</span> loss_group</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span>(<span class="params">s, t</span>):</span></span><br><span class="line">        <span class="keyword">return</span> (s - t).pow(<span class="number">2</span>).mean()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_fsp</span>(<span class="params">g</span>):</span></span><br><span class="line">        fsp_list = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(g) - <span class="number">1</span>):</span><br><span class="line">            bot, top = g[i], g[i + <span class="number">1</span>]</span><br><span class="line">            b_H, t_H = bot.shape[<span class="number">2</span>], top.shape[<span class="number">2</span>]</span><br><span class="line">            <span class="keyword">if</span> b_H &gt; t_H:</span><br><span class="line">                bot = F.adaptive_avg_pool2d(bot, (t_H, t_H))</span><br><span class="line">            <span class="keyword">elif</span> b_H &lt; t_H:</span><br><span class="line">                top = F.adaptive_avg_pool2d(top, (b_H, b_H))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            bot = bot.unsqueeze(<span class="number">1</span>)</span><br><span class="line">            top = top.unsqueeze(<span class="number">2</span>)</span><br><span class="line">            bot = bot.view(bot.shape[<span class="number">0</span>], bot.shape[<span class="number">1</span>], bot.shape[<span class="number">2</span>], <span class="number">-1</span>)</span><br><span class="line">            top = top.view(top.shape[<span class="number">0</span>], top.shape[<span class="number">1</span>], top.shape[<span class="number">2</span>], <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">            fsp = (bot * top).mean(<span class="number">-1</span>)</span><br><span class="line">            fsp_list.append(fsp)</span><br><span class="line">        <span class="keyword">return</span> fsp_list</span><br></pre></td></tr></table></figure>
<h1 id="Paraphrasing-Complex-Network-Network-Compression-via-Factor-Transfer"><a href="#Paraphrasing-Complex-Network-Network-Compression-via-Factor-Transfer" class="headerlink" title="Paraphrasing Complex Network: Network Compression via Factor Transfer"></a>Paraphrasing Complex Network: Network Compression via Factor Transfer</h1><p>NIPS 2018[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.04977">https://arxiv.org/abs/1802.04977</a>]</p>
<p>这篇文章提出了两个卷积模块，释义器(paraphrarer)和翻译器(traslator)。释义器由无监督方法训练，用于提取Teacher网络中的因子作为Teacher网络中的信息，翻译器用于提取student网络中的因子并且对Teacher中的因子进行翻译学习。</p>
<p><strong>网络架构</strong></p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201102212901.png" alt></p>
<p>文章提出，例如KD，AT等蒸馏方法虽然可以有很好的知识传递效果，但是忽略了Teacher网络和Student网络中的比如网络架构，通道数量，初始化条件等因素。在文章中，提出的架构拥有一个释义器将Teacher 的特征进行解释，可以让Student更好的学习知识。总结如下：</p>
<ul>
<li>用无监督的方式训练一个teacher网络的释义器。</li>
<li>student通过卷积化的翻译器对teacher释义的因子进行学习</li>
</ul>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201102215412.png" alt></p>
<p><strong>Teacher无监督paraphraser</strong></p>
<script type="math/tex; mode=display">
Lrec=‖x−P(x)‖2</script><p>用输入的特征图与释义器输出的释义信息P(x)进行损失计算用于无监督的训练paraphraser。如果网络输出的为m通道特征，则将释义器释义的特征通道设置为mxk，k为释义系数。</p>
<p><strong>Student translator</strong> — Factor Transfer</p>
<p>translator作为一个“缓冲区”来学习Teacher提供的Factor信息。</p>
<p>具体的Loss如下</p>
<script type="math/tex; mode=display">
Lstudent=Lcls+βLF T\\
Lcls=C(S(Ix),y)\\
LF T=‖\frac{FT}{‖FT‖^2}−\frac{FS}{‖FS‖^2}‖_p</script><p><strong>代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FactorTransfer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Paraphrasing Complex Network: Network Compression via Factor Transfer, NeurIPS 2018&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, p1=<span class="number">2</span>, p2=<span class="number">1</span></span>):</span></span><br><span class="line">        super(FactorTransfer, self).__init__()</span><br><span class="line">        self.p1 = p1</span><br><span class="line">        self.p2 = p2</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, f_s, f_t</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.factor_loss(f_s, f_t)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">factor_loss</span>(<span class="params">self, f_s, f_t</span>):</span></span><br><span class="line">        s_H, t_H = f_s.shape[<span class="number">2</span>], f_t.shape[<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> s_H &gt; t_H:</span><br><span class="line">            f_s = F.adaptive_avg_pool2d(f_s, (t_H, t_H))</span><br><span class="line">        <span class="keyword">elif</span> s_H &lt; t_H:</span><br><span class="line">            f_t = F.adaptive_avg_pool2d(f_t, (s_H, s_H))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">if</span> self.p2 == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> (self.factor(f_s) - self.factor(f_t)).abs().mean()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (self.factor(f_s) - self.factor(f_t)).pow(self.p2).mean()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">factor</span>(<span class="params">self, f</span>):</span></span><br><span class="line">        <span class="keyword">return</span> F.normalize(f.pow(self.p1).mean(<span class="number">1</span>).view(f.size(<span class="number">0</span>), <span class="number">-1</span>))</span><br></pre></td></tr></table></figure>
<h1 id="Learning-Deep-Representations-with-Probabilistic-Knowledge-Transfer"><a href="#Learning-Deep-Representations-with-Probabilistic-Knowledge-Transfer" class="headerlink" title="Learning Deep Representations with Probabilistic Knowledge Transfer"></a>Learning Deep Representations with Probabilistic Knowledge Transfer</h1><p>ECCV 2018[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.10837">https://arxiv.org/abs/1803.10837</a>]</p>
<p>这篇文章提出了一种KT方法，通过匹配特征在空间中的概率分布来进行知识迁移，而不是使用实际的特征表示。</p>
<p>由于一般的KT方法都是用classification的layer来进行知识迁移，输出的特征图无论网络的架构大小size都是固定的，所以普通的KT不适用于一些其他的任务。还有一点是一般的KT网络都忽略了样本之间的相似性，特征的空间几何形状等信息。</p>
<p>本文提出的概率知识转移方法：</p>
<p>TODO</p>
<h1 id="Knowledge-Transfer-via-Distillation-of-Activation-Boundaries-Formed-by-Hidden-Neurons"><a href="#Knowledge-Transfer-via-Distillation-of-Activation-Boundaries-Formed-by-Hidden-Neurons" class="headerlink" title="Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons"></a>Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons</h1><p>AAAI 2019[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.03233">https://arxiv.org/abs/1811.03233</a>]</p>
<p>文章提出了一种通过隐藏神经元形成的激活边界蒸馏的知识转移方法。student网络学习teacher网络神经元的激活区域和非激活区域的分离边界。运用激活转移损失来最小化Teacher和Student网络之间的神经元激活差异，这个损失考虑的不是神经元的反应强度以及数值特征，而是考虑神经元是否被激活。但是激活函数不可微所以文中提出了替代损失和激活转移损失近似来训练。</p>
<p>一般的方法是对RELU之后的特征图进行迁移学习,只能反映激活的程度,而这篇文章提出的方法是考虑神经元是否激活,如下图所示.</p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201104161619.png" alt></p>
<p>MSE based</p>
<script type="math/tex; mode=display">
L(I) = ||\sigma(T(I))-\sigma(S(I))||^2_2</script><p>Proposed</p>
<script type="math/tex; mode=display">
consider \ whether \ neuron \ is \ activated\\
p(x) = \begin{cases}1, \ if \ x>0 \\ 0, \ other wise\end{cases}\\
loss\\
L(I) = ||p(T(I))-p(S(I))||_1</script><p>但是上面公式是不可微的,所以提出一种近似的可微形式</p>
<script type="math/tex; mode=display">
L(I) =‖ρ(T(I))σ(μ1−r(S(I)))+ (1−ρ(T(I)))σ(μ1+r(S(I)))‖^2_2</script><p>r代表将student的向量转换为teacher的大小.</p>
<p>对于卷积层</p>
<script type="math/tex; mode=display">
L(I) =\sum^H_i\sum^W_j‖ρ(T(I)ij)σ(μ1−r(S(I)ij))+ (1−ρ(T(I)ij))σ(μ1+r(S(I)ij))‖^2_2</script><p>将HXW特征图每个点(1x1xM)作为计算的vector</p>
<p><strong>代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ABLoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons</span></span><br><span class="line"><span class="string">    code: https://github.com/bhheo/AB_distillation</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, feat_num, margin=<span class="number">1.0</span></span>):</span></span><br><span class="line">        super(ABLoss, self).__init__()</span><br><span class="line">        self.w = [<span class="number">2</span>**(i-feat_num+<span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(feat_num)]</span><br><span class="line">        self.margin = margin</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, g_s, g_t</span>):</span></span><br><span class="line">        bsz = g_s[<span class="number">0</span>].shape[<span class="number">0</span>]</span><br><span class="line">        losses = [self.criterion_alternative_l2(s, t) <span class="keyword">for</span> s, t <span class="keyword">in</span> zip(g_s, g_t)]</span><br><span class="line">        losses = [w * l <span class="keyword">for</span> w, l <span class="keyword">in</span> zip(self.w, losses)]</span><br><span class="line">        <span class="comment"># loss = sum(losses) / bsz</span></span><br><span class="line">        <span class="comment"># loss = loss / 1000 * 3</span></span><br><span class="line">        losses = [l / bsz <span class="keyword">for</span> l <span class="keyword">in</span> losses]</span><br><span class="line">        losses = [l / <span class="number">1000</span> * <span class="number">3</span> <span class="keyword">for</span> l <span class="keyword">in</span> losses]</span><br><span class="line">        <span class="keyword">return</span> losses</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">criterion_alternative_l2</span>(<span class="params">self, source, target</span>):</span></span><br><span class="line">        loss = ((source + self.margin) ** <span class="number">2</span> * ((source &gt; -self.margin) &amp; (target &lt;= <span class="number">0</span>)).float() +</span><br><span class="line">                (source - self.margin) ** <span class="number">2</span> * ((source &lt;= self.margin) &amp; (target &gt; <span class="number">0</span>)).float())</span><br><span class="line">        <span class="keyword">return</span> torch.abs(loss).sum()</span><br></pre></td></tr></table></figure>
<h1 id="Correlation-Congruence-for-Knowledge-Distillation"><a href="#Correlation-Congruence-for-Knowledge-Distillation" class="headerlink" title="Correlation Congruence for Knowledge Distillation"></a>Correlation Congruence for Knowledge Distillation</h1><p>ICCV 2019[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.01802">https://arxiv.org/abs/1904.01802</a>]</p>
<p>文章提出了一种方法不仅可以在distillation中进行实例级信息传递，并且可以对实例之间的相关性进行传递。提出了一种基于内核的方法以及采样策略能够在一个mini-batch中找到实例的一致性。</p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201105141616.png" alt></p>
<p><strong>具体结构</strong></p>
<ol>
<li>实例一致性(student和teacher预测的KL散度)</li>
<li>相关一致性(student和teacher相关性的欧式距离)</li>
</ol>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201105153251.png" alt></p>
<script type="math/tex; mode=display">
特征F_t \ F_s
\\F_t = matrix(f_1^t,f_2^t,...,f_n^t)
\\F_s = matrix(f_1^s,f_2^s,...,f_n^s)
\\C代表相关性矩阵, \phi代表相关性运算
\\C_{ij} = \phi(f_i,f_j)
\\correlation congruence \ LOSS
\\L_{CC} = \frac{1}{n^2}||\psi(F_t)-\psi(F_s)||_2^2
\\=\frac{1}{n^2}\sum_{i,j}(\phi(f_i^s,f_j^s)-\phi(f_i^t,f_j^t))^2
\\总Loss
\\L_{KKCD} = \alpha L_{CE} + (1-\alpha)L_{KD}+\beta L_{CC}</script><ul>
<li><p>kernel-based相关性计算</p>
</li>
<li><p>Mini-batch采样策略</p>
</li>
</ul>
<p>TODO</p>
<p><strong>代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CC</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">	<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">	Correlation Congruence for Knowledge Distillation</span></span><br><span class="line"><span class="string">	http://openaccess.thecvf.com/content_ICCV_2019/papers/</span></span><br><span class="line"><span class="string">	Peng_Correlation_Congruence_for_Knowledge_Distillation_ICCV_2019_paper.pdf</span></span><br><span class="line"><span class="string">	&#x27;&#x27;&#x27;</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, gamma, P_order</span>):</span></span><br><span class="line">		super(CC, self).__init__()</span><br><span class="line">		self.gamma = gamma</span><br><span class="line">		self.P_order = P_order</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, feat_s, feat_t</span>):</span></span><br><span class="line">		corr_mat_s = self.get_correlation_matrix(feat_s)</span><br><span class="line">		corr_mat_t = self.get_correlation_matrix(feat_t)</span><br><span class="line"></span><br><span class="line">		loss = F.mse_loss(corr_mat_s, corr_mat_t)</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">get_correlation_matrix</span>(<span class="params">self, feat</span>):</span></span><br><span class="line">		feat = F.normalize(feat, p=<span class="number">2</span>, dim=<span class="number">-1</span>)</span><br><span class="line">		sim_mat  = torch.matmul(feat, feat.t())</span><br><span class="line">		corr_mat = torch.zeros_like(sim_mat)</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> p <span class="keyword">in</span> range(self.P_order+<span class="number">1</span>):</span><br><span class="line">			corr_mat += math.exp(<span class="number">-2</span>*self.gamma) * (<span class="number">2</span>*self.gamma)**p / \</span><br><span class="line">						math.factorial(p) * torch.pow(sim_mat, p)</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> corr_mat</span><br></pre></td></tr></table></figure>
<h1 id="like-what-you-like-knowledge-distill-via-neuron-selectivity-transfer"><a href="#like-what-you-like-knowledge-distill-via-neuron-selectivity-transfer" class="headerlink" title="like what you like: knowledge distill via neuron selectivity transfer"></a>like what you like: knowledge distill via neuron selectivity transfer</h1><p>ICLR 2019[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.01219">https://arxiv.org/abs/1707.01219</a>]</p>
<p>这篇文章提出了一种新的KT方法,通过匹配Teacher和student网络中的神经元选择模式的分布来进行知识迁移.</p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201105160610.png" alt></p>
<p><strong>Maximum Mean Discrepancy</strong></p>
<script type="math/tex; mode=display">
L_{NST}(W_S) = H(y_{true}, P_S)+\frac{\lambda}{2}L_{MMD^2}(F_T,F_S)</script><p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201105193834.png" alt></p>
<p><strong>Choice of Kernels</strong></p>
<ul>
<li>Linear Kernel: $k(x,y) = x^Ty$</li>
<li>Polynomial Kernel:$k(x,y) = (x^Ty+c)^d$    d=2 ,c=0</li>
<li>Gaussian Kernel: $k(x,y) = exp(-\frac{||x-y||^2_2}{2\sigma^2})  \ \sigma^2是平方距离$  </li>
</ul>
<p>TODO</p>
<p><strong>代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NSTLoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;like what you like: knowledge distill via neuron selectivity transfer&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(NSTLoss, self).__init__()</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, g_s, g_t</span>):</span></span><br><span class="line">        <span class="keyword">return</span> [self.nst_loss(f_s, f_t) <span class="keyword">for</span> f_s, f_t <span class="keyword">in</span> zip(g_s, g_t)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">nst_loss</span>(<span class="params">self, f_s, f_t</span>):</span></span><br><span class="line">        s_H, t_H = f_s.shape[<span class="number">2</span>], f_t.shape[<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> s_H &gt; t_H:</span><br><span class="line">            f_s = F.adaptive_avg_pool2d(f_s, (t_H, t_H))</span><br><span class="line">        <span class="keyword">elif</span> s_H &lt; t_H:</span><br><span class="line">            f_t = F.adaptive_avg_pool2d(f_t, (s_H, s_H))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">        f_s = f_s.view(f_s.shape[<span class="number">0</span>], f_s.shape[<span class="number">1</span>], <span class="number">-1</span>)</span><br><span class="line">        f_s = F.normalize(f_s, dim=<span class="number">2</span>)</span><br><span class="line">        f_t = f_t.view(f_t.shape[<span class="number">0</span>], f_t.shape[<span class="number">1</span>], <span class="number">-1</span>)</span><br><span class="line">        f_t = F.normalize(f_t, dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># set full_loss as False to avoid unnecessary computation</span></span><br><span class="line">        full_loss = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> full_loss:</span><br><span class="line">            <span class="keyword">return</span> (self.poly_kernel(f_t, f_t).mean().detach() + self.poly_kernel(f_s, f_s).mean()</span><br><span class="line">                    - <span class="number">2</span> * self.poly_kernel(f_s, f_t).mean())<span class="comment">#detach在当前计算图中剥离variable并不求其梯度</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.poly_kernel(f_s, f_s).mean() - <span class="number">2</span> * self.poly_kernel(f_s, f_t).mean()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">poly_kernel</span>(<span class="params">self, a, b</span>):</span></span><br><span class="line">        a = a.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        b = b.unsqueeze(<span class="number">2</span>)</span><br><span class="line">        res = (a * b).sum(<span class="number">-1</span>).pow(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h1 id="Relational-Knowledge-Disitllation"><a href="#Relational-Knowledge-Disitllation" class="headerlink" title="Relational Knowledge Disitllation"></a>Relational Knowledge Disitllation</h1><p>CVPR 2019[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.05068">https://arxiv.org/abs/1904.05068</a>]</p>
<p>文章提出了一种基于关系的知识蒸馏方法.作者认为知识的表达用结构关系比单独的实例来表达更好.<br><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201106151948.png" style="zoom:80%;"></p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201105213654.png" alt></p>
<p>文章提出了两种关系知识蒸馏方法,基于距离和基于角度的方法.</p>
<script type="math/tex; mode=display">
L_{RKD} = \sum_{(x_1,...,x_n)\in \chi^N}l(\psi(t_1,..,t_n),\psi(s_1,...,s_n))</script><p>即用特征的n元组的关系进行知识迁移.ψ是关系势函数.不管teacher网络的尺寸和student是否匹配,都可以通过关系势函数进行高阶的知识迁移.</p>
<p><strong>Distance-wise</strong>(二元组)</p>
<script type="math/tex; mode=display">
\psi_D(t_i,t_j) = \frac{1}{\mu}||t_i-t_j||_2
\\μ是基于距离的归一化因子
\\\mu = \frac{1}{|\chi^2|}\sum_{(x_i,x_j)\in\chi^2}||t_i-t_j||_2
\\L_{RKD-D}=\sum_{(xi,xj)∈\chi^2}l_δ(ψ_D(t_i,t_j),ψ_D(s_i,s_j))
\\l_δ=\begin{cases}\frac{1}{2}(x-y)^2, \ \ for|x-y|<=1\\|x-y|-\frac{1}{2}, \ otherwise\end{cases}</script><p>这种方法不会强制student网络去匹配teacher的logits,而是匹配teacher的距离结构.</p>
<p><strong>Angle-wise</strong>(三元组)</p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201105220703.png" alt></p>
<p><strong>代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RKDLoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Relational Knowledge Disitllation, CVPR2019&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, w_d=<span class="number">25</span>, w_a=<span class="number">50</span></span>):</span></span><br><span class="line">        super(RKDLoss, self).__init__()</span><br><span class="line">        self.w_d = w_d</span><br><span class="line">        self.w_a = w_a</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, f_s, f_t</span>):</span></span><br><span class="line">        student = f_s.view(f_s.shape[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">        teacher = f_t.view(f_t.shape[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># RKD distance loss</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            t_d = self.pdist(teacher, squared=<span class="literal">False</span>)</span><br><span class="line">            mean_td = t_d[t_d &gt; <span class="number">0</span>].mean()</span><br><span class="line">            t_d = t_d / mean_td</span><br><span class="line"></span><br><span class="line">        d = self.pdist(student, squared=<span class="literal">False</span>)</span><br><span class="line">        mean_d = d[d &gt; <span class="number">0</span>].mean()</span><br><span class="line">        d = d / mean_d</span><br><span class="line"></span><br><span class="line">        loss_d = F.smooth_l1_loss(d, t_d)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># RKD Angle loss</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            td = (teacher.unsqueeze(<span class="number">0</span>) - teacher.unsqueeze(<span class="number">1</span>))</span><br><span class="line">            norm_td = F.normalize(td, p=<span class="number">2</span>, dim=<span class="number">2</span>)</span><br><span class="line">            t_angle = torch.bmm(norm_td, norm_td.transpose(<span class="number">1</span>, <span class="number">2</span>)).view(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        sd = (student.unsqueeze(<span class="number">0</span>) - student.unsqueeze(<span class="number">1</span>))</span><br><span class="line">        norm_sd = F.normalize(sd, p=<span class="number">2</span>, dim=<span class="number">2</span>)</span><br><span class="line">        s_angle = torch.bmm(norm_sd, norm_sd.transpose(<span class="number">1</span>, <span class="number">2</span>)).view(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        loss_a = F.smooth_l1_loss(s_angle, t_angle)</span><br><span class="line"></span><br><span class="line">        loss = self.w_d * loss_d + self.w_a * loss_a</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pdist</span>(<span class="params">e, squared=False, eps=<span class="number">1e-12</span></span>):</span></span><br><span class="line">        e_square = e.pow(<span class="number">2</span>).sum(dim=<span class="number">1</span>)</span><br><span class="line">        prod = e @ e.t()</span><br><span class="line">        res = (e_square.unsqueeze(<span class="number">1</span>) + e_square.unsqueeze(<span class="number">0</span>) - <span class="number">2</span> * prod).clamp(min=eps)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> squared:</span><br><span class="line">            res = res.sqrt()</span><br><span class="line"></span><br><span class="line">        res = res.clone()</span><br><span class="line">        res[range(len(e)), range(len(e))] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h1 id="Similarity-Preserving-Knowledge-Distillation"><a href="#Similarity-Preserving-Knowledge-Distillation" class="headerlink" title="Similarity-Preserving Knowledge Distillation"></a>Similarity-Preserving Knowledge Distillation</h1><p>ICCV 2019[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.09682">https://arxiv.org/abs/1907.09682</a>]</p>
<p>文章提出了一种新的蒸馏损失,利用输入的相似性会有相似的激活模式,student网络不用模仿teacher的表征空间,在自己的特征空间中保持相对性.</p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201105235509.png" alt></p>
<p><img src="/2020/10/29/Knowledge-Distillation/Users\58341\AppData\Roaming\Typora\typora-user-images\image-20201106002236910.png" alt="image-20201106002236910"></p>
<p>QT是特征的reshape,大小为(b,chw),所以G大小为(b,b)</p>
<p>损失函数为</p>
<script type="math/tex; mode=display">
L_{SP}(G_T,G_S) =\frac{1}{b^2}\sum_{(l,l′)∈I}||G(l)_T−G(l′)_S||^2_F</script><p>l,l’是相同depth的层的特征或者block输出层的特征.</p>
<p><strong>代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Similarity</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Similarity-Preserving Knowledge Distillation, ICCV2019, verified by original author&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(Similarity, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, g_s, g_t</span>):</span></span><br><span class="line">        <span class="keyword">return</span> [self.similarity_loss(f_s, f_t) <span class="keyword">for</span> f_s, f_t <span class="keyword">in</span> zip(g_s, g_t)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">similarity_loss</span>(<span class="params">self, f_s, f_t</span>):</span></span><br><span class="line">        bsz = f_s.shape[<span class="number">0</span>]</span><br><span class="line">        f_s = f_s.view(bsz, <span class="number">-1</span>)</span><br><span class="line">        f_t = f_t.view(bsz, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        G_s = torch.mm(f_s, torch.t(f_s))</span><br><span class="line">        <span class="comment"># G_s = G_s / G_s.norm(2)</span></span><br><span class="line">        G_s = torch.nn.functional.normalize(G_s)</span><br><span class="line">        G_t = torch.mm(f_t, torch.t(f_t))</span><br><span class="line">        <span class="comment"># G_t = G_t / G_t.norm(2)</span></span><br><span class="line">        G_t = torch.nn.functional.normalize(G_t)</span><br><span class="line"></span><br><span class="line">        G_diff = G_t - G_s</span><br><span class="line">        loss = (G_diff * G_diff).view(<span class="number">-1</span>, <span class="number">1</span>).sum(<span class="number">0</span>) / (bsz * bsz)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h1 id="Variational-Information-Distillation-for-Knowledge-Transfer"><a href="#Variational-Information-Distillation-for-Knowledge-Transfer" class="headerlink" title="Variational Information Distillation for Knowledge Transfer"></a>Variational Information Distillation for Knowledge Transfer</h1><p>CVPR 2019[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.05835">https://arxiv.org/abs/1904.05835</a>]</p>
<p>文章提出了一种基于信息理论的蒸馏方法，采用变分信息最大化方法将知识迁移公式化为最大化teacher和student网络之间的相互信息。</p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201106144135.png" style="zoom: 67%;"></p>
<script type="math/tex; mode=display">
I(t;s) = H(t)-H(t|s)
\\=-E_t[log \ p(t)]+E_t,s[log \ p(t|s)]</script><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/120180529">互信息</a>为[教师模型的熵值] - [已知学生模型的条件下的教师模型熵值]当学生模型已知，能够使得教师模型的熵很小，这说明学生模型以及获得了能够恢复教师模型所需要的“压缩”知识，间接说明了此时学生模型已经学习的很好了。而这种情况下也就是说明上述公式中的H(t|s)很小，从而使得互信息I(t;s)会很大。作者从这个角度解释了为什么可以通过最大化互信息的方式来进行蒸馏学习。</p>
<p>TODO</p>
<p><strong>代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VIDLoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Variational Information Distillation for Knowledge Transfer (CVPR 2019),</span></span><br><span class="line"><span class="string">    code from author: https://github.com/ssahn0215/variational-information-distillation&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_input_channels,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_mid_channel,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_target_channels,</span></span></span><br><span class="line"><span class="function"><span class="params">                 init_pred_var=<span class="number">5.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 eps=<span class="number">1e-5</span></span>):</span></span><br><span class="line">        super(VIDLoss, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">conv1x1</span>(<span class="params">in_channels, out_channels, stride=<span class="number">1</span></span>):</span></span><br><span class="line">            <span class="keyword">return</span> nn.Conv2d(</span><br><span class="line">                in_channels, out_channels,</span><br><span class="line">                kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>,</span><br><span class="line">                bias=<span class="literal">False</span>, stride=stride)</span><br><span class="line"></span><br><span class="line">        self.regressor = nn.Sequential(</span><br><span class="line">            conv1x1(num_input_channels, num_mid_channel),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            conv1x1(num_mid_channel, num_mid_channel),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            conv1x1(num_mid_channel, num_target_channels),</span><br><span class="line">        )</span><br><span class="line">        self.log_scale = torch.nn.Parameter(</span><br><span class="line">            np.log(np.exp(init_pred_var-eps)<span class="number">-1.0</span>) * torch.ones(num_target_channels)</span><br><span class="line">            )</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input, target</span>):</span></span><br><span class="line">        <span class="comment"># pool for dimentsion match</span></span><br><span class="line">        s_H, t_H = input.shape[<span class="number">2</span>], target.shape[<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> s_H &gt; t_H:</span><br><span class="line">            input = F.adaptive_avg_pool2d(input, (t_H, t_H))</span><br><span class="line">        <span class="keyword">elif</span> s_H &lt; t_H:</span><br><span class="line">            target = F.adaptive_avg_pool2d(target, (s_H, s_H))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        pred_mean = self.regressor(input)</span><br><span class="line">        pred_var = torch.log(<span class="number">1.0</span>+torch.exp(self.log_scale))+self.eps</span><br><span class="line">        pred_var = pred_var.view(<span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        neg_log_prob = <span class="number">0.5</span>*(</span><br><span class="line">            (pred_mean-target)**<span class="number">2</span>/pred_var+torch.log(pred_var)</span><br><span class="line">            )</span><br><span class="line">        loss = torch.mean(neg_log_prob)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p><strong>1.Logits(Response)-based</strong></p>
<ul>
<li><strong>Distilling the Knowledge in a Neural Network</strong></li>
<li></li>
</ul>
<p><strong>2.Feature-based</strong></p>
<ul>
<li><strong>FITNETS: HINTS FOR THIN DEEP NETS</strong></li>
<li><strong>Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer</strong></li>
<li><strong>Paraphrasing Complex Network: Network Compression via Factor Transfer</strong></li>
<li><strong>Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons</strong></li>
<li><strong>like what you like: knowledge distill via neuron selectivity transfer</strong></li>
</ul>
<p><strong>3.Relation-based</strong></p>
<ul>
<li><strong>A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning</strong></li>
<li><strong>Learning Deep Representations with Probabilistic Knowledge Transfer</strong></li>
<li><strong>Correlation Congruence for Knowledge Distillation</strong></li>
<li><strong>Relational Knowledge Disitllation</strong></li>
<li><strong>Similarity-Preserving Knowledge Distillation</strong></li>
<li><strong>Variational Information Distillation for Knowledge Transfer</strong></li>
</ul>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">BROWALLIA</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://www.browallia.top/2020/10/29/Knowledge-Distillation/">https://www.browallia.top/2020/10/29/Knowledge-Distillation/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.browallia.top" target="_blank">Viva La Vida</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/">知识蒸馏</a></div><div class="post_share"><div class="social-share" data-image="https://gitee.com/browallia/tuchuang/raw/master/img/KD.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/zfb.png" target="_blank"><img class="post-qr-code-img" src="/img/zfb.png" alt="Alipay"/></a><div class="post-qr-code-desc">Alipay</div></li></ul></div></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2020/06/03/DETR-note/"><img class="next-cover" src="https://gitee.com/browallia/tuchuang/raw/master/img/DETR.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">DETR-note</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></article></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2020 By BROWALLIA</div><div class="footer_custom_text">早安，打工人！</div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  var script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const initData = {
      el: '#vcomment',
      appId: 'N9FvFdatw3iiNqYayrrAfRhJ-gzGzoHsz',
      appKey: 'oE1wULzwlDDcdjA0kAbxd3k0',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'en',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
    }

    if (true) { 
      initData.requiredFields= ('nick,mail'.split(','))
    }

    const valine = new Valine(initData)
  }

  if (typeof Valine === 'function') initValine() 
  else $.getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js', initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.querySelector('#vcomment'),loadValine)
  else setTimeout(() => loadValine(), 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>