<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>DL面试知识点 | Viva La Vida</title><meta name="keywords" content="面试准备"><meta name="author" content="BROWALLIA"><meta name="copyright" content="BROWALLIA"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="【本文转载自Github,对其知识点进行补充】 深度学习神经网络中的Epoch、Iteration、Batchsize神经网络中epoch与iteration是不相等的  batchsize：中文翻译为批大小（批尺寸）。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；  iteration：中文翻译为迭代，1个iteration等于使用batchsize个样本">
<meta property="og:type" content="article">
<meta property="og:title" content="DL面试知识点">
<meta property="og:url" content="https://www.browallia.top/2020/10/19/DL%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9/index.html">
<meta property="og:site_name" content="Viva La Vida">
<meta property="og:description" content="【本文转载自Github,对其知识点进行补充】 深度学习神经网络中的Epoch、Iteration、Batchsize神经网络中epoch与iteration是不相等的  batchsize：中文翻译为批大小（批尺寸）。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；  iteration：中文翻译为迭代，1个iteration等于使用batchsize个样本">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gitee.com/browallia/tuchuang/raw/master/img/cover.jpg">
<meta property="article:published_time" content="2020-10-19T01:59:25.000Z">
<meta property="article:modified_time" content="2020-12-21T08:47:43.188Z">
<meta property="article:author" content="BROWALLIA">
<meta property="article:tag" content="面试准备">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/browallia/tuchuang/raw/master/img/cover.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://www.browallia.top/2020/10/19/DL%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin="crossorigin"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="content"/><meta name="baidu-site-verification" content="content"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6df37eb9bea6fc6acfb352324998b672";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-D99NED65DC"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-D99NED65DC');
</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.2.0',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-12-21 16:47:43'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {
  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }

  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Viva La Vida" type="application/atom+xml">
</head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/dbs.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">27</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">16</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> POI</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> Me</span></a></div></div></div></div><div id="body-wrap"><div id="web_bg"></div><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">深度学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84Epoch%E3%80%81Iteration%E3%80%81Batchsize"><span class="toc-number">1.1.</span> <span class="toc-text">神经网络中的Epoch、Iteration、Batchsize</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%88BP%EF%BC%89"><span class="toc-number">1.2.</span> <span class="toc-text">反向传播（BP）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN%E6%9C%AC%E8%B4%A8%E5%92%8C%E4%BC%98%E5%8A%BF"><span class="toc-number">1.3.</span> <span class="toc-text">CNN本质和优势</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9E%8D%E7%82%B9%E7%9A%84%E5%AE%9A%E4%B9%89%E5%92%8C%E7%89%B9%E7%82%B9%EF%BC%9F"><span class="toc-number">1.4.</span> <span class="toc-text">鞍点的定义和特点？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9E%8D%E7%82%B9%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="toc-number">1.5.</span> <span class="toc-text">鞍点的定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">1.6.</span> <span class="toc-text">神经网络数据预处理方法有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%80%8E%E6%A0%B7%E8%BF%9B%E8%A1%8C%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%9F"><span class="toc-number">1.7.</span> <span class="toc-text">神经网络怎样进行参数初始化？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF"><span class="toc-number">1.8.</span> <span class="toc-text">卷积</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%87%E7%A8%8B"><span class="toc-number">1.8.1.</span> <span class="toc-text">卷积的反向传播过程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN-%E6%A8%A1%E5%9E%8B%E6%89%80%E9%9C%80%E7%9A%84%E8%AE%A1%E7%AE%97%E5%8A%9B%EF%BC%88flops%EF%BC%89%E5%92%8C%E5%8F%82%E6%95%B0%EF%BC%88parameters%EF%BC%89%E6%95%B0%E9%87%8F%E6%98%AF%E6%80%8E%E4%B9%88%E8%AE%A1%E7%AE%97%E7%9A%84%EF%BC%9F"><span class="toc-number">1.9.</span> <span class="toc-text">CNN 模型所需的计算力（flops）和参数（parameters）数量是怎么计算的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%EF%BC%88Pooling%EF%BC%89"><span class="toc-number">1.10.</span> <span class="toc-text">池化（Pooling）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82%E6%80%8E%E4%B9%88%E6%8E%A5%E6%94%B6%E5%90%8E%E9%9D%A2%E4%BC%A0%E8%BF%87%E6%9D%A5%E7%9A%84%E6%8D%9F%E5%A4%B1%EF%BC%9F"><span class="toc-number">1.10.1.</span> <span class="toc-text">池化层怎么接收后面传过来的损失？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%84%9F%E5%8F%97%E9%87%8E"><span class="toc-number">1.11.</span> <span class="toc-text">感受野</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%84%9F%E5%8F%97%E9%87%8E%E8%AE%A1%E7%AE%97"><span class="toc-number">1.11.1.</span> <span class="toc-text">感受野计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%84%9F%E5%8F%97%E9%87%8E"><span class="toc-number">1.11.2.</span> <span class="toc-text">卷积神经网络的感受野</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">1.12.</span> <span class="toc-text">权重初始化方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Xavier"><span class="toc-number">1.12.1.</span> <span class="toc-text">Xavier</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kaiming%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.12.2.</span> <span class="toc-text">Kaiming初始化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">1.13.</span> <span class="toc-text">正则化方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Batch-Normalization%EF%BC%88BN%EF%BC%89"><span class="toc-number">1.14.</span> <span class="toc-text">Batch Normalization（BN）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#BN-%E5%8E%9F%E7%90%86"><span class="toc-number">1.14.1.</span> <span class="toc-text">BN 原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81BN%EF%BC%9F"><span class="toc-number">1.14.1.1.</span> <span class="toc-text">为什么需要BN？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Internal-Covariate-Shift%EF%BC%88ICS%EF%BC%89"><span class="toc-number">1.14.1.2.</span> <span class="toc-text">Internal Covariate Shift（ICS）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.14.1.3.</span> <span class="toc-text">具体实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#BN%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">1.14.1.4.</span> <span class="toc-text">BN的作用</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E5%86%99-BN"><span class="toc-number">1.14.2.</span> <span class="toc-text">手写 BN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BN-%E5%8F%AF%E4%BB%A5%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88%E4%B9%88%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88"><span class="toc-number">1.14.3.</span> <span class="toc-text">BN 可以防止过拟合么？为什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BN-%E6%9C%89%E5%93%AA%E4%BA%9B%E5%8F%82%E6%95%B0%EF%BC%9F"><span class="toc-number">1.14.4.</span> <span class="toc-text">BN 有哪些参数？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BN-%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%8E%A8%E5%AF%BC"><span class="toc-number">1.14.5.</span> <span class="toc-text">BN 的反向传播推导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BN-%E5%9C%A8%E8%AE%AD%E7%BB%83%E5%92%8C%E6%B5%8B%E8%AF%95%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-number">1.14.6.</span> <span class="toc-text">BN 在训练和测试的区别？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Weight-Normalization%EF%BC%88WN%EF%BC%89"><span class="toc-number">1.15.</span> <span class="toc-text">Weight Normalization（WN）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Layer-Normalization%EF%BC%88LN%EF%BC%89"><span class="toc-number">1.16.</span> <span class="toc-text">Layer Normalization（LN）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Instance-Normalization%EF%BC%88IN%EF%BC%89"><span class="toc-number">1.17.</span> <span class="toc-text">Instance Normalization（IN）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Group-Normalization%EF%BC%88GN%EF%BC%89"><span class="toc-number">1.18.</span> <span class="toc-text">Group Normalization（GN）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#BN%E3%80%81LN%E3%80%81WN%E3%80%81IN%E5%92%8CGN%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.18.1.</span> <span class="toc-text">BN、LN、WN、IN和GN的区别</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-number">1.19.</span> <span class="toc-text">优化算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">1.19.1.</span> <span class="toc-text">梯度下降法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mini-batch%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">1.19.2.</span> <span class="toc-text">mini-batch梯度下降法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%88SGD%EF%BC%89"><span class="toc-number">1.19.3.</span> <span class="toc-text">随机梯度下降法（SGD）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#SGD%E6%AF%8F%E6%AD%A5%E5%81%9A%E4%BB%80%E4%B9%88%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BDonline-learning%EF%BC%9F"><span class="toc-number">1.19.3.1.</span> <span class="toc-text">SGD每步做什么，为什么能online learning？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A8%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%88Momentum%EF%BC%89"><span class="toc-number">1.19.4.</span> <span class="toc-text">动量梯度下降法（Momentum）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RMSprop"><span class="toc-number">1.19.5.</span> <span class="toc-text">RMSprop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adagrad"><span class="toc-number">1.19.6.</span> <span class="toc-text">Adagrad</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adam"><span class="toc-number">1.19.7.</span> <span class="toc-text">Adam</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Adam-%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%84%E8%BF%AD%E4%BB%A3%E5%85%AC%E5%BC%8F"><span class="toc-number">1.19.7.1.</span> <span class="toc-text">Adam 优化器的迭代公式</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">1.20.</span> <span class="toc-text">激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Sigmoid-amp-Tanh"><span class="toc-number">1.20.1.</span> <span class="toc-text">Sigmoid&amp;Tanh</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Sigmoid%E7%94%A8%E4%BD%9C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%97%B6%EF%BC%8C%E5%88%86%E7%B1%BB%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%EF%BC%8C%E8%80%8C%E4%B8%8D%E7%94%A8%E5%9D%87%E6%96%B9%E6%8D%9F%E5%A4%B1%EF%BC%9F"><span class="toc-number">1.20.1.1.</span> <span class="toc-text">Sigmoid用作激活函数时，分类为什么要用交叉熵损失，而不用均方损失？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ReLU"><span class="toc-number">1.20.2.</span> <span class="toc-text">ReLU</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ReLU-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%B8%BA%E4%BB%80%E4%B9%88%E6%AF%94sigmoid%E5%92%8Ctanh%E5%A5%BD%EF%BC%9F"><span class="toc-number">1.20.2.1.</span> <span class="toc-text">ReLU 激活函数为什么比sigmoid和tanh好？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ReLU-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E8%A7%A3%E5%86%B3%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">1.20.2.2.</span> <span class="toc-text">ReLU 激活函数为什么能解决梯度消失问题？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ReLU-%E6%9C%89%E5%93%AA%E4%BA%9B%E5%8F%98%E4%BD%93%EF%BC%9F"><span class="toc-number">1.20.2.3.</span> <span class="toc-text">ReLU 有哪些变体？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dropout"><span class="toc-number">1.21.</span> <span class="toc-text">Dropout</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout-%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86"><span class="toc-number">1.21.1.</span> <span class="toc-text">Dropout 基本原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%EF%BC%9F"><span class="toc-number">1.21.2.</span> <span class="toc-text">Dropout 如何实现？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Drop-%E5%9C%A8%E8%AE%AD%E7%BB%83%E5%92%8C%E6%B5%8B%E8%AF%95%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.21.3.</span> <span class="toc-text">Drop 在训练和测试的区别</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%88Loss%EF%BC%89"><span class="toc-number">1.22.</span> <span class="toc-text">损失函数（Loss）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MSE"><span class="toc-number">1.22.1.</span> <span class="toc-text">MSE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cross-Entropy-Loss%EF%BC%88CE%EF%BC%89"><span class="toc-number">1.22.2.</span> <span class="toc-text">Cross Entropy Loss（CE）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hinge-Loss"><span class="toc-number">1.22.3.</span> <span class="toc-text">Hinge Loss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Focal-Loss"><span class="toc-number">1.22.4.</span> <span class="toc-text">Focal Loss</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E5%8D%B7%E7%A7%AF%E6%9C%89%E4%BB%80%E4%B9%88%E4%BD%9C%E7%94%A8%EF%BC%9F"><span class="toc-number">1.23.</span> <span class="toc-text">1*1 卷积有什么作用？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AlexNet"><span class="toc-number">1.24.</span> <span class="toc-text">AlexNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VGG"><span class="toc-number">1.25.</span> <span class="toc-text">VGG</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ResNet"><span class="toc-number">1.26.</span> <span class="toc-text">ResNet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ResNet%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E7%94%A8Dropout"><span class="toc-number">1.26.1.</span> <span class="toc-text">ResNet为什么不用Dropout?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88-ResNet-%E4%B8%8D%E5%9C%A8%E4%B8%80%E5%BC%80%E5%A7%8B%E5%B0%B1%E4%BD%BF%E7%94%A8residual-block-%E8%80%8C%E6%98%AF%E4%BD%BF%E7%94%A8%E4%B8%80%E4%B8%AA7%C3%977%E7%9A%84%E5%8D%B7%E7%A7%AF%EF%BC%9F"><span class="toc-number">1.26.2.</span> <span class="toc-text">为什么 ResNet 不在一开始就使用residual block,而是使用一个7×7的卷积？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFBottlenet-layer%EF%BC%9F"><span class="toc-number">1.26.3.</span> <span class="toc-text">什么是Bottlenet layer？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ResNet%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%EF%BC%9F"><span class="toc-number">1.26.4.</span> <span class="toc-text">ResNet如何解决梯度消失？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ResNet%E7%BD%91%E7%BB%9C%E8%B6%8A%E6%9D%A5%E8%B6%8A%E6%B7%B1%EF%BC%8C%E5%87%86%E7%A1%AE%E7%8E%87%E4%BC%9A%E4%B8%8D%E4%BC%9A%E6%8F%90%E5%8D%87%EF%BC%9F"><span class="toc-number">1.26.5.</span> <span class="toc-text">ResNet网络越来越深，准确率会不会提升？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ResNet-v2"><span class="toc-number">1.27.</span> <span class="toc-text">ResNet v2</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ResNet-v1-%E4%B8%8E-ResNet-v2%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.27.1.</span> <span class="toc-text">ResNet v1 与 ResNet v2的区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ResNet-v2-%E7%9A%84-ReLU-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%9C%89%E4%BB%80%E4%B9%88%E4%B8%8D%E5%90%8C%EF%BC%9F"><span class="toc-number">1.27.2.</span> <span class="toc-text">ResNet v2 的 ReLU 激活函数有什么不同？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ResNeXt"><span class="toc-number">1.28.</span> <span class="toc-text">ResNeXt</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%A6%82%E4%BD%95%E8%B0%83%E6%95%B4"><span class="toc-number">1.29.</span> <span class="toc-text">学习率如何调整</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%92%8C%E5%AE%BD%E5%BA%A6%E4%BD%9C%E7%94%A8"><span class="toc-number">1.30.</span> <span class="toc-text">神经网络的深度和宽度作用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E5%8E%8B%E7%BC%A9%E4%B8%8E%E9%87%8F%E5%8C%96"><span class="toc-number">1.31.</span> <span class="toc-text">网络压缩与量化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Batch-Size"><span class="toc-number">1.32.</span> <span class="toc-text">Batch Size</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BN%E5%92%8CDropout%E5%9C%A8%E8%AE%AD%E7%BB%83%E5%92%8C%E6%B5%8B%E8%AF%95%E6%97%B6%E7%9A%84%E5%B7%AE%E5%88%AB"><span class="toc-number">1.33.</span> <span class="toc-text">BN和Dropout在训练和测试时的差别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B0%83%E5%8F%82%E6%9C%89%E5%93%AA%E4%BA%9B%E6%8A%80%E5%B7%A7%EF%BC%9F"><span class="toc-number">1.34.</span> <span class="toc-text">深度学习调参有哪些技巧？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%9F%BA%E6%9C%AC%E7%94%A83x3%E5%92%8C5x5%E7%9A%84%E5%8D%B7%E7%A7%AF%EF%BC%88%E5%A5%87%E6%95%B0%EF%BC%89%EF%BC%8C%E8%80%8C%E4%B8%8D%E6%98%AF2x2%E5%92%8C4x4%E7%9A%84%E5%8D%B7%E7%A7%AF%EF%BC%88%E5%81%B6%E6%95%B0%EF%BC%89%EF%BC%9F"><span class="toc-number">1.35.</span> <span class="toc-text">为什么深度学习中的模型基本用3x3和5x5的卷积（奇数），而不是2x2和4x4的卷积（偶数）？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83%E4%B8%AD%E6%98%AF%E5%90%A6%E6%9C%89%E5%BF%85%E8%A6%81%E4%BD%BF%E7%94%A8L1%E8%8E%B7%E5%BE%97%E7%A8%80%E7%96%8F%E8%A7%A3"><span class="toc-number">1.36.</span> <span class="toc-text">深度学习训练中是否有必要使用L1获得稀疏解?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EfficientNet"><span class="toc-number">1.37.</span> <span class="toc-text">EfficientNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%88Normalization%EF%BC%89%E5%AF%B9%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%89%E7%9A%84%E5%B8%AE%E5%8A%A9%EF%BC%9F"><span class="toc-number">1.38.</span> <span class="toc-text">如何理解归一化（Normalization）对于神经网络（深度学习）的帮助？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3%EF%BC%9F"><span class="toc-number">1.39.</span> <span class="toc-text">多标签分类怎么解决？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%8D%B7%E7%A7%AF%EF%BC%88deconv%EF%BC%89-%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%EF%BC%88trans%EF%BC%89"><span class="toc-number">1.40.</span> <span class="toc-text">反卷积（deconv）&#x2F;转置卷积（trans）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF%EF%BC%88dilated-Atrous-conv%EF%BC%89"><span class="toc-number">1.41.</span> <span class="toc-text">空洞卷积（dilated&#x2F;Atrous conv）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pooling%E5%B1%82%E5%8E%9F%E7%90%86"><span class="toc-number">1.42.</span> <span class="toc-text">Pooling层原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#depthwise%E5%8D%B7%E7%A7%AF%E5%8A%A0%E9%80%9F%E6%AF%94%E6%8E%A8%E5%AF%BC"><span class="toc-number">1.43.</span> <span class="toc-text">depthwise卷积加速比推导</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%99%8D%E9%87%87%E7%94%A8%E4%BD%BF%E7%94%A8max-pooling%EF%BC%8C%E8%80%8C%E5%88%86%E7%B1%BB%E4%BD%BF%E7%94%A8average-pooling"><span class="toc-number">1.44.</span> <span class="toc-text">为什么降采用使用max pooling，而分类使用average pooling</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#max-pooling%E5%A6%82%E4%BD%95%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.45.</span> <span class="toc-text">max pooling如何反向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%8D%B7%E7%A7%AF"><span class="toc-number">1.46.</span> <span class="toc-text">反卷积</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%84%E5%8D%B7%E7%A7%AF%EF%BC%88group-convolution%EF%BC%89"><span class="toc-number">1.47.</span> <span class="toc-text">组卷积（group convolution）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%A4%E9%94%99%E7%BB%84%E5%8D%B7%E7%A7%AF%EF%BC%88Interleaved-group-convolutions%EF%BC%8CIGC%EF%BC%89"><span class="toc-number">1.48.</span> <span class="toc-text">交错组卷积（Interleaved group convolutions，IGC）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A9%BA%E6%B4%9E-%E6%89%A9%E5%BC%A0%E5%8D%B7%E7%A7%AF%EF%BC%88Dilated-Atrous-Convolution%EF%BC%89"><span class="toc-number">1.49.</span> <span class="toc-text">空洞&#x2F;扩张卷积（Dilated&#x2F;Atrous Convolution）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%EF%BC%88Transposed-Convolutions-deconvlution%EF%BC%89"><span class="toc-number">1.50.</span> <span class="toc-text">转置卷积（Transposed Convolutions&#x2F;deconvlution）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Inception%E7%B3%BB%E5%88%97%EF%BC%88V1-V4%EF%BC%89"><span class="toc-number">1.51.</span> <span class="toc-text">Inception系列（V1-V4）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#InceptionV1"><span class="toc-number">1.51.1.</span> <span class="toc-text">InceptionV1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#InceptionV2"><span class="toc-number">1.51.2.</span> <span class="toc-text">InceptionV2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#InceptionV3"><span class="toc-number">1.51.3.</span> <span class="toc-text">InceptionV3</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#InceptionV4"><span class="toc-number">1.51.4.</span> <span class="toc-text">InceptionV4</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DenseNet"><span class="toc-number">1.52.</span> <span class="toc-text">DenseNet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88-DenseNet-%E6%AF%94-ResNet-%E5%A5%BD%EF%BC%9F"><span class="toc-number">1.52.1.</span> <span class="toc-text">为什么 DenseNet 比 ResNet 好？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88-DenseNet-%E6%AF%94-ResNet-%E6%9B%B4%E8%80%97%E6%98%BE%E5%AD%98%EF%BC%9F"><span class="toc-number">1.52.2.</span> <span class="toc-text">为什么 DenseNet 比 ResNet 更耗显存？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SE-Net"><span class="toc-number">1.53.</span> <span class="toc-text">SE-Net</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Squeeze-Excitation%E7%BB%93%E6%9E%84%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84%EF%BC%9F"><span class="toc-number">1.53.1.</span> <span class="toc-text">Squeeze-Excitation结构是怎么实现的？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FCN"><span class="toc-number">1.54.</span> <span class="toc-text">FCN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#U-Net"><span class="toc-number">1.55.</span> <span class="toc-text">U-Net</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DeepLab-%E7%B3%BB%E5%88%97"><span class="toc-number">1.56.</span> <span class="toc-text">DeepLab 系列</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%B9%E6%A1%86%E5%9B%9E%E9%A1%BE%EF%BC%88Bounding-Box-Regression%EF%BC%89"><span class="toc-number">1.57.</span> <span class="toc-text">边框回顾（Bounding-Box Regression）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Xception"><span class="toc-number">1.58.</span> <span class="toc-text">Xception</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SENet"><span class="toc-number">1.59.</span> <span class="toc-text">SENet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SKNet"><span class="toc-number">1.60.</span> <span class="toc-text">SKNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GCNet"><span class="toc-number">1.61.</span> <span class="toc-text">GCNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Octave-Convolution"><span class="toc-number">1.62.</span> <span class="toc-text">Octave Convolution</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MobileNet-%E7%B3%BB%E5%88%97%EF%BC%88V1-V3%EF%BC%89"><span class="toc-number">1.63.</span> <span class="toc-text">MobileNet 系列（V1-V3）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MobileNetV1"><span class="toc-number">1.63.1.</span> <span class="toc-text">MobileNetV1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MobileNetV2"><span class="toc-number">1.63.2.</span> <span class="toc-text">MobileNetV2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MobileNetV3"><span class="toc-number">1.63.3.</span> <span class="toc-text">MobileNetV3</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MobileNet%E7%B3%BB%E5%88%97%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BF%AB%EF%BC%9F%E5%90%84%E6%9C%89%E5%A4%9A%E5%B0%91%E5%B1%82%EF%BC%9F%E5%A4%9A%E5%B0%91%E5%8F%82%E6%95%B0%EF%BC%9F"><span class="toc-number">1.63.4.</span> <span class="toc-text">MobileNet系列为什么快？各有多少层？多少参数？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MobileNetV1%E3%80%81MobileNetV2%E5%92%8CMobileNetV3%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB"><span class="toc-number">1.63.5.</span> <span class="toc-text">MobileNetV1、MobileNetV2和MobileNetV3有什么区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MobileNetv2%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%8A%A0shotcut%EF%BC%9F"><span class="toc-number">1.63.6.</span> <span class="toc-text">MobileNetv2为什么会加shotcut？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MobileNet-V2%E4%B8%AD%E7%9A%84Residual%E7%BB%93%E6%9E%84%E6%9C%80%E5%85%88%E6%98%AF%E5%93%AA%E4%B8%AA%E7%BD%91%E7%BB%9C%E6%8F%90%E5%87%BA%E6%9D%A5%E7%9A%84%EF%BC%9F"><span class="toc-number">1.63.7.</span> <span class="toc-text">MobileNet V2中的Residual结构最先是哪个网络提出来的？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ShuffleNet-%E7%B3%BB%E5%88%97%EF%BC%88V1-V2-%EF%BC%89"><span class="toc-number">1.64.</span> <span class="toc-text">ShuffleNet 系列（V1-V2++）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ShuffleNetV1"><span class="toc-number">1.64.1.</span> <span class="toc-text">ShuffleNetV1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ShuffleNetV2"><span class="toc-number">1.64.2.</span> <span class="toc-text">ShuffleNetV2</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#IGC-%E7%B3%BB%E5%88%97%EF%BC%88V1-V3%EF%BC%89"><span class="toc-number">1.65.</span> <span class="toc-text">IGC 系列（V1-V3）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E7%BD%91%E7%BB%9C%EF%BC%88Depth-separable-convolution%EF%BC%89"><span class="toc-number">1.66.</span> <span class="toc-text">深度可分离网络（Depth separable convolution）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.67.</span> <span class="toc-text"> </span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89"><span class="toc-number">2.</span> <span class="toc-text">计算机视觉</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#IoU"><span class="toc-number">2.1.</span> <span class="toc-text">IoU</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97-mIoU%EF%BC%9F"><span class="toc-number">2.1.1.</span> <span class="toc-text">如何计算 mIoU？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#mAP"><span class="toc-number">2.2.</span> <span class="toc-text">mAP</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97-mAP%EF%BC%9F"><span class="toc-number">2.2.1.</span> <span class="toc-text">如何计算 mAP？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%BA%A6%E9%87%8F%E6%A0%87%E5%87%86"><span class="toc-number">2.3.</span> <span class="toc-text">目标检测度量标准</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E5%BA%A6%E9%87%8F%E6%A0%87%E5%87%86"><span class="toc-number">2.4.</span> <span class="toc-text">图像分割度量标准</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6NMS"><span class="toc-number">2.5.</span> <span class="toc-text">非极大值抑制NMS</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84Anchor"><span class="toc-number">2.6.</span> <span class="toc-text">目标检测中的Anchor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E5%A7%8B%E5%9B%BE%E7%89%87%E4%B8%AD%E7%9A%84ROI%E5%A6%82%E4%BD%95%E6%98%A0%E5%B0%84%E5%88%B0%E5%88%B0feature-map"><span class="toc-number">2.7.</span> <span class="toc-text">原始图片中的ROI如何映射到到feature map?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%B7%E9%97%AEFaster-R-CNN%E5%92%8CSSD-%E4%B8%AD%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8smooth-l1-loss%EF%BC%8C%E5%92%8Cl2%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-number">2.8.</span> <span class="toc-text">请问Faster R-CNN和SSD 中为什么用smooth l1 loss，和l2有什么区别？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%99%E5%AE%9A5%E4%B8%AA%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E5%92%8C5%E4%B8%AA%E5%AF%B9%E9%BD%90%E5%90%8E%E7%9A%84%E7%82%B9%EF%BC%8C%E6%B1%82%E6%80%8E%E4%B9%88%E5%8F%98%E6%8D%A2%E7%9A%84%EF%BC%9F"><span class="toc-number">2.9.</span> <span class="toc-text">给定5个人脸关键点和5个对齐后的点，求怎么变换的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bounding-boxes-%E5%9B%9E%E5%BD%92%E5%8E%9F%E7%90%86-%E5%85%AC%E5%BC%8F"><span class="toc-number">2.10.</span> <span class="toc-text">Bounding boxes 回归原理&#x2F;公式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#U-Net-%E5%92%8C-FCN%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">2.11.</span> <span class="toc-text">U-Net 和 FCN的区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8DKCF%E7%AE%97%E6%B3%95"><span class="toc-number">2.12.</span> <span class="toc-text">介绍KCF算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8DMobileNet-SSD%E7%AE%97%E6%B3%95"><span class="toc-number">2.13.</span> <span class="toc-text">介绍MobileNet-SSD算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E5%A4%9A%E5%B0%BA%E5%BA%A6%E8%AE%AD%E7%BB%83-%E6%B5%8B%E8%AF%95%EF%BC%9F"><span class="toc-number">2.14.</span> <span class="toc-text">目标检测中的多尺度训练&#x2F;测试？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E6%AD%A3%E8%B4%9F%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%B9%B3%E8%A1%A1%E9%97%AE%E9%A2%98"><span class="toc-number">2.15.</span> <span class="toc-text">目标检测中的正负样本不平衡问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E7%B1%BB%E5%88%AB%E6%BC%8F%E6%A3%80%E9%97%AE%E9%A2%98%E8%AF%A5%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3%EF%BC%9F"><span class="toc-number">2.16.</span> <span class="toc-text">目标检测中的类别漏检问题该怎么解决？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RPN"><span class="toc-number">2.17.</span> <span class="toc-text">RPN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RPN-%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.17.1.</span> <span class="toc-text">RPN 的损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RPN%E4%B8%AD%E7%9A%84anchor-box%E6%98%AF%E6%80%8E%E4%B9%88%E9%80%89%E5%8F%96%E7%9A%84%EF%BC%9F"><span class="toc-number">2.17.2.</span> <span class="toc-text">RPN中的anchor box是怎么选取的？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RoI-Pooling"><span class="toc-number">2.18.</span> <span class="toc-text">RoI Pooling</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RoI-Align"><span class="toc-number">2.19.</span> <span class="toc-text">RoI Align</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E8%A6%81%E5%85%88%E7%BC%96%E7%A0%81%E5%86%8D%E8%A7%A3%E7%A0%81%EF%BC%9F"><span class="toc-number">2.20.</span> <span class="toc-text">为什么深度学习中的图像分割要先编码再解码？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NMS"><span class="toc-number">2.21.</span> <span class="toc-text">NMS</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NMS%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93"><span class="toc-number">2.22.</span> <span class="toc-text">NMS及其变体</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#R-CNN-%E7%B3%BB%E5%88%97"><span class="toc-number">2.23.</span> <span class="toc-text">R-CNN 系列</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#R-CNN"><span class="toc-number">2.23.1.</span> <span class="toc-text">R-CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Fast-R-CNN"><span class="toc-number">2.23.2.</span> <span class="toc-text">Fast R-CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Faster-R-CNN"><span class="toc-number">2.23.3.</span> <span class="toc-text">Faster R-CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Faster-R-CNN-%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8smooth-l1-loss%EF%BC%8C%E5%92%8Cl2%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-number">2.23.3.1.</span> <span class="toc-text">Faster R-CNN 为什么用smooth l1 loss，和l2有什么区别？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SSD-%E7%AE%97%E6%B3%95"><span class="toc-number">2.24.</span> <span class="toc-text">SSD 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#YOLO%E7%B3%BB%E5%88%97%EF%BC%88V1-V3%EF%BC%89"><span class="toc-number">2.25.</span> <span class="toc-text">YOLO系列（V1-V3）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#YOLOV1"><span class="toc-number">2.25.1.</span> <span class="toc-text">YOLOV1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#YOLOv2%E7%AE%97%E6%B3%95"><span class="toc-number">2.25.2.</span> <span class="toc-text">YOLOv2算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#YOLOv3%E7%AE%97%E6%B3%95"><span class="toc-number">2.25.3.</span> <span class="toc-text">YOLOv3算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#YOLOv1-YOLOv2-YOLOv3%E7%9A%84%E5%8F%91%E5%B1%95"><span class="toc-number">2.25.4.</span> <span class="toc-text">YOLOv1 YOLOv2 YOLOv3的发展</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#YOLOv2%E5%92%8CYOLOv3%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%8C%BA%E5%88%AB"><span class="toc-number">2.25.5.</span> <span class="toc-text">YOLOv2和YOLOv3的损失函数区别</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RetinaNet%EF%BC%88Focal-loss%EF%BC%89"><span class="toc-number">2.26.</span> <span class="toc-text">RetinaNet（Focal loss）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FPN-%E7%89%B9%E5%BE%81%E9%87%91%E5%AD%97%E5%A1%94%E7%BD%91%E7%BB%9C"><span class="toc-number">2.27.</span> <span class="toc-text">FPN 特征金字塔网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Faster-R-CNN%E7%9A%84RPN%E7%BD%91%E7%BB%9C"><span class="toc-number">2.28.</span> <span class="toc-text">Faster R-CNN的RPN网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ROI-Pooling%E3%80%81ROI-Align%E5%92%8CROI-Warping%E5%AF%B9%E6%AF%94"><span class="toc-number">2.29.</span> <span class="toc-text">ROI Pooling、ROI Align和ROI Warping对比</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DeepLab%E7%B3%BB%E5%88%97%EF%BC%88V1-V3-%EF%BC%89"><span class="toc-number">2.30.</span> <span class="toc-text">DeepLab系列（V1-V3+）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#U-Net%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%9C%A8%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E8%A1%A8%E7%8E%B0%E5%A5%BD%EF%BC%9F"><span class="toc-number">2.31.</span> <span class="toc-text">U-Net神经网络为什么会在医学图像分割表现好？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scene-Parsing%E5%92%8CSemantic-Segmentation%E6%9C%89%E4%BB%80%E4%B9%88%E4%B8%8D%E5%90%8C"><span class="toc-number">2.32.</span> <span class="toc-text">Scene Parsing和Semantic Segmentation有什么不同?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CenterNet"><span class="toc-number">2.33.</span> <span class="toc-text">CenterNet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CornerPooling%E6%98%AF%E6%80%8E%E4%B9%88%E5%81%9A%E7%9A%84%EF%BC%9F"><span class="toc-number">2.33.1.</span> <span class="toc-text">CornerPooling是怎么做的？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TODO"><span class="toc-number">2.34.</span> <span class="toc-text">TODO</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Tricks"><span class="toc-number">3.</span> <span class="toc-text">深度学习Tricks</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Data-Augmentation"><span class="toc-number">3.0.1.</span> <span class="toc-text">Data Augmentation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pre-Processing"><span class="toc-number">3.0.2.</span> <span class="toc-text">Pre-Processing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Initialization"><span class="toc-number">3.0.3.</span> <span class="toc-text">Initialization</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#During-Training"><span class="toc-number">3.0.3.1.</span> <span class="toc-text">During Training</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Activation-Functions"><span class="toc-number">3.0.4.</span> <span class="toc-text">Activation Functions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Regularizations"><span class="toc-number">3.0.5.</span> <span class="toc-text">Regularizations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Insight-from-Figures"><span class="toc-number">3.0.6.</span> <span class="toc-text">Insight from Figures</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Ensemble"><span class="toc-number">3.0.7.</span> <span class="toc-text">Ensemble</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mixup"><span class="toc-number">3.0.8.</span> <span class="toc-text">Mixup</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#label-smoothing"><span class="toc-number">3.0.9.</span> <span class="toc-text">label smoothing</span></a></li></ol></li></ol></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://gitee.com/browallia/tuchuang/raw/master/img/cover.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Viva La Vida</a></span><span id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> POI</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> Me</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">DL面试知识点</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-10-19T01:59:25.000Z" title="发表于 2020-10-19 09:59:25">2020-10-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2020-12-21T08:47:43.188Z" title="更新于 2020-12-21 16:47:43">2020-12-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">1.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>3分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><p>【<strong>本文转载自<a target="_blank" rel="noopener" href="https://github.com/amusi/Deep-Learning-Interview-Book">Github</a>,对其知识点进行补充</strong>】</p>
<h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><h2 id="神经网络中的Epoch、Iteration、Batchsize"><a href="#神经网络中的Epoch、Iteration、Batchsize" class="headerlink" title="神经网络中的Epoch、Iteration、Batchsize"></a>神经网络中的Epoch、Iteration、Batchsize</h2><p>神经网络中epoch与iteration是不相等的</p>
<ul>
<li><p>batchsize：中文翻译为批大小（批尺寸）。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；</p>
</li>
<li><p>iteration：中文翻译为迭代，1个iteration等于使用batchsize个样本训练一次；一个迭代 = 一个正向通过+一个反向通过</p>
</li>
<li><p>epoch：迭代次数，1个epoch等于使用训练集中的全部样本训练一次；一个epoch = 所有训练样本的一个正向传递和一个反向传递</p>
</li>
</ul>
<p>举个例子，训练集有1000个样本，batchsize=10，那么：训练完整个样本集需要：100次iteration，1次epoch。</p>
<p><img src="https://gss0.baidu.com/-vo3dSag_xI4khGko9WTAnF6hhy/zhidao/wh%3D600%2C800/sign=36204981f1039245a1e0e909b7a488fa/e61190ef76c6a7ef3bf5176af0faaf51f2de66af.jpg" alt="img"></p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/67414365">神经网络中的Epoch、Iteration、Batchsize</a></li>
<li><a target="_blank" rel="noopener" href="https://zhidao.baidu.com/question/716300338908227765.html">神经网络中epoch与iteration相等吗</a></li>
</ul>
<h2 id="反向传播（BP）"><a href="#反向传播（BP）" class="headerlink" title="反向传播（BP）"></a>反向传播（BP）</h2><p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201019155522.png" alt></p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201019155550.png" alt></p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/964345dddb70">一文搞懂反向传播算法</a></li>
<li><a href="https://www.browallia.top/2018/10/13/%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95%E7%9A%84%E4%B8%89%E5%B1%82bp%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">numpy实现简单的BP神经网络</a></li>
</ul>
<h2 id="CNN本质和优势"><a href="#CNN本质和优势" class="headerlink" title="CNN本质和优势"></a>CNN本质和优势</h2><p>局部卷积（提取局部特征）</p>
<p>权值共享（降低训练难度）</p>
<p>Pooling（降维，将低层次组合为高层次的特征，保留主要特征，减少下一层的参数和计算量，防止过拟合）</p>
<p>多层次结构</p>
<h2 id="鞍点的定义和特点？"><a href="#鞍点的定义和特点？" class="headerlink" title="鞍点的定义和特点？"></a>鞍点的定义和特点？</h2><h2 id="鞍点的定义"><a href="#鞍点的定义" class="headerlink" title="鞍点的定义"></a>鞍点的定义</h2><p> 一个不是局部最小值的<strong>驻点（一阶导数为0的点）</strong>称为鞍点。数学含义是： 目标函数在此点上的梯度（一阶导数）值为 0， <strong>但从改点出发的一个方向是函数的极大值点，而在另一个方向是函数的极小值点。</strong></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lanchunhui/article/details/52504859">极值点、驻点、鞍点、拐点</a></p>
<h2 id="神经网络数据预处理方法有哪些？"><a href="#神经网络数据预处理方法有哪些？" class="headerlink" title="神经网络数据预处理方法有哪些？"></a>神经网络数据预处理方法有哪些？</h2><p><a href="https://www.browallia.top/2019/10/21/CS231n-note-3/">1.3数据预处理</a></p>
<h2 id="神经网络怎样进行参数初始化？"><a href="#神经网络怎样进行参数初始化？" class="headerlink" title="神经网络怎样进行参数初始化？"></a>神经网络怎样进行参数初始化？</h2><p><a href="https://www.browallia.top/2019/10/21/CS231n-note-3/">1.3数据预处理</a></p>
<h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/Convolution_schematic.gif" alt></p>
<p>卷积的可以看作是<strong>滤波器</strong></p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="http://ufldl.stanford.edu/tutorial/supervised/FeatureExtractionUsingConvolution/">Feature Extraction Using Convolution</a></li>
<li><p><a target="_blank" rel="noopener" href="https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/convolution.html">convolution</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/chaipp0607/article/details/72236892?locationNum=9&amp;fps=1">理解图像卷积操作的意义</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/Yu-FeiFei/p/6800519.html">关于深度学习中卷积核操作</a></p>
</li>
</ul>
<h3 id="卷积的反向传播过程"><a href="#卷积的反向传播过程" class="headerlink" title="卷积的反向传播过程"></a>卷积的反向传播过程</h3><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="http://cogprints.org/5869/1/cnn_tutorial.pdf">Notes on Convolutional Neural Network</a></li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zouxy09/article/details/9993371">Deep Learning论文笔记之（四）CNN卷积神经网络推导和实现</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95">反向传导算法</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/tornadomeet/p/3468450.html">Deep learning：五十一(CNN的反向求导及练习)</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6494810.html">卷积神经网络(CNN)反向传播算法</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/walegahaha/article/details/51945421">卷积神经网络(CNN)反向传播算法公式详细推导</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/61863634">全连接神经网络中反向传播算法数学推导</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/61898234">卷积神经网络(CNN)反向传播算法推导</a></p>
</li>
</ul>
<h2 id="CNN-模型所需的计算力（flops）和参数（parameters）数量是怎么计算的？"><a href="#CNN-模型所需的计算力（flops）和参数（parameters）数量是怎么计算的？" class="headerlink" title="CNN 模型所需的计算力（flops）和参数（parameters）数量是怎么计算的？"></a>CNN 模型所需的计算力（flops）和参数（parameters）数量是怎么计算的？</h2><p>对于一个卷积层，假设其大小为 $h<em>w</em>c<em>n$ （其中c为input channel, n为output channel），输出的feature map尺寸为 $H</em>W$ ，则该卷积层的</p>
<ul>
<li><p>paras =<img src="https://www.zhihu.com/equation?tex=n+%5Ctimes+%28h+%5Ctimes+w+%5Ctimes+c+%2B+1%29" alt></p>
</li>
<li><p>FLOPs = <img src="https://www.zhihu.com/equation?tex=H%27+%5Ctimes+W%27+%5Ctimes+n+%5Ctimes%28h+%5Ctimes+w+%5Ctimes+c+%2B+1%29" alt></p>
</li>
</ul>
<ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/65305385/answer/256845252">CNN 模型所需的计算力（flops）和参数（parameters）数量是怎么计算的？</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/sinat_34460960/article/details/84779219">CNN中parameters和FLOPs计算</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/smallhujiu/article/details/80876875">FLOPS理解</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/Lyken17/pytorch-OpCounter">PyTorch-OpCounter</a></li>
</ul>
<h2 id="池化（Pooling）"><a href="#池化（Pooling）" class="headerlink" title="池化（Pooling）"></a>池化（Pooling）</h2><p><strong>平均池化（Mean Pooling）</strong></p>
<p>mean pooling的前向传播就是把一个patch中的值求取平均来做pooling，那么反向传播的过程也就是把某个元素的梯度等分为n份分配给前一层，这样就保证池化前后的梯度（残差）之和保持不变，还是比较理解的，图示如下 </p>
<p><img src="https://img-blog.csdn.net/20170615205352655?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjExOTAwODE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p>
<p><strong>最大池化（Max Pooling）</strong></p>
<p>max pooling也要满足梯度之和不变的原则，max pooling的前向传播是把patch中最大的值传递给后一层，而其他像素的值直接被舍弃掉。那么反向传播也就是把梯度直接传给前一层某一个像素，而其他像素不接受梯度，也就是为0。所以max pooling操作和mean pooling操作不同点在于需要记录下池化操作时到底哪个像素的值是最大，也就是max id，这个可以看caffe源码的pooling_layer.cpp，下面是caffe框架max pooling部分的源码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// If max pooling, we will initialize the vector index part.</span><br><span class="line"></span><br><span class="line">if (this-&gt;layer_param_.pooling_param().pool() == PoolingParameter_PoolMethod_MAX &amp;&amp; top.size() == 1)</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">    max_idx_.Reshape(bottom[0]-&gt;num(), channels_, pooled_height_,pooled_width_);</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdn.net/20170615211413093?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjExOTAwODE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/35769417">如何理解CNN中的池化？</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21190081/article/details/72871704">深度学习笔记（3）——CNN中一些特殊环节的反向传播</a></li>
</ul>
<h3 id="池化层怎么接收后面传过来的损失？"><a href="#池化层怎么接收后面传过来的损失？" class="headerlink" title="池化层怎么接收后面传过来的损失？"></a>池化层怎么接收后面传过来的损失？</h3><p><strong>Relu函数的导数计算</strong></p>
<script type="math/tex; mode=display">
f(x)=\left\{ \begin{aligned} 1 ,x>0\\0,x<=0   \end{aligned} \right.</script><p><strong>池化层的反向传播</strong></p>
<p>池化层没有激活函数，可以将池化看成用线性激活函数，所以直接做一次逆池化的操作，将池化后的单元映射到原来特征图的位置。</p>
<h2 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h2><p>在卷积神经网络中，感受野的定义是 卷积神经网络每一层输出的特征图（feature map）上的像素点在<strong>原始图像</strong>上映射的区域大小。</p>
<h3 id="感受野计算"><a href="#感受野计算" class="headerlink" title="感受野计算"></a>感受野计算</h3><p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/objectDetect/p/5947169.html">卷积神经网络物体检测之感受野大小计算</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31004121">如何计算感受野(Receptive Field)——原理</a></li>
<li><a target="_blank" rel="noopener" href="https://distill.pub/2019/computing-receptive-fields/">Computing Receptive Fields of Convolutional Neural Networks</a></li>
</ul>
<h3 id="卷积神经网络的感受野"><a href="#卷积神经网络的感受野" class="headerlink" title="卷积神经网络的感受野"></a>卷积神经网络的感受野</h3><p>(N-1)_RF = f(N_RF, stride, kernel) = (N_RF - 1) * stride + kernel</p>
<p>其中，RF是感受野。N_RF和RF有点像，<strong>N代表 neighbour</strong>，指的是第n层的 a feature在n-1层的RF，<strong>记住N_RF只是一个中间变量</strong>，不要和<strong>RF</strong>混淆。 stride是步长，ksize是卷积核大小。</p>
<p><strong>这个公式就是卷积之后特征图计算(N-ksize)/stride+1的逆公式</strong></p>
<p><strong>与padding没有关系，感受野只是表示两者的映射关系，与原始图的大小无关！</strong></p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/44106492">卷积神经网络的感受野</a></li>
</ul>
<h2 id="权重初始化方法"><a href="#权重初始化方法" class="headerlink" title="权重初始化方法"></a>权重初始化方法</h2><p>权重初始化太小会造成网络崩溃，权重太大网络饱和，导致梯度消失。</p>
<h3 id="Xavier"><a href="#Xavier" class="headerlink" title="Xavier"></a>Xavier</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w &#x3D; np.random.randn(fan_in, fan_out) &#x2F; np.sqrt(fan_in)</span><br></pre></td></tr></table></figure>
<h3 id="Kaiming初始化"><a href="#Kaiming初始化" class="headerlink" title="Kaiming初始化"></a>Kaiming初始化</h3><p>如果使用ReLU激活函数，会造成一半左右的神经元消失</p>
<p>在权重初始化的时候<code>w = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in / 2)</code></p>
<h2 id="正则化方法"><a href="#正则化方法" class="headerlink" title="正则化方法"></a>正则化方法</h2><ol>
<li><p><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#1">参数范数惩罚</a></p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201126222217.png" alt></p>
<p>深度学习中只对网络权重θ添加约束，对偏置项不加约束。主要原因是偏置项一般需要较少的数据就能精确的拟合，不对其正则化也不会引起太大的方差。另外，正则化偏置参数反而可能会引起显著的欠拟合。</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#2">L2参数正则化</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#3">L1参数正则化</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#4">L1正则化和L2正则化的区别</a></p>
<script type="math/tex; mode=display">
L1范数 \ ||w||_1 = \sum_{i=1}^n|w_i|\\L2范数||w||_2 = \sqrt{\sum_{i=1}^nw_i^2}\\</script><p>L1为曼哈顿距离</p>
<p>L2为欧式距离</p>
<ul>
<li>损失函数</li>
</ul>
<p>L1(LAD绝对值损失函数)和L2(LSE最小平方误差)都可以用作损失函数，LAE的<strong>鲁棒性</strong>更强，因为LSE会将误差放大，对样本更加敏感，导致模型会为了拟合一些异常样本进行调整会牺牲一些正常样本的训练效果。</p>
<ul>
<li>正则化</li>
</ul>
<p>L1正则化更倾向于稀疏解，常用来做特征选择，一定程度上可以防止过拟合。</p>
<p>L2正则化主要是用来防止模型过拟合，直观上理解是对较大的参数进行惩罚。</p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201123194902.png" style="zoom: 67%;"></p>
<p>L1正则化输出稀疏，鲁棒性更强，但可能解不唯一，L2正则化有唯一解，计算相对容易</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#5">数据集增强</a></p>
<p>利用一些数据增广操作</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#6">噪音的鲁棒性</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#7">向输出目标注入噪声</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#8">半监督学习</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#9">多任务学习</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#10">提前终止</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#11">参数绑定和共享</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#12">稀疏表示</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#13">集成化方法</a></p>
<p>模型ensemble</p>
</li>
</ol>
<p><strong>参考资料</strong></p>
<ul>
<li>[ ] <a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin">正则化方法</a></li>
</ul>
<h2 id="Batch-Normalization（BN）"><a href="#Batch-Normalization（BN）" class="headerlink" title="Batch Normalization（BN）"></a>Batch Normalization（BN）</h2><h3 id="BN-原理"><a href="#BN-原理" class="headerlink" title="BN 原理"></a>BN 原理</h3><h4 id="为什么需要BN？"><a href="#为什么需要BN？" class="headerlink" title="为什么需要BN？"></a>为什么需要BN？</h4><p>传入一个模型的数据如果是独立同分布的，那么会加速模型的拟合以及简化训练等。因此需要对传入模型的数据进行<strong>白化</strong></p>
<p>白化一般有两个目的：</p>
<ol>
<li>去除特征之间的相关性（独立）</li>
<li>使所有的特征具有相同的均值和方差（同分布）</li>
</ol>
<p>典型的白化方法为PCA</p>
<p>但是白化的计算成本会很高，并且会改变分布，导致失去原有数据的表达能力，所以提出了Batch Normalization</p>
<h4 id="Internal-Covariate-Shift（ICS）"><a href="#Internal-Covariate-Shift（ICS）" class="headerlink" title="Internal Covariate Shift（ICS）"></a>Internal Covariate Shift（ICS）</h4><p>在深层神经网络中经过每一层之后的数据分布都会发生变化，通过叠加高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新所以会导致网络训练缓慢难以训练，为了训练好模型会非常谨慎的设定学习率，初始化权重等操作。</p>
<blockquote>
<p>大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如 transfer learning / domain adaptation 等。而 covariate shift 就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同，即：对所有</p>
<p><img src="https://www.zhihu.com/equation?tex=x%5Cin+%5Cmathcal%7BX%7D" alt="[公式]">,<img src="https://www.zhihu.com/equation?tex=P_s%28Y%7CX%3Dx%29%3DP_t%28Y%7CX%3Dx%29%5C%5C" alt="[公式]"></p>
<p>但是<img src="https://www.zhihu.com/equation?tex=P_s%28X%29%5Cne+P_t%28X%29%5C%5C" alt="[公式]">大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。</p>
</blockquote>
<p>带来的问题</p>
<p><strong>1）上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低</strong></p>
<p><strong>2）网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度</strong></p>
<p><strong>3）每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。</strong></p>
<h4 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h4><p>1.将每个特征进行normalization，让每个特征成为均值为0方差为1分布。</p>
<p>2.利用参数进行线性变换操作，让数据恢复表达能力</p>
<h4 id="BN的作用"><a href="#BN的作用" class="headerlink" title="BN的作用"></a>BN的作用</h4><p><strong>（1）BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度</strong></p>
<p><strong>（2）BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定</strong></p>
<p><strong>（3）BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题</strong></p>
<p><strong>（4）BN具有一定的正则化效果</strong></p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/34879333">Batch Normalization原理与实战</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33173246">BN</a></li>
</ul>
<h3 id="手写-BN"><a href="#手写-BN" class="headerlink" title="手写 BN"></a>手写 BN</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Batchnorm</span>(<span class="params">x, gamma, beta, bn_param</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># x_shape:[B, C, H, W]</span></span><br><span class="line">    running_mean = bn_param[<span class="string">&#x27;running_mean&#x27;</span>]</span><br><span class="line">    running_var = bn_param[<span class="string">&#x27;running_var&#x27;</span>]</span><br><span class="line">    results = <span class="number">0.</span></span><br><span class="line">    eps = <span class="number">1e-5</span></span><br><span class="line"></span><br><span class="line">    x_mean = np.mean(x, axis=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">    x_var = np.var(x, axis=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdims=True0)</span><br><span class="line">    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)</span><br><span class="line">    results = gamma * x_normalized + beta</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 因为在测试时是单个图片测试，这里保留训练时的均值和方差，用在后面测试时用</span></span><br><span class="line">    running_mean = momentum * running_mean + (<span class="number">1</span> - momentum) * x_mean</span><br><span class="line">    running_var = momentum * running_var + (<span class="number">1</span> - momentum) * x_var</span><br><span class="line"></span><br><span class="line">    bn_param[<span class="string">&#x27;running_mean&#x27;</span>] = running_mean</span><br><span class="line">    bn_param[<span class="string">&#x27;running_var&#x27;</span>] = running_var</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> results, bn_param</span><br></pre></td></tr></table></figure>
<h3 id="BN-可以防止过拟合么？为什么"><a href="#BN-可以防止过拟合么？为什么" class="headerlink" title="BN 可以防止过拟合么？为什么"></a>BN 可以防止过拟合么？为什么</h3><p>在Batch Normalization中，由于我们使用mini-batch的均值与方差作为对整体训练样本均值与方差的估计，尽管每一个batch中的数据都是从总体样本中抽样得到，但不同mini-batch的均值与方差会有所不同，这就为网络的学习过程中增加了随机噪音，与Dropout通过关闭神经元给网络训练带来噪音类似，在一定程度上对模型起到了正则化的效果。</p>
<p>另外，原作者通过也证明了网络加入BN后，可以丢弃Dropout，模型也同样具有很好的泛化效果。</p>
<h3 id="BN-有哪些参数？"><a href="#BN-有哪些参数？" class="headerlink" title="BN 有哪些参数？"></a>BN 有哪些参数？</h3><script type="math/tex; mode=display">
输入向量X = (x_1,x_2,x_3,...,x_d)
\\经过网络层有y = f(x)
\\经过BN之后对输入数据进行了简化的白化操作。\\
h = f(g·\frac{x-\mu}{\sigma}+b)</script><p>首先对原始数据进行去均值除方差的操作编程均值为0方差为1的分布</p>
<p>再用可学习的参数g，b将输入的数据变成均值为b，方差为g2的分布</p>
<p>加入g,b参数的原因是<strong>保证模型的表达能力不因规范化而下降</strong>。可以让上层的网络利用下层学习的结果进行学习。</p>
<p>在旧参数中，X的均值取决于下层神经网络的复杂关联；但在新参数中， Y 的均值仅由 b来确定，去除了与下层计算的密切耦合。新参数很容易通过梯度下降来学习，简化了神经网络的训练。</p>
<h3 id="BN-的反向传播推导"><a href="#BN-的反向传播推导" class="headerlink" title="BN 的反向传播推导"></a>BN 的反向传播推导</h3><p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201124155822.png" style="zoom:80%;"></p>
<p>我们可以看到，经过BN操作以后，权重的缩放值会被“抹去”，因此保证了输入数据分布稳定在一定范围内。另外，权重的缩放并不会影响到对$u$的梯度计算；并且当权重越大时，即$a$越大， $\frac{1}{a}$越小，意味着权重 $W$的梯度反而越小，这样BN就保证了梯度不会依赖于参数的scale，使得参数的更新处在更加稳定的状态。</p>
<p>因此，在使用Batch Normalization之后，抑制了参数微小变化随着网络层数加深被放大的问题，使得网络对参数大小的适应能力更强，此时我们可以设置较大的学习率而不用过于担心模型divergence的风险。</p>
<h3 id="BN-在训练和测试的区别？"><a href="#BN-在训练和测试的区别？" class="headerlink" title="BN 在训练和测试的区别？"></a>BN 在训练和测试的区别？</h3><p>训练时$\mu$和$\sigma$是从mini_batch的样本求得的均值和方差</p>
<p>在测试时,使用在训练时BN保留的每组mini-batch训练数据在网络中每一层的均值和方差，即用整个样本的均值和方差来对test数据进行归一化。</p>
<script type="math/tex; mode=display">
\mu_{test} = \mathbb{E}(\mu_{batch})
\\\sigma^2_{test} = \frac{m}{m-1}\mathbb{E}(\sigma^2_{test})</script><h2 id="Weight-Normalization（WN）"><a href="#Weight-Normalization（WN）" class="headerlink" title="Weight Normalization（WN）"></a>Weight Normalization（WN）</h2><h2 id="Layer-Normalization（LN）"><a href="#Layer-Normalization（LN）" class="headerlink" title="Layer Normalization（LN）"></a>Layer Normalization（LN）</h2><p>层规范化就是针对 BN 的上述不足而提出的。与 BN 不同，LN 是一种横向的规范化，如图所示。它综合考虑一层所有维度的输入，计算该层的平均输入值和输入方差，然后用同一个规范化操作来转换各个维度的输入。</p>
<script type="math/tex; mode=display">
\mu = \sum_i\frac{1}{H}x_i, \ \sigma = \sqrt{\frac{1}{H}\sum_i(x_i-\mu)^2+\epsilon}</script><p>即将该层所有输入的神经元进行归一化，将所有特征一起考虑，如果特征差距过大则会降低模型的表现能力</p>
<p>LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差</p>
<p>LN主要用在RNN中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ln</span>(<span class="params">x, b, s</span>):</span></span><br><span class="line">    _eps = <span class="number">1e-5</span></span><br><span class="line">    output = (x - x.mean(<span class="number">1</span>)[:,<span class="literal">None</span>]) / tensor.sqrt((x.var(<span class="number">1</span>)[:,<span class="literal">None</span>] + _eps))</span><br><span class="line">    output = s[<span class="literal">None</span>, :] * output + b[<span class="literal">None</span>,:]</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h2 id="Instance-Normalization（IN）"><a href="#Instance-Normalization（IN）" class="headerlink" title="Instance Normalization（IN）"></a>Instance Normalization（IN）</h2><p>IN对输入的每个图像实例进行归一化</p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201124203005.png" style="zoom:80%;"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Instancenorm</span>(<span class="params">x, gamma, beta</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># x_shape:[B, C, H, W]</span></span><br><span class="line">    results = <span class="number">0.</span></span><br><span class="line">    eps = <span class="number">1e-5</span></span><br><span class="line"></span><br><span class="line">    x_mean = np.mean(x, axis=(<span class="number">2</span>, <span class="number">3</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">    x_var = np.var(x, axis=(<span class="number">2</span>, <span class="number">3</span>), keepdims=True0)</span><br><span class="line">    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)</span><br><span class="line">    results = gamma * x_normalized + beta</span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure>
<h2 id="Group-Normalization（GN）"><a href="#Group-Normalization（GN）" class="headerlink" title="Group Normalization（GN）"></a>Group Normalization（GN）</h2><p>主要是针对Batch Normalization对小batchsize效果差，GN将channel方向分group，然后每个group内做归一化，算(C//G)<em>H</em>W的均值，这样与batchsize无关，不受其约束。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GroupNorm</span>(<span class="params">x, gamma, beta, G=<span class="number">16</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># x_shape:[B, C, H, W]</span></span><br><span class="line">    results = <span class="number">0.</span></span><br><span class="line">    eps = <span class="number">1e-5</span></span><br><span class="line">    x = np.reshape(x, (x.shape[<span class="number">0</span>], G, x.shape[<span class="number">1</span>]/<span class="number">16</span>, x.shape[<span class="number">2</span>], x.shape[<span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line">    x_mean = np.mean(x, axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">    x_var = np.var(x, axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), keepdims=True0)</span><br><span class="line">    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)</span><br><span class="line">    results = gamma * x_normalized + beta</span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure>
<h3 id="BN、LN、WN、IN和GN的区别"><a href="#BN、LN、WN、IN和GN的区别" class="headerlink" title="BN、LN、WN、IN和GN的区别"></a>BN、LN、WN、IN和GN的区别</h3><p>将输入的图像shape记为[N, C, H, W]，这几个方法主要的区别就是在，</p>
<ul>
<li>batchNorm是在batch上，对NHW做归一化，对小batchsize效果不好；</li>
<li>layerNorm在通道方向上，对CHW归一化，主要对RNN作用明显；</li>
<li>instanceNorm在图像像素上，对HW做归一化，用在风格化迁移；</li>
<li>GroupNorm将channel分组，然后再做归一化；</li>
<li>SwitchableNorm是将BN、LN、IN结合，赋予权重，让网络自己去学习归一化层应该使用什么方法。</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/liuxiao214/article/details/81037416">BatchNormalization、LayerNormalization、InstanceNorm、GroupNorm、SwitchableNorm总结</a></li>
</ul>
<h2 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h2><p>见<a href="https://www.browallia.top/2019/10/21/CS231n-note-3/">神经网络优化</a></p>
<ul>
<li>随机梯度下降（SGD）</li>
<li>Mini-Batch</li>
<li>动量（Momentum）</li>
<li>Nesterov 动量</li>
<li>AdaGrad</li>
<li>AdaDelta</li>
<li>RMSProp</li>
<li>Adam</li>
<li>Adamax</li>
<li>Nadam</li>
<li><a target="_blank" rel="noopener" href="http://ruder.io/optimizing-gradient-descent/index.html#amsgrad">AMSGrad</a></li>
<li>AdaBound</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://exacity.github.io/deeplearningbook-chinese/Chapter8_optimization_for_training_deep_models/">《Deep Learning》第八章：深度模型中的优化</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32626442">从 SGD 到 Adam —— 深度学习优化算法概览(一)</a></p>
</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/37269222">Adam 究竟还有什么问题 —— 深度学习优化算法概览(二)</a></li>
<li><a target="_blank" rel="noopener" href="http://ruder.io/optimizing-gradient-descent/">An overview of gradient descent optimization algorithms</a></li>
</ul>
<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>在梯度下降中，对于$\theta$的更新，需要计算所有的样本然后求平均.其计算得到的是一个标准梯度。因而理论上来说一次更新的幅度是比较大的。</p>
<h3 id="mini-batch梯度下降法"><a href="#mini-batch梯度下降法" class="headerlink" title="mini-batch梯度下降法"></a>mini-batch梯度下降法</h3><p>为了克服SGD这种随机性带来的缺点，保留其优点。Mini-batch gradient descent 每次随机选择m个数据样本进行梯度下降的参数更新。m的取值可以是2，4，8，16，32，64，256等。这样做的好处是抵消了一部分随机性，又不会花费太多的时间。而且也能够在online streaming数据流的状态下使用。只要每次收集完m个数据后开始更新即可。</p>
<p>Batch size大，收敛速度会比较慢，因为参数每次更新所需要的样本量增加了，但是会沿着比较准确的方向进行。</p>
<h3 id="随机梯度下降法（SGD）"><a href="#随机梯度下降法（SGD）" class="headerlink" title="随机梯度下降法（SGD）"></a>随机梯度下降法（SGD）</h3><h4 id="SGD每步做什么，为什么能online-learning？"><a href="#SGD每步做什么，为什么能online-learning？" class="headerlink" title="SGD每步做什么，为什么能online learning？"></a>SGD每步做什么，为什么能online learning？</h4><ul>
<li>只对一个方向的敏感度高，会在不敏感的方向反复增减。</li>
<li>会找到局部极小值或者鞍点(梯度为零)，在高维参数空间中，局部最小值不常见，常见的是鞍点。</li>
<li>随机性，因为SGD使用的是minibatch(=1)，会产生噪声，如果在梯度下降时加入噪声会花费很长的时间</li>
</ul>
<p>online learning强调的是学习是实时的，流式的，每次训练不用使用全部样本，而是以之前训练好的模型为基础，每来一个样本就更新一次模型，这种方法叫做OGD（online gradient descent）。</p>
<h3 id="动量梯度下降法（Momentum）"><a href="#动量梯度下降法（Momentum）" class="headerlink" title="动量梯度下降法（Momentum）"></a>动量梯度下降法（Momentum）</h3><ul>
<li>在局部最优点或者鞍点时，梯度为0，但依旧会有一个速度，能够越过这个点继续进行梯度下降。</li>
<li>加入动量之后，噪声会被抵消，下降曲线更平滑。</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/77503211">pytorch中的SGD</a></li>
</ul>
<h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><script type="math/tex; mode=display">
S_{dW}=\beta S_{dW}+\left ( 1-\beta  \right )dW^{2}</script><script type="math/tex; mode=display">
S_{db}=\beta S_{db}+\left ( 1-\beta  \right )db^{2}</script><script type="math/tex; mode=display">
W=W-\alpha\frac{dW}{\sqrt{S_{dW}}}, b=b-\alpha\frac{db}{\sqrt{S_{db}}}</script><ul>
<li>[ ] TODO</li>
</ul>
<h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><ul>
<li>[ ] TODO</li>
</ul>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>Adam算法结合了Momentum和RMSprop梯度下降法，是一种极其常见的学习算法，被证明能有效适用于不同神经网络，适用于广泛的结构。</p>
<script type="math/tex; mode=display">
v_{dW}=\beta_{1} v_{dW}+\left ( 1-\beta_{1}  \right )dW</script><script type="math/tex; mode=display">
v_{db}=\beta_{1} v_{db}+\left ( 1-\beta_{1}  \right )db</script><script type="math/tex; mode=display">
S_{dW}=\beta_{2} S_{dW}+\left ( 1-\beta_{2}  \right )dW^{2}</script><script type="math/tex; mode=display">
S_{db}=\beta_{2} S_{db}+\left ( 1-\beta_{2}  \right )db^{2}</script><script type="math/tex; mode=display">
v_{dW}^{corrected}=\frac{v_{dW}}{1-\beta_{1}^{t}}</script><script type="math/tex; mode=display">
v_{db}^{corrected}=\frac{v_{db}}{1-\beta_{1}^{t}}</script><script type="math/tex; mode=display">
S_{dW}^{corrected}=\frac{S_{dW}}{1-\beta_{2}^{t}}</script><script type="math/tex; mode=display">
S_{db}^{corrected}=\frac{S_{db}}{1-\beta_{2}^{t}}</script><script type="math/tex; mode=display">
W:=W-\frac{av_{dW}^{corrected}}{\sqrt{S_{dW}^{corrected}}+\varepsilon }</script><p>超参数：</p>
<script type="math/tex; mode=display">
\alpha ,\beta _{1},\beta_{2},\varepsilon</script><script type="math/tex; mode=display">
\alpha ,\beta _{1},\beta_{2},\varepsilon</script><ul>
<li>[ ] TODO </li>
</ul>
<h4 id="Adam-优化器的迭代公式"><a href="#Adam-优化器的迭代公式" class="headerlink" title="Adam 优化器的迭代公式"></a>Adam 优化器的迭代公式</h4><ul>
<li>[ ] TODO</li>
</ul>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>每个激活函数（或非线性函数）的输入都是一个数字，然后对其进行某种固定的数学操作。</p>
<p>见<a href="https://www.browallia.top/2019/10/15/CS231n-note-2/">2.1激活函数</a></p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://yogayu.github.io/DeepLearningCourse/03/ActivateFunction.html">What is activate function?</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/7DgiXCNBS5vb07WIKTFYRQ">资源 | 从ReLU到Sinc，26种神经网络激活函数可视化</a></li>
</ul>
<h3 id="Sigmoid-amp-Tanh"><a href="#Sigmoid-amp-Tanh" class="headerlink" title="Sigmoid&amp;Tanh"></a>Sigmoid&amp;Tanh</h3><p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201124212701.png" alt></p>
<script type="math/tex; mode=display">
sigmoid(x) = \frac{1}{1+e^{-x}}\\
tanh(x) = 2sigmoid(2x)-1</script><p>sigmod函数饱和使梯度消失，并且是非零中心的</p>
<p>tanh也有饱和问题，但是是零中心的</p>
<h4 id="Sigmoid用作激活函数时，分类为什么要用交叉熵损失，而不用均方损失？"><a href="#Sigmoid用作激活函数时，分类为什么要用交叉熵损失，而不用均方损失？" class="headerlink" title="Sigmoid用作激活函数时，分类为什么要用交叉熵损失，而不用均方损失？"></a>Sigmoid用作激活函数时，分类为什么要用交叉熵损失，而不用均方损失？</h4><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_29679623/article/details/99441913">https://blog.csdn.net/qq_29679623/article/details/99441913</a></p>
<p>当使用sigmoid作为激活函数的时候，常用<strong>交叉熵损失函数</strong>而不用<strong>均方误差损失函数</strong>，因为它可以<strong>完美解决平方损失函数权重更新过慢</strong>的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。</p>
<h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><script type="math/tex; mode=display">
ReLU(x) = max(0,x)</script><p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201124213722.png" alt></p>
<p>ReLU 相关变体</p>
<h4 id="ReLU-激活函数为什么比sigmoid和tanh好？"><a href="#ReLU-激活函数为什么比sigmoid和tanh好？" class="headerlink" title="ReLU 激活函数为什么比sigmoid和tanh好？"></a>ReLU 激活函数为什么比sigmoid和tanh好？</h4><ol>
<li>可以有效解决饱和问题带来的梯度消失问题</li>
<li>计算简单，导数为常数收敛速度快</li>
<li>Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生</li>
</ol>
<h4 id="ReLU-激活函数为什么能解决梯度消失问题？"><a href="#ReLU-激活函数为什么能解决梯度消失问题？" class="headerlink" title="ReLU 激活函数为什么能解决梯度消失问题？"></a>ReLU 激活函数为什么能解决梯度消失问题？</h4><p>从信号方面来看，即神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，更好更快地提取稀疏特征。当 x<0 时，relu 硬饱和，而当 x>0 时，则不存在饱和问题。ReLU 能够在 x&gt;0 时保持梯度不衰减，从而缓解梯度消失问题。</0></p>
<h4 id="ReLU-有哪些变体？"><a href="#ReLU-有哪些变体？" class="headerlink" title="ReLU 有哪些变体？"></a>ReLU 有哪些变体？</h4><p>Leaky ReLU PReLU ELU</p>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><h3 id="Dropout-基本原理"><a href="#Dropout-基本原理" class="headerlink" title="Dropout 基本原理"></a>Dropout 基本原理</h3><p>解决过拟合和模型耗时问题，随机选择部分神经元失活</p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/stdcoutzyx/article/details/49022443">理解dropout</a></li>
</ul>
<h3 id="Dropout-如何实现？"><a href="#Dropout-如何实现？" class="headerlink" title="Dropout 如何实现？"></a>Dropout 如何实现？</h3><ul>
<li>[ ] TODO</li>
</ul>
<h3 id="Drop-在训练和测试的区别"><a href="#Drop-在训练和测试的区别" class="headerlink" title="Drop 在训练和测试的区别"></a>Drop 在训练和测试的区别</h3><p>在测试的时候每个单元的参数需要乘p</p>
<h2 id="损失函数（Loss）"><a href="#损失函数（Loss）" class="headerlink" title="损失函数（Loss）"></a>损失函数（Loss）</h2><p>在机器学习中，损失函数（loss function）是用来估量模型的预测值f(x)与真实值Y的不一致程度，损失函数越小，一般就代表模型的鲁棒性越好，正是损失函数指导了模型的学习。</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/58883095">常见的损失函数</a></p>
<h3 id="MSE"><a href="#MSE" class="headerlink" title="MSE"></a>MSE</h3><p><a target="_blank" rel="noopener" href="https://rohanvarma.me/Loss-Functions/">参考</a></p>
<script type="math/tex; mode=display">
loss = \frac{1}{N}\sum_{i=1}^N(y_i-y_{predict_i})^2</script><p>均方误差主要用于回归，不经常用于分类</p>
<p>使用MSE的一个缺点就是其偏导值在输出概率值接近0或者接近1的时候非常小，这可能会造成模型刚开始训练时，偏导值几乎消失。</p>
<blockquote>
<p>如果假设您的输出是输入的实值函数，并且具有一定量的不可约高斯噪声，并且均值和方差恒定，那么使用MSE损失才有意义。 如果这些假设不成立（例如在分类的情况下），那么MSE亏损可能不是最好的选择。</p>
</blockquote>
<h3 id="Cross-Entropy-Loss（CE）"><a href="#Cross-Entropy-Loss（CE）" class="headerlink" title="Cross Entropy Loss（CE）"></a>Cross Entropy Loss（CE）</h3><script type="math/tex; mode=display">
二分类
\\loss=-\frac{1}{n}\sum_x[ylna+(1-y)ln(1-a)]
\\多分类
\\loss = -\frac{1}{n}\sum_iy_ilna_i</script><p>log损失函数是凸函数能够求得全局最优</p>
<p>如果用 MSE 计算 loss， 输出的曲线是波动的，有很多局部的极值点。 即，非凸优化问题 (non-convex)</p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201126163847.png" alt></p>
<script type="math/tex; mode=display">
\frac{\partial L_i}{\partial w_i} = \frac{1}{N}\frac{\partial L_i}{\partial w_i}=\frac{1}{N}\frac{\partial L_i}{\partial p_i}\frac{\partial p_i}{\partial s_i}\frac{\partial s_i}{\partial w_i}</script><p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201126170739.png" style="zoom:80%;"></p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201126170817.png" style="zoom: 80%;"></p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201126171024.png" style="zoom: 80%;"></p>
<p>并且在反向传播时CE可以很容易的进行计算</p>
<p><strong>优点</strong></p>
<p>在用梯度下降法做参数更新的时候，模型学习的速度取决于两个值：一、<strong>学习率</strong>；二、<strong>偏导值</strong>。其中，学习率是我们需要设置的超参数，所以我们重点关注偏导值。从上面的式子中，我们发现，偏导值的大小取决于$x_i$ 和 $[\sigma(s)-y]$ ，我们重点关注后者，后者的大小值反映了我们模型的错误程度，该值越大，说明模型效果越差，但是该值越大同时也会使得偏导值越大，从而模型学习速度更快。所以，使用逻辑函数得到概率，并结合交叉熵当损失函数时，在模型效果差的时候学习速度比较快，在模型效果好的时候学习速度变慢。</p>
<p><strong>缺点</strong></p>
<p>sigmoid(softmax)+cross-entropy loss 擅长于学习类间的信息，因为它采用了类间竞争机制，它只关心对于正确标签预测概率的准确性，忽略了其他非正确标签的差异，导致学习到的特征比较散。基于这个问题的优化有很多，比如对softmax进行改进，如L-Softmax、SM-Softmax、AM-Softmax等。</p>
<p><strong>pytorch中的CE Loss</strong></p>
<p>pytorch中的CE Loss是由softmax-log-NLL Loss(将label对应位置的值拿出来去掉负号求均值)组合而成的</p>
<h3 id="Hinge-Loss"><a href="#Hinge-Loss" class="headerlink" title="Hinge Loss"></a>Hinge Loss</h3><p>在机器学习中，<strong>hinge loss</strong>是一种损失函数，它通常用于”maximum-margin”的分类任务中，如支持向量机。数学表达式为：</p>
<script type="math/tex; mode=display">
L(y) = max(0, 1-y\hat{y})</script><h3 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/80594704">参考资料</a></p>
<p>Focal Loss的引入主要是为了解决<strong>难易样本数量不平衡（注意，有区别于正负样本数量不平衡）</strong>的问题。</p>
<p>在普通的CE loss加上一个参数$\alpha$可以解决正负样本不平衡的问题。</p>
<script type="math/tex; mode=display">
CE = \begin{cases}-\alpha logp, \ y=1\\ -(1-\alpha)log(1-p), \ y=0\end{cases}</script><p>即减小负样本的权重</p>
<p>但是这样并不能解决难易样本不平衡的问题</p>
<p>样本可以分为四类</p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201126195937.png" alt></p>
<p><strong>尽管</strong> $\alpha$<strong>平衡了正负样本，但对难易样本的不平衡没有任何帮助。</strong>而实际上，目标检测中大量的候选目标都是像下图一样的易分样本。</p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201126200114.png" style="zoom:67%;"></p>
<p>这些样本的损失很低，但是由于数量极不平衡，易分样本的数量相对来讲太多，最终主导了总的损失。而本文的作者认为，<strong>易分样本（即，置信度高的样本）对模型的提升效果非常小，模型应该主要关注与那些难分样本</strong>（<strong>这个假设是有问题的，是GHM的主要改进对象</strong>）</p>
<p>一个简单的思想：<strong>把高置信度(p)样本的损失降低</strong></p>
<script type="math/tex; mode=display">
CE = \begin{cases}-\alpha (1-p)^\gamma logp, \ y=1\\ -(1-\alpha)p^\gamma log(1-p), \ y=0\end{cases}</script><p>实验表明$\gamma$取2, $\alpha$取0.25的时候效果最佳。</p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201126200453.png" alt></p>
<p>在pytorch中</p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201126200856.png" style="zoom:67%;"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">py_sigmoid_focal_loss</span>(<span class="params">pred,</span></span></span><br><span class="line"><span class="function"><span class="params">                          target,</span></span></span><br><span class="line"><span class="function"><span class="params">                          weight=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                          gamma=<span class="number">2.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                          alpha=<span class="number">0.25</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                          reduction=<span class="string">&#x27;mean&#x27;</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                          avg_factor=None</span>):</span></span><br><span class="line">    pred_sigmoid = pred.sigmoid()</span><br><span class="line">    target = target.type_as(pred)</span><br><span class="line">    pt = (<span class="number">1</span> - pred_sigmoid) * target + pred_sigmoid * (<span class="number">1</span> - target)</span><br><span class="line">    focal_weight = (alpha * target + (<span class="number">1</span> - alpha) *</span><br><span class="line">                    (<span class="number">1</span> - target)) * pt.pow(gamma)</span><br><span class="line">    loss = F.binary_cross_entropy_with_logits(</span><br><span class="line">        pred, target, reduction=<span class="string">&#x27;none&#x27;</span>) * focal_weight</span><br><span class="line">    loss = weight_reduce_loss(loss, weight, reduction, avg_factor)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h2 id="1-1-卷积有什么作用？"><a href="#1-1-卷积有什么作用？" class="headerlink" title="1*1 卷积有什么作用？"></a>1*1 卷积有什么作用？</h2><ol>
<li>调整特征图的深度进行降维或者升维</li>
<li>减少参数</li>
<li><strong>跨通道的信息组合，并增加了非线性特征</strong></li>
</ol>
<p>使用1<em>1卷积核，实现降维和升维的操作其实就是channel间信息的线性组合变化，3</em>3，64channels的卷积核前面添加一个1<em>1，28channels的卷积核，就变成了3</em>3，28channels的卷积核，原来的64个channels就可以理解为跨通道线性组合变成了28channels，这就是通道间的信息交互。因为1*1卷积核，可以在保持feature map尺度不变的（即不损失分辨率）的前提下大幅增加非线性特性（利用后接的非线性激活函数），把网络做的很deep，增加非线性特性。</p>
<h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><ul>
<li>使用ReLU激活函数</li>
<li>Dropout</li>
<li>数据增广</li>
</ul>
<p>先给出AlexNet的一些参数和结构图： </p>
<p>卷积层：5层 </p>
<p>全连接层：3层 </p>
<p>深度：8层 </p>
<p>参数个数：60M </p>
<p>神经元个数：650k </p>
<p>分类数目：1000类</p>
<p><strong>参考资料</strong></p>
<p><a target="_blank" rel="noopener" href="https://dgschwend.github.io/netscope/#/preset/alexnet">AlexNet</a></p>
<h2 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h2><p><strong>《Very Deep Convolutional Networks for Large-Scale Image Recognition》</strong></p>
<ul>
<li>arXiv：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</a></li>
<li>intro：ICLR 2015</li>
<li>homepage：<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/">http://www.robots.ox.ac.uk/~vgg/research/very_deep/</a></li>
</ul>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.1556">VGG</a> 是Oxford的<strong>V</strong>isual <strong>G</strong>eometry <strong>G</strong>roup的组提出的（大家应该能看出VGG名字的由来了）。该网络是在ILSVRC 2014上的相关工作，主要工作是证明了增加网络的深度能够在一定程度上影响网络最终的性能。VGG有两种结构，分别是VGG16和VGG19，两者并没有本质上的区别，只是网络深度不一样。</p>
<p>VGG16相比AlexNet的一个改进是采用<strong>连续的几个3x3的卷积核代替AlexNet中的较大卷积核（11x11，7x7，5x5）</strong>。对于给定的感受野（与输出有关的输入图片的局部大小），<strong>采用堆积的小卷积核是优于采用大的卷积核，因为多层非线性层可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）</strong>。</p>
<p>简单来说，在VGG中，使用了3个3x3卷积核来代替7x7卷积核，使用了2个3x3卷积核来代替5*5卷积核，这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。</p>
<p>比如，3个步长为1的3x3卷积核的一层层叠加作用可看成一个大小为7的感受野（其实就表示3个3x3连续卷积相当于一个7x7卷积），其参数总量为 3x(9xC^2) ，如果直接使用7x7卷积核，其参数总量为 49xC^2 ，这里 C 指的是输入和输出的通道数。很明显，27xC^2小于49xC^2，即减少了参数；而且3x3卷积核有利于更好地保持图像性质。</p>
<p>这里解释一下为什么使用2个3x3卷积核可以来代替5*5卷积核：</p>
<p>5x5卷积看做一个小的全连接网络在5x5区域滑动，我们可以先用一个3x3的卷积滤波器卷积，然后再用一个全连接层连接这个3x3卷积输出，这个全连接层我们也可以看做一个3x3卷积层。这样我们就可以用两个3x3卷积级联（叠加）起来代替一个 5x5卷积。</p>
<p>具体如下图所示：</p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201126203259.png" alt></p>
<p>至于为什么使用3个3x3卷积核可以来代替7*7卷积核，推导过程与上述类似，大家可以自行绘图理解。</p>
<p>下面是VGG网络的结构（VGG16和VGG19都在）：</p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201126203428.png" alt></p>
<ul>
<li>VGG16包含了16个隐藏层（13个卷积层和3个全连接层），如上图中的D列所示</li>
<li>VGG19包含了19个隐藏层（16个卷积层和3个全连接层），如上图中的E列所示</li>
</ul>
<p>VGG网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的max pooling。</p>
<p>如果你想看到更加形象化的VGG网络，可以使用<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/gktWxh1p2rR2Jz-A7rs_UQ">经典卷积神经网络（CNN）结构可视化工具</a>来查看高清无码的<a target="_blank" rel="noopener" href="https://dgschwend.github.io/netscope/#/preset/vgg-16">VGG网络</a>。</p>
<p><strong>VGG优点：</strong></p>
<ul>
<li>VGGNet的结构非常简洁，整个网络都使用了同样大小的卷积核尺寸（3x3）和最大池化尺寸（2x2）。</li>
<li>几个小滤波器（3x3）卷积层的组合比一个大滤波器（5x5或7x7）卷积层好：</li>
<li>验证了通过不断加深网络结构可以提升性能。</li>
</ul>
<p><strong>VGG缺点</strong>：</p>
<p>VGG耗费更多计算资源，并且使用了更多的参数（这里不是3x3卷积的锅），导致更多的内存占用（140M）。其中绝大多数的参数都是来自于第一个全连接层。VGG可是有3个全连接层啊！</p>
<p>PS：有的文章称：发现这些全连接层即使被去除，对于性能也没有什么影响，这样就显著降低了参数数量。</p>
<p>注：很多pretrained的方法就是使用VGG的model（主要是16和19），VGG相对其他的方法，参数空间很大，最终的model有500多m，AlexNet只有200m，GoogLeNet更少，所以train一个vgg模型通常要花费更长的时间，所幸有公开的pretrained model让我们很方便的使用。</p>
<p>关于感受野：</p>
<p>假设你一层一层地重叠了3个3x3的卷积层（层与层之间有非线性激活函数）。在这个排列下，第一个卷积层中的每个神经元都对输入数据体有一个3x3的视野。</p>
<p><strong>代码篇：VGG训练与测试</strong></p>
<p>这里推荐两个开源库，训练请参考<a target="_blank" rel="noopener" href="https://github.com/machrisaa/tensorflow-vgg">tensorflow-vgg</a>，快速测试请参考<a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~frossard/post/vgg16/">VGG-in TensorFlow</a>。</p>
<p>代码我就不介绍了，其实跟上述内容一致，跟着原理看code应该会很快。我快速跑了一下<a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~frossard/post/vgg16/">VGG-in TensorFlow</a>，代码亲测可用，效果很nice，就是model下载比较烦。</p>
<p>贴心的Amusi已经为你准备好了<a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~frossard/post/vgg16/">VGG-in TensorFlow</a>的测试代码、model和图像。需要的同学可以关注CVer微信公众号，后台回复：VGG。</p>
<p>天道酬勤，还有很多知识要学，想想都刺激~Fighting！</p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.1556">《Very Deep Convolutional Networks for Large-Scale Image Recognition》</a></li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/wcy12341189/article/details/56281618">深度网络VGG理解</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/marsjhao/article/details/72955935">深度学习经典卷积神经网络之VGGNet</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://dgschwend.github.io/netscope/#/preset/vgg-16">VGG16 结构可视化</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/machrisaa/tensorflow-vgg">tensorflow-vgg</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~frossard/post/vgg16/">VGG-in TensorFlow</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/23518167">机器学习进阶笔记之五 | 深入理解VGG\Residual Network</a></p>
</li>
</ul>
<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p><strong>1.ResNet意义</strong></p>
<p>随着网络的加深，出现了训练集准确率下降的现象，我们可以确定这不是由于Overfit过拟合造成的(过拟合的情况训练集应该准确率很高)；所以作者针对这个问题提出了一种全新的网络，叫深度残差网络，它允许网络尽可能的加深，其中引入了全新的结构如图1；<br>这里问大家一个问题 </p>
<p>残差指的是什么？ </p>
<p>其中ResNet提出了两种mapping：一种是identity mapping，指的就是图1中”弯弯的曲线”，另一种residual mapping，指的就是除了”弯弯的曲线“那部分，所以最后的输出是 y=F(x)+x<br>identity mapping顾名思义，就是指本身，也就是公式中的x，而residual mapping指的是“差”，也就是y−x，所以残差指的就是F(x)部分。 </p>
<p>为什么ResNet可以解决“随着网络加深，准确率不下降”的问题？ </p>
<p>理论上，对于“随着网络加深，准确率下降”的问题，Resnet提供了两种选择方式，也就是identity mapping和residual mapping，如果网络已经到达最优，继续加深网络，residual mapping将被push为0，只剩下identity mapping，这样理论上网络一直处于最优状态了，网络的性能也就不会随着深度增加而降低了。</p>
<p>可以解决网络退化问题，缓解梯度消失的问题。</p>
<p><strong>2.ResNet结构</strong></p>
<p>它使用了一种连接方式叫做“shortcut connection”，顾名思义，shortcut就是“抄近道”的意思，看下图我们就能大致理解：</p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201126204308.png" style="zoom:67%;"> </p>
<p><strong>ResNet的F(x)究竟长什么样子？</strong></p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/lanran2/article/details/79057994">ResNet解析</a></li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/wspba/article/details/56019373">ResNet论文笔记</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/e58437f39f65">残差网络ResNet笔记</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.waya.ai/deep-residual-learning-9610bb62c355">Understand Deep Residual Networks — a simple, modular learning framework that has redefined state-of-the-art</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035">An Overview of ResNet and its Variants</a>     </p>
</li>
</ul>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/46d76bd56766">译文</a></li>
<li><a target="_blank" rel="noopener" href="https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624">Understanding and Implementing Architectures of ResNet and ResNeXt for state-of-the-art Image Classification: From Microsoft to Facebook [Part 1]</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/28413039">给妹纸的深度学习教学(4)——同Residual玩耍</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32173684">Residual Networks 理解</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32206896">Identity Mapping in ResNet</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/53224378">resnet（残差网络）的F（x）究竟长什么样子？</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/64494691">ResNet到底在解决一个什么问题呢？</a></li>
</ul>
<p><strong>网络的深度为什么重要？</strong></p>
<p>因为CNN能够提取low/mid/high-level的特征，网络的层数越多，意味着能够提取到不同level的特征越丰富。并且，越深的网络提取的特征越抽象，越具有语义信息。</p>
<p><strong>为什么不能简单地增加网络层数？</strong></p>
<ul>
<li><p>对于原来的网络，如果简单地增加深度，会导致梯度弥散或梯度爆炸。</p>
</li>
<li><p>会出现另一个问题，就是<strong>退化问题</strong>，网络层数增加，但是在训练集上的准确率却饱和甚至下降了。这个不能解释为overfitting，因为overfit应该表现为在训练集上表现更好才对。退化问题说明了深度网络不能很简单地被很好地优化。</p>
<blockquote>
<p>F是求和前网络映射，H是从输入到求和后的网络映射。</p>
<p>比如把5映射到5.1，</p>
<p>那么引入残差前是F’(5)=5.1，</p>
<p>引入残差后是H(5)=5.1, H(5)=F(5)+5, F(5)=0.1。</p>
<p>这里的F’和F都表示网络参数映射，引入残差后的映射对输出的变化更敏感。比如原来是从5.1到5.2，映射F’的输出增加了1/51=2%，而对于残差结构从5.1到5.2，映射F是从0.1到0.2，增加了100%。明显后者输出变化对权重的调整作用更大，所以效果更好。</p>
<p>残差的思想都是去掉相同的主体部分，从而突出微小的变化，看到残差网络我第一反应就是差分放大器…</p>
</blockquote>
<p><img src="https://pic4.zhimg.com/80/v2-6c0aa95bc2bdc174b92453621c857fa7_720w.jpg" alt="img"></p>
</li>
</ul>
<h3 id="ResNet为什么不用Dropout"><a href="#ResNet为什么不用Dropout" class="headerlink" title="ResNet为什么不用Dropout?"></a>ResNet为什么不用Dropout?</h3><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/325139089">https://www.zhihu.com/question/325139089</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/60923972">https://zhuanlan.zhihu.com/p/60923972</a></li>
</ul>
<h3 id="为什么-ResNet-不在一开始就使用residual-block-而是使用一个7×7的卷积？"><a href="#为什么-ResNet-不在一开始就使用residual-block-而是使用一个7×7的卷积？" class="headerlink" title="为什么 ResNet 不在一开始就使用residual block,而是使用一个7×7的卷积？"></a>为什么 ResNet 不在一开始就使用residual block,而是使用一个7×7的卷积？</h3><p>原因: 7x7卷积实际上是用来直接对<strong>输入图片</strong>降采样(early downsampling), 注意像7x7这样的大卷积核一般只出现在<strong>input layer</strong></p>
<p><strong>目的是:</strong>  尽可能<strong>保留原始图像的信息,</strong> 而不需要增加channels数.</p>
<p><strong>本质上是:</strong> 多channels的非线性激活层是非常昂贵的, 在<strong>input laye</strong>r用<strong>big kernel</strong>换多channels是划算的</p>
<p>注意一下, resnet接入residual block前pixel为56x56的layer, channels数才<strong>64</strong>, 但是同样大小的layer, 在vgg-19里已经有<strong>256</strong>个channels了.</p>
<p>这里要强调一下, 只有在input layer层, 也就是<strong>最靠近输入图片</strong>的那层, 才用大卷积, 原因如下:</p>
<p>深度学习领域, 有一种广泛的直觉，即更大的卷积更好，但更昂贵。输入层中的特征数量(224x224)是如此之小（相对于隐藏层），第一卷积可以非常大而不会大幅增加实际的权重数。<strong>如果你想在某个地方进行大卷积，第一层通常是唯一的选择</strong>。</p>
<p>我认为神经网络的第一层是最基本的，因为它基本上只是将数据嵌入到一个新的更大的向量空间中。ResNet在第二层之前没有开始其特征层跳过，所以看起来作者想要在开始整花里胡哨的layers之前尽可能保留图像里更多的primary features.</p>
<p>题外话, 同时期的GoogLeNet也在input layer用到了7x7大卷积, 所以resnet作者的灵感来源于GoogLeNet也说不定, 至于非要追问为啥这么用, 也许最直接的理由就是”深度学习就像炼丹, 因为这样网络工作得更好, 所以作者就这么用了”. </p>
<p>再说个有趣的例子, resnet模型是实验先于理论, 实验证明有效, 后面才陆续有人研究为啥有效, 比如<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1702.08591">The Shattered Gradients Problem: If resnets are the answer, then what is the question?</a>  可不就是炼丹么?</p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/330735327/answer/725695411">为什么resnet不在一开始就使用residual block,而是使用一个7×7的卷积？</a></li>
</ul>
<h3 id="什么是Bottlenet-layer？"><a href="#什么是Bottlenet-layer？" class="headerlink" title="什么是Bottlenet layer？"></a>什么是Bottlenet layer？</h3><p>对于常规ResNet，可以用于34层或者更少的网络中，对于Bottleneck Design的ResNet通常用于更深的如101这样的网络中，目的是减少计算和参数量（<strong>实用目的</strong>）。</p>
<h3 id="ResNet如何解决梯度消失？"><a href="#ResNet如何解决梯度消失？" class="headerlink" title="ResNet如何解决梯度消失？"></a>ResNet如何解决梯度消失？</h3><p>H(x) = F(x)+x</p>
<p>H’(x) = F’(x)+1</p>
<h3 id="ResNet网络越来越深，准确率会不会提升？"><a href="#ResNet网络越来越深，准确率会不会提升？" class="headerlink" title="ResNet网络越来越深，准确率会不会提升？"></a>ResNet网络越来越深，准确率会不会提升？</h3><ul>
<li>[ ] TODO</li>
</ul>
<h2 id="ResNet-v2"><a href="#ResNet-v2" class="headerlink" title="ResNet v2"></a>ResNet v2</h2><p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201126220547.png" style="zoom:67%;"></p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.05027">《Identity Mappings in Deep Residual Networks》</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/shouhuxianjian/p/7770658.html">Feature Extractor[ResNet v2]</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/lanran2/article/details/80247515">ResNetV2：ResNet深度解析</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/u014061630/article/details/80558661">ResNet v2论文笔记</a></li>
<li><a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000011228906">[ResNet系] 002 ResNet-v2</a></li>
</ul>
<h3 id="ResNet-v1-与-ResNet-v2的区别"><a href="#ResNet-v1-与-ResNet-v2的区别" class="headerlink" title="ResNet v1 与 ResNet v2的区别"></a>ResNet v1 与 ResNet v2的区别</h3><p>原始的resnet是上图中的a的模式，我们可以看到相加后需要进入ReLU做一个非线性激活，这里一个改进就是砍掉了这个非线性激活，不难理解，<strong>如果将ReLU放在原先的位置，那么残差块输出永远是非负的，这制约了模型的表达能力</strong>，因此我们需要做一些调整，我们将这个ReLU移入了残差块内部，也就是图e的模式。</p>
<h3 id="ResNet-v2-的-ReLU-激活函数有什么不同？"><a href="#ResNet-v2-的-ReLU-激活函数有什么不同？" class="headerlink" title="ResNet v2 的 ReLU 激活函数有什么不同？"></a>ResNet v2 的 ReLU 激活函数有什么不同？</h3><ul>
<li>[ ] TODO</li>
</ul>
<h2 id="ResNeXt"><a href="#ResNeXt" class="headerlink" title="ResNeXt"></a>ResNeXt</h2><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/u014380165/article/details/71667916">ResNeXt算法详解</a></li>
</ul>
<h2 id="学习率如何调整"><a href="#学习率如何调整" class="headerlink" title="学习率如何调整"></a>学习率如何调整</h2><ul>
<li>[ ] TODO</li>
</ul>
<h2 id="神经网络的深度和宽度作用"><a href="#神经网络的深度和宽度作用" class="headerlink" title="神经网络的深度和宽度作用"></a>神经网络的深度和宽度作用</h2><ul>
<li>[ ] TODO</li>
</ul>
<h2 id="网络压缩与量化"><a href="#网络压缩与量化" class="headerlink" title="网络压缩与量化"></a>网络压缩与量化</h2><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/shuzfan/article/details/51678499">网络压缩-量化方法对比</a></li>
</ul>
<h2 id="Batch-Size"><a href="#Batch-Size" class="headerlink" title="Batch Size"></a>Batch Size</h2><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/61607442">怎么选取训练神经网络时的Batch size?</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lien0906/article/details/79166196">谈谈深度学习中的 Batch_Size</a></p>
</li>
</ul>
<h2 id="BN和Dropout在训练和测试时的差别"><a href="#BN和Dropout在训练和测试时的差别" class="headerlink" title="BN和Dropout在训练和测试时的差别"></a>BN和Dropout在训练和测试时的差别</h2><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/61725100">BN和Dropout在训练和测试时的差别</a></li>
</ul>
<h2 id="深度学习调参有哪些技巧？"><a href="#深度学习调参有哪些技巧？" class="headerlink" title="深度学习调参有哪些技巧？"></a>深度学习调参有哪些技巧？</h2><p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/25097993/answer/651617880">https://www.zhihu.com/question/25097993/answer/651617880</a></li>
</ul>
<h2 id="为什么深度学习中的模型基本用3x3和5x5的卷积（奇数），而不是2x2和4x4的卷积（偶数）？"><a href="#为什么深度学习中的模型基本用3x3和5x5的卷积（奇数），而不是2x2和4x4的卷积（偶数）？" class="headerlink" title="为什么深度学习中的模型基本用3x3和5x5的卷积（奇数），而不是2x2和4x4的卷积（偶数）？"></a>为什么深度学习中的模型基本用3x3和5x5的卷积（奇数），而不是2x2和4x4的卷积（偶数）？</h2><p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/321773456">https://www.zhihu.com/question/321773456</a></li>
</ul>
<h2 id="深度学习训练中是否有必要使用L1获得稀疏解"><a href="#深度学习训练中是否有必要使用L1获得稀疏解" class="headerlink" title="深度学习训练中是否有必要使用L1获得稀疏解?"></a>深度学习训练中是否有必要使用L1获得稀疏解?</h2><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/51822759">https://www.zhihu.com/question/51822759</a></li>
</ul>
<h2 id="EfficientNet"><a href="#EfficientNet" class="headerlink" title="EfficientNet"></a>EfficientNet</h2><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/326833457">如何评价谷歌大脑的EfficientNet？</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/67834114">EfficientNet-可能是迄今为止最好的CNN网络</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/70369784">EfficientNet论文解读</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/69349360">EfficientNet：调参侠的福音（ICML 2019）</a></li>
</ul>
<h2 id="如何理解归一化（Normalization）对于神经网络（深度学习）的帮助？"><a href="#如何理解归一化（Normalization）对于神经网络（深度学习）的帮助？" class="headerlink" title="如何理解归一化（Normalization）对于神经网络（深度学习）的帮助？"></a>如何理解归一化（Normalization）对于神经网络（深度学习）的帮助？</h2><p>BN最早被认为通过降低所谓<strong>Internal Covariate Shift</strong>，这种想法的出处可考至<a href="https://link.zhihu.com/?target=http%3A//proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a>，想必这也是batch norm作者这么设计的初衷。但是这种想法并没有过多实验支持，比如说去年NeurlPS这篇paper作者做了实验，在batch norm之后加上一些随机扰动（non-zero mean and non-unit variance，人为引入covariate shift），发现效果仍然比不加好很多。为什么放在batch norm layer之后而不是之前？因为为了证伪batch norm通过forward pass这一步降低covariate shift来提升网络训练效率的。这样说来故事就变得很有趣了，也就是说我们大概都理解一些BN对BN层之前网络噪音的好处，那么能不能研究一下它对它后面layer的影响？所以这些研究从优化的角度，有如下几种观点。</p>
<ol>
<li>BN通过修改loss function， 可以令loss的和loss的梯度均满足更强的Lipschitzness性质（即函数f满足L-Lipschitz和 <img src="https://www.zhihu.com/equation?tex=%5Cbeta" alt="[公式]"> -smooth，令L和 <img src="https://www.zhihu.com/equation?tex=%5Cbeta" alt="[公式]"> 更小，后者其实等同于f Hessian的eigenvalue小于 <img src="https://www.zhihu.com/equation?tex=%5Cbeta" alt="[公式]"> ，可以作为光滑程度的度量，其实吧我觉得，一般convex optimization里拿这个度量算convergence rate是神器，对于non-convex optimization，不懂鸭，paper里好像也没写的样子），这么做的好处是当步子迈得大的时候，我们可以更自信地告诉自己计算出来的梯度可以更好地近似实际的梯度，因此也不容易让优化掉进小坑里。有意思的地方来了，是不是我在某些地方插入一个1/1000 layer，把梯度的L-Lipschitz变成1/1000L-Lipschitz就能让函数优化的更好了呢？其实不是的，因为单纯除以函数会改变整个优化问题，而BN做了不仅仅rescale这件事情，还让原来近似最优的点在做完变化之后，仍然保留在原来不远的位置。这也就是这篇文章的核心论点，BN做的是问题reparametrization而不是简单的scaling。 [1]</li>
<li>BN把优化这件事情分解成了优化参数的方向和长度两个任务，这么做呢可以解耦层与层之间的dependency因此会让curvature结构更易于优化。这篇证了convergence rate，但由于没有认真读，所以感觉没太多资格评价。[2]</li>
</ol>
<p>归一化手段是否殊途同归？很可能是的，在[1]的3.3作者也尝试了Lp normalization，也得到了和BN差不多的效果。至于Layer norm还是weight norm，可能都可以顺着这个思路进行研究鸭，无论是通过[1]还是[2]，可能今年的paper里就见分晓了，let’s see。</p>
<ol>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1805.11604">How Does Batch Normalization Help Optimization?</a> </li>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1805.10694">Exponential convergence rates for Batch Normalization: The power of length-direction decoupling in non-convex optimization</a></li>
</ol>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/326034346/answer/708331566">如何理解归一化（Normalization）对于神经网络（深度学习）的帮助？</a></li>
</ul>
<h2 id="多标签分类怎么解决？"><a href="#多标签分类怎么解决？" class="headerlink" title="多标签分类怎么解决？"></a>多标签分类怎么解决？</h2><p>TODO</p>
<h2 id="反卷积（deconv）-转置卷积（trans）"><a href="#反卷积（deconv）-转置卷积（trans）" class="headerlink" title="反卷积（deconv）/转置卷积（trans）"></a>反卷积（deconv）/转置卷积（trans）</h2><p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/a_a_ron/article/details/79181108">反卷积(Deconvolution)、上采样(UNSampling)与上池化(UnPooling)</a></li>
<li><a target="_blank" rel="noopener" href="https://buptldy.github.io/2016/10/29/2016-10-29-deconv/">Transposed Convolution, Fractionally Strided Convolution or Deconvolution</a></li>
</ul>
<h2 id="空洞卷积（dilated-Atrous-conv）"><a href="#空洞卷积（dilated-Atrous-conv）" class="headerlink" title="空洞卷积（dilated/Atrous conv）"></a>空洞卷积（dilated/Atrous conv）</h2><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/54149221">如何理解空洞卷积（dilated convolution）？</a></li>
</ul>
<h2 id="Pooling层原理"><a href="#Pooling层原理" class="headerlink" title="Pooling层原理"></a>Pooling层原理</h2><ul>
<li>[ ] TODO</li>
</ul>
<h2 id="depthwise卷积加速比推导"><a href="#depthwise卷积加速比推导" class="headerlink" title="depthwise卷积加速比推导"></a>depthwise卷积加速比推导</h2><ul>
<li>[ ] TODO</li>
</ul>
<h2 id="为什么降采用使用max-pooling，而分类使用average-pooling"><a href="#为什么降采用使用max-pooling，而分类使用average-pooling" class="headerlink" title="为什么降采用使用max pooling，而分类使用average pooling"></a>为什么降采用使用max pooling，而分类使用average pooling</h2><ul>
<li>[ ] TODO</li>
</ul>
<h2 id="max-pooling如何反向传播"><a href="#max-pooling如何反向传播" class="headerlink" title="max pooling如何反向传播"></a>max pooling如何反向传播</h2><ul>
<li>[ ] TODO</li>
</ul>
<h2 id="反卷积"><a href="#反卷积" class="headerlink" title="反卷积"></a>反卷积</h2><p>TODO</p>
<h2 id="组卷积（group-convolution）"><a href="#组卷积（group-convolution）" class="headerlink" title="组卷积（group convolution）"></a>组卷积（group convolution）</h2><ul>
<li>[ ] TODO</li>
</ul>
<p>在说明分组卷积之前我们用一张图来体会一下一般的卷积操作。 </p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201126220834.png" style="zoom:80%;"></p>
<p>从上图可以看出，一般的卷积会对输入数据的整体一起做卷积操作，即输入数据：H1×W1×C1；而卷积核大小为h1×w1，通道为C1，一共有C2个，然后卷积得到的输出数据就是H2×W2×C2。这里我们假设输出和输出的分辨率是不变的。主要看这个过程是一气呵成的，这对于存储器的容量提出了更高的要求。 </p>
<p>但是分组卷积明显就没有那么多的参数。先用图片直观地感受一下分组卷积的过程。对于上面所说的同样的一个问题，分组卷积就如下图所示。 </p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201126220857.png" style="zoom:67%;"></p>
<p>可以看到，图中将输入数据分成了2组（组数为g），需要注意的是，这种分组只是在深度上进行划分，即某几个通道编为一组，这个具体的数量由（C1/g）决定。因为输出数据的改变，相应的，卷积核也需要做出同样的改变。即每组中卷积核的深度也就变成了（C1/g），而卷积核的大小是不需要改变的，此时每组的卷积核的个数就变成了（C2/g）个，而不是原来的C2了。然后用每组的卷积核同它们对应组内的输入数据卷积，得到了输出数据以后，再用concatenate的方式组合起来，最终的输出数据的通道仍旧是C2。也就是说，分组数g决定以后，那么我们将并行的运算g个相同的卷积过程，每个过程里（每组），输入数据为H1×W1×C1/g，卷积核大小为h1×w1×C1/g，一共有C2/g个，输出数据为H2×W2×C2/g。</p>
<p>举个例子：</p>
<p>Group conv本身就极大地减少了参数。比如当输入通道为256，输出通道也为256，kernel size为3×3，不做Group conv参数为256×3×3×256。实施分组卷积时，若group为8，每个group的input channel和output channel均为32，参数为8×32×3×3×32，是原来的八分之一。而Group conv最后每一组输出的feature maps应该是以concatenate的方式组合。<br>Alex认为group conv的方式能够增加 filter之间的对角相关性，而且能够减少训练参数，不容易过拟合，这类似于正则的效果。</p>
<p><strong>参考资料</strong></p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://blog.yani.io/filter-group-tutorial/">A Tutorial on Filter Groups (Grouped Convolution)</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/chaolei3/article/details/79374563">深度可分离卷积、分组卷积、扩张卷积、转置卷积（反卷积）的理解</a></p>
</li>
</ul>
<h2 id="交错组卷积（Interleaved-group-convolutions，IGC）"><a href="#交错组卷积（Interleaved-group-convolutions，IGC）" class="headerlink" title="交错组卷积（Interleaved group convolutions，IGC）"></a>交错组卷积（Interleaved group convolutions，IGC）</h2><p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.sohu.com/a/161110049_465975">学界 | MSRA王井东详解ICCV 2017入选论文：通用卷积神经网络交错组卷积</a></li>
<li><a target="_blank" rel="noopener" href="https://edu.csdn.net/course/play/8320/171433?s=1">视频：基于交错组卷积的高效深度神经网络</a></li>
</ul>
<h2 id="空洞-扩张卷积（Dilated-Atrous-Convolution）"><a href="#空洞-扩张卷积（Dilated-Atrous-Convolution）" class="headerlink" title="空洞/扩张卷积（Dilated/Atrous Convolution）"></a>空洞/扩张卷积（Dilated/Atrous Convolution）</h2><p>Dilated convolution/Atrous convolution可以叫空洞卷积或者扩张卷积。</p>
<p>背景：语义分割中pooling 和 up-sampling layer层。pooling会降低图像尺寸的同时增大感受野，而up-sampling操作扩大图像尺寸，这样虽然恢复了大小，但很多细节被池化操作丢失了。</p>
<p>需求：能不能设计一种新的操作，不通过pooling也能有较大的感受野看到更多的信息呢？</p>
<p>目的：替代pooling和up-sampling运算，既增大感受野又不减小图像大小。</p>
<p>简述：在标准的 convolution map 里注入空洞，以此来增加 reception field。相比原来的正常convolution，dilated convolution 多了一个 hyper-parameter 称之为 dilation rate 指的是kernel的间隔数量(e.g. 正常的 convolution 是 dilatation rate 1)。</p>
<p>空洞卷积诞生于图像分割领域，图像输入到网络中经过CNN提取特征，再经过pooling降低图像尺度的同时增大感受野。由于图像分割是pixel−wise预测输出，所以还需要通过upsampling将变小的图像恢复到原始大小。upsampling通常是通过deconv(转置卷积)完成。因此图像分割FCN有两个关键步骤：池化操作增大感受野，upsampling操作扩大图像尺寸。这儿有个问题，就是虽然图像经过upsampling操作恢复了大小，但是很多细节还是被池化操作丢失了。那么有没有办法既增大了感受野又不减小图像大小呢？Dilated conv横空出世。</p>
<p><img src="/2020/10/19/DL%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9/Users/58341/Desktop/Deep-L/Deep-Learning-Interview-Book/docs/imgs/DLIB-0016.png" alt="image.png"></p>
<p>注意事项：</p>
<p>1.为什么不直接使用5x5或者7x7的卷积核？这不也增加了感受野么？</p>
<p>答：增大卷积核能增大感受野，但是只是线性增长，参考答案里的那个公式，(kernel-1)*layer，并不能达到空洞卷积的指数增长。</p>
<p>2.2-dilated要在1-dilated的基础上才能达到7的感受野（如上图a、b所示）</p>
<p>关于空洞卷积的另一种概括：</p>
<p>Dilated Convolution问题的引出，是因为down-sample之后的为了让input和output的尺寸一致。我们需要up-sample，但是up-sample会丢失信息。如果不采用pooling，就无需下采样和上采样步骤了。但是这样会导致kernel 的感受野变小，导致预测不精确。。如果采用大的kernel话，一来训练的参数变大。二来没有小的kernel叠加的正则作用，所以kernel size变大行不通。</p>
<p>由此Dilated Convolution是在不改变kernel size的条件下，增大感受野。</p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1511.07122">《Multi-Scale Context Aggregation by Dilated Convolutions》</a> </li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.05587">《Rethinking Atrous Convolution for Semantic Image Segmentation》</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/54149221">如何理解空洞卷积（dilated convolution）？</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/silence2015/article/details/79748729">Dilated/Atrous conv 空洞卷积/多孔卷积</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/guvcolie/article/details/77884530?locationNum=10&amp;fps=1">Multi-Scale Context Aggregation by Dilated Convolution 对空洞卷积（扩张卷积）、感受野的理解</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/chaolei3/article/details/79374563">对深度可分离卷积、分组卷积、扩张卷积、转置卷积（反卷积）的理解</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/nn/atrous_conv2d">tf.nn.atrous_conv2d</a></p>
</li>
</ul>
<h2 id="转置卷积（Transposed-Convolutions-deconvlution）"><a href="#转置卷积（Transposed-Convolutions-deconvlution）" class="headerlink" title="转置卷积（Transposed Convolutions/deconvlution）"></a>转置卷积（Transposed Convolutions/deconvlution）</h2><p>转置卷积（transposed Convolutions）又名反卷积（deconvolution）或是分数步长卷积（fractially straced convolutions）。反卷积（Transposed Convolution, Fractionally Strided Convolution or Deconvolution）的概念第一次出现是 Zeiler 在2010年发表的论文 Deconvolutional networks 中。</p>
<p><strong>转置卷积和反卷积的区别</strong></p>
<p>那什么是反卷积？从字面上理解就是卷积的逆过程。值得注意的反卷积虽然存在，但是在深度学习中并不常用。而转置卷积虽然又名反卷积，却不是真正意义上的反卷积。因为根据反卷积的数学含义，通过反卷积可以将通过卷积的输出信号，完全还原输入信号。而事实是，转置卷积只能还原shape大小，而不能还原value。你可以理解成，至少在数值方面上，转置卷积不能实现卷积操作的逆过程。所以说转置卷积与真正的反卷积有点相似，因为两者产生了相同的空间分辨率。但是又名反卷积（deconvolutions）的这种叫法是不合适的，因为它不符合反卷积的概念。</p>
<p>简单来说，转置矩阵就是一种上采样过程。</p>
<p>正常卷积过程如下，利用3x3的卷积核对4x4的输入进行卷积，输出结果为2x2</p>
<p><img src="https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/no_padding_no_strides.gif?raw=true" alt="卷积过程"></p>
<p>转置卷积过程如下，利用3x3的卷积核对”做了补0”的2x2输入进行卷积，输出结果为4x4。</p>
<p><img src="https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/no_padding_no_strides_transposed.gif?raw=true" alt="转置卷积"></p>
<p>上述的卷积运算和转置卷积是”尺寸”对应的，卷积的输入大小与转置卷积的输出大小一致，分别可以看成下采样和上采样操作。</p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://buptldy.github.io/2016/10/29/2016-10-29-deconv/">Transposed Convolution, Fractionally Strided Convolution or Deconvolution</a></li>
<li>[深度学习 | 反卷积/转置卷积 的理解 transposed conv/deconv](</li>
</ul>
<h2 id="Inception系列（V1-V4）"><a href="#Inception系列（V1-V4）" class="headerlink" title="Inception系列（V1-V4）"></a>Inception系列（V1-V4）</h2><h3 id="InceptionV1"><a href="#InceptionV1" class="headerlink" title="InceptionV1"></a>InceptionV1</h3><ul>
<li>[ ] TODO</li>
</ul>
<h3 id="InceptionV2"><a href="#InceptionV2" class="headerlink" title="InceptionV2"></a>InceptionV2</h3><ul>
<li>[ ] TODO</li>
</ul>
<h3 id="InceptionV3"><a href="#InceptionV3" class="headerlink" title="InceptionV3"></a>InceptionV3</h3><ul>
<li>[ ] TODO</li>
</ul>
<h3 id="InceptionV4"><a href="#InceptionV4" class="headerlink" title="InceptionV4"></a>InceptionV4</h3><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://baijiahao.baidu.com/s?id=1601882944953788623&amp;wfr=spider&amp;for=pc">一文概览Inception家族的「奋斗史」</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_39953502/article/details/80966046">inception-v1,v2,v3,v4——论文笔记</a></li>
</ul>
<h2 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h2><ul>
<li>[ ] TODO</li>
</ul>
<h3 id="为什么-DenseNet-比-ResNet-好？"><a href="#为什么-DenseNet-比-ResNet-好？" class="headerlink" title="为什么 DenseNet 比 ResNet 好？"></a>为什么 DenseNet 比 ResNet 好？</h3><ul>
<li>[ ] TODO</li>
</ul>
<h3 id="为什么-DenseNet-比-ResNet-更耗显存？"><a href="#为什么-DenseNet-比-ResNet-更耗显存？" class="headerlink" title="为什么 DenseNet 比 ResNet 更耗显存？"></a>为什么 DenseNet 比 ResNet 更耗显存？</h3><ul>
<li>[ ] TODO</li>
</ul>
<h2 id="SE-Net"><a href="#SE-Net" class="headerlink" title="SE-Net"></a>SE-Net</h2><ul>
<li>[ ] TODO</li>
</ul>
<h3 id="Squeeze-Excitation结构是怎么实现的？"><a href="#Squeeze-Excitation结构是怎么实现的？" class="headerlink" title="Squeeze-Excitation结构是怎么实现的？"></a>Squeeze-Excitation结构是怎么实现的？</h3><p>TODO</p>
<h2 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h2><p>一句话概括就是：FCN将传统网络后面的全连接层换成了卷积层，这样网络输出不再是类别而是 heatmap；同时为了解决因为卷积和池化对图像尺寸的影响，提出使用上采样的方式恢复。</p>
<p>作者的FCN主要使用了三种技术：</p>
<ul>
<li><p>卷积化（Convolutional）</p>
</li>
<li><p>上采样（Upsample）</p>
</li>
<li>跳跃结构（Skip Layer）</li>
</ul>
<p>卷积化</p>
<p>卷积化即是将普通的分类网络，比如VGG16，ResNet50/101等网络丢弃全连接层，换上对应的卷积层即可。</p>
<p>上采样</p>
<p>此处的上采样即是反卷积（Deconvolution）。当然关于这个名字不同框架不同，Caffe和Kera里叫Deconvolution，而tensorflow里叫conv_transpose。CS231n这门课中说，叫conv_transpose更为合适。</p>
<p>众所诸知，普通的池化（为什么这儿是普通的池化请看后文）会缩小图片的尺寸，比如VGG16 五次池化后图片被缩小了32倍。为了得到和原图等大的分割图，我们需要上采样/反卷积。</p>
<p>反卷积和卷积类似，都是相乘相加的运算。只不过后者是多对一，前者是一对多。而反卷积的前向和后向传播，只用颠倒卷积的前后向传播即可。所以无论优化还是后向传播算法都是没有问题。</p>
<p>跳跃结构（Skip Layers）</p>
<p>（这个奇怪的名字是我翻译的，好像一般叫忽略连接结构）这个结构的作用就在于优化结果，因为如果将全卷积之后的结果直接上采样得到的结果是很粗糙的，所以作者将不同池化层的结果进行上采样之后来优化输出。</p>
<p>上采样获得与输入一样的尺寸<br>文章采用的网络经过5次卷积+池化后，图像尺寸依次缩小了 2、4、8、16、32倍，对最后一层做32倍上采样，就可以得到与原图一样的大小</p>
<p>作者发现，仅对第5层做32倍反卷积（deconvolution），得到的结果不太精确。于是将第 4 层和第 3 层的输出也依次反卷积（图５）</p>
<p><strong>参考资料</strong></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/22308032">【总结】图像语义分割之FCN和CRF</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zizi7/article/details/77093447">图像语义分割（1）- FCN</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/gujianhan/p/6030639.html">全卷积网络 FCN 详解</a></p>
<h2 id="U-Net"><a href="#U-Net" class="headerlink" title="U-Net"></a>U-Net</h2><p>本文介绍一种编码器-解码器结构。编码器逐渐减少池化层的空间维度，解码器逐步修复物体的细节和空间维度。编码器和解码器之间通常存在快捷连接，因此能帮助解码器更好地修复目标的细节。U-Net 是这种方法中最常用的结构。</p>
<p>fcn(fully convolutional natwork)的思想是：修改一个普通的逐层收缩的网络，用上采样(up sampling)(？？反卷积)操作代替网络后部的池化(pooling)操作。因此，这些层增加了输出的分辨率。为了使用局部的信息，在网络收缩过程（路径）中产生的高分辨率特征(high resolution features) ，被连接到了修改后网络的上采样的结果上。在此之后，一个卷积层基于这些信息综合得到更精确的结果。</p>
<p>与fcn(fully convolutional natwork)不同的是，我们的网络在上采样部分依然有大量的特征通道(feature channels)，这使得网络可以将环境信息向更高的分辨率层(higher resolution layers)传播。结果是，扩张路径基本对称于收缩路径。网络不存在任何全连接层(fully connected layers)，并且，只使用每个卷积的有效部分，例如，分割图(segmentation map)只包含这样一些像素点，这些像素点的完整上下文都出现在输入图像中。为了预测图像边界区域的像素点，我们采用镜像图像的方式补全缺失的环境像素。这个tiling方法在使用网络分割大图像时是非常有用的，因为如果不这么做，GPU显存会限制图像分辨率。<br>我们的训练数据太少，因此我们采用弹性形变的方式增加数据。这可以让模型学习得到形变不变性。这对医学图像分割是非常重要的，因为组织的形变是非常常见的情况，并且计算机可以很有效的模拟真实的形变。在[3]中指出了在无监督特征学习中，增加数据以获取不变性的重要性。</p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/natsuka/article/details/78565229">U-net翻译</a></li>
</ul>
<h2 id="DeepLab-系列"><a href="#DeepLab-系列" class="headerlink" title="DeepLab 系列"></a>DeepLab 系列</h2><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/u011974639/article/details/79148719">Semantic Segmentation —DeepLab(1,2,3)系列总结</a></li>
</ul>
<h2 id="边框回顾（Bounding-Box-Regression）"><a href="#边框回顾（Bounding-Box-Regression）" class="headerlink" title="边框回顾（Bounding-Box Regression）"></a>边框回顾（Bounding-Box Regression）</h2><p>如下图所示，绿色的框表示真实值Ground Truth, 红色的框为Selective Search提取的候选区域/框Region Proposal。那么即便红色的框被分类器识别为飞机，但是由于红色的框定位不准(IoU&lt;0.5)， 这张图也相当于没有正确的检测出飞机。</p>
<p><img src="https://www.julyedu.com/Public/Image/Question/1525499418_635.png" alt></p>
<p>如果我们能对红色的框进行微调fine-tuning，使得经过微调后的窗口跟Ground Truth 更接近， 这样岂不是定位会更准确。 而Bounding-box regression 就是用来微调这个窗口的。</p>
<p>边框回归是什么？</p>
<p>对于窗口一般使用四维向量(x,y,w,h)(x,y,w,h) 来表示， 分别表示窗口的中心点坐标和宽高。 对于图2, 红色的框 P 代表原始的Proposal, 绿色的框 G 代表目标的 Ground Truth， 我们的目标是寻找一种关系使得输入原始的窗口 P 经过映射得到一个跟真实窗口 G 更接近的回归窗口G^。</p>
<p><img src="https://www.julyedu.com/Public/Image/Question/1525499529_241.png" alt></p>
<p>所以，边框回归的目的即是：给定(Px,Py,Pw,Ph)寻找一种映射f， 使得f(Px,Py,Pw,Ph)=(Gx^,Gy^,Gw^,Gh^)并且(Gx^,Gy^,Gw^,Gh^)≈(Gx,Gy,Gw,Gh)</p>
<p>边框回归怎么做的？</p>
<p>那么经过何种变换才能从图2中的窗口 P 变为窗口G^呢？ 比较简单的思路就是: 平移+尺度放缩</p>
<p>先做平移(Δx,Δy)，Δx=Pwdx(P),Δy=Phdy(P)这是R-CNN论文的：<br>G^x=Pwdx(P)+Px,(1)<br>G^y=Phdy(P)+Py,(2)</p>
<p>然后再做尺度缩放(Sw,Sh), Sw=exp(dw(P)),Sh=exp(dh(P)),对应论文中：<br>G^w=Pwexp(dw(P)),(3)<br>G^h=Phexp(dh(P)),(4)</p>
<p>观察(1)-(4)我们发现， 边框回归学习就是dx(P),dy(P),dw(P),dh(P)这四个变换。</p>
<p>下一步就是设计算法那得到这四个映射。</p>
<p>线性回归就是给定输入的特征向量 X, 学习一组参数 W, 使得经过线性回归后的值跟真实值 Y(Ground Truth)非常接近. 即Y≈WX。 那么 Bounding-box 中我们的输入以及输出分别是什么呢？</p>
<p>Input:<br>RegionProposal→P=(Px,Py,Pw,Ph)这个是什么？ 输入就是这四个数值吗？其实真正的输入是这个窗口对应的 CNN 特征，也就是 R-CNN 中的 Pool5 feature（特征向量）。 (注：训练阶段输入还包括 Ground Truth， 也就是下边提到的t∗=(tx,ty,tw,th))</p>
<p>Output:<br>需要进行的平移变换和尺度缩放 dx(P),dy(P),dw(P),dh(P)，或者说是Δx,Δy,Sw,Sh。我们的最终输出不应该是 Ground Truth 吗？ 是的， 但是有了这四个变换我们就可以直接得到 Ground Truth。</p>
<p>这里还有个问题， 根据(1)~(4)我们可以知道， P 经过 dx(P),dy(P),dw(P),dh(P)得到的并不是真实值 G，而是预测值G^。的确，这四个值应该是经过 Ground Truth 和 Proposal 计算得到的真正需要的平移量(tx,ty)和尺度缩放(tw,th)。 </p>
<p>这也就是 R-CNN 中的(6)~(9)：<br>tx=(Gx−Px)/Pw,(6)</p>
<p>ty=(Gy−Py)/Ph,(7)</p>
<p>tw=log(Gw/Pw),(8)</p>
<p>th=log(Gh/Ph),(9)</p>
<p>那么目标函数可以表示为 d∗(P)=wT∗Φ5(P)，Φ5(P)是输入 Proposal 的特征向量，w∗是要学习的参数（*表示 x,y,w,h， 也就是每一个变换对应一个目标函数） , d∗(P) 是得到的预测值。</p>
<p>我们要让预测值跟真实值t∗=(tx,ty,tw,th)差距最小， 得到损失函数为：<br>Loss=∑iN(ti∗−w^T∗ϕ5(Pi))2</p>
<p>函数优化目标为：</p>
<p>W∗=argminw∗∑iN(ti∗−w^T∗ϕ5(Pi))2+λ||w^∗||2</p>
<p>利用梯度下降法或者最小二乘法就可以得到 w∗。</p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="http://caffecn.cn/?/question/160">bounding box regression</a></li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zijin0802034/article/details/77685438">边框回归(Bounding Box Regression)详解</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.julyedu.com/question/big/kp_id/26/ques_id/2139">什么是边框回归Bounding-Box regression，以及为什么要做、怎么做</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u014722627/article/details/60574260">https://blog.csdn.net/u014722627/article/details/60574260</a>)</p>
</li>
</ul>
<h2 id="Xception"><a href="#Xception" class="headerlink" title="Xception"></a>Xception</h2><ul>
<li>[ ] TODO</li>
</ul>
<h2 id="SENet"><a href="#SENet" class="headerlink" title="SENet"></a>SENet</h2><p><strong>SENet</strong></p>
<p>论文：《Squeeze-and-Excitation Networks》 </p>
<p>论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1709.01507">https://arxiv.org/abs/1709.01507</a> </p>
<p>代码地址：<a target="_blank" rel="noopener" href="https://github.com/hujie-frank/SENet">https://github.com/hujie-frank/SENet</a></p>
<p>论文的动机是从特征通道之间的关系入手，希望显式地建模特征通道之间的相互依赖关系。另外，没有引入一个新的空间维度来进行特征通道间的融合，而是采用了一种全新的“特征重标定”策略。具体来说，就是通过学习的方式来自动获取到每个特征通道的重要程度，然后依照这个重要程度去增强有用的特征并抑制对当前任务用处不大的特征，通俗来讲，就是让网络利用全局信息有选择的增强有益feature通道并抑制无用feature通道，从而能实现feature通道自适应校准。 </p>
<p><img src="/2020/10/19/DL%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9/Users/58341/Desktop/Deep-L/Deep-Learning-Interview-Book/docs/imgs/DLIB-0017.png" alt="Schema of SE-Inception and SE-ResNet modules"></p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/xjz18298268521/article/details/79078551">SENet学习笔记</a></li>
</ul>
<h2 id="SKNet"><a href="#SKNet" class="headerlink" title="SKNet"></a>SKNet</h2><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/59690223">SKNet——SENet孪生兄弟篇</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/60187262">后ResNet时代：SENet与SKNet</a></li>
</ul>
<h2 id="GCNet"><a href="#GCNet" class="headerlink" title="GCNet"></a>GCNet</h2><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/64988633">GCNet：当Non-local遇见SENet</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/65776424">2019 GCNet（attention机制，目标检测backbone性能提升）论文阅读笔记</a></li>
</ul>
<h2 id="Octave-Convolution"><a href="#Octave-Convolution" class="headerlink" title="Octave Convolution"></a>Octave Convolution</h2><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/320462422/">如何评价最新的Octave Convolution？</a></li>
</ul>
<h2 id="MobileNet-系列（V1-V3）"><a href="#MobileNet-系列（V1-V3）" class="headerlink" title="MobileNet 系列（V1-V3）"></a>MobileNet 系列（V1-V3）</h2><ul>
<li>[ ] TODO</li>
</ul>
<h3 id="MobileNetV1"><a href="#MobileNetV1" class="headerlink" title="MobileNetV1"></a>MobileNetV1</h3><p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/t800ghb/article/details/78879612">深度解读谷歌MobileNet</a></li>
</ul>
<h3 id="MobileNetV2"><a href="#MobileNetV2" class="headerlink" title="MobileNetV2"></a>MobileNetV2</h3><ul>
<li>[ ] TODO</li>
</ul>
<h3 id="MobileNetV3"><a href="#MobileNetV3" class="headerlink" title="MobileNetV3"></a>MobileNetV3</h3><ul>
<li><p>[ ] TODO</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/323419310">如何评价google Searching for MobileNetV3？</a></p>
</li>
</ul>
<h3 id="MobileNet系列为什么快？各有多少层？多少参数？"><a href="#MobileNet系列为什么快？各有多少层？多少参数？" class="headerlink" title="MobileNet系列为什么快？各有多少层？多少参数？"></a>MobileNet系列为什么快？各有多少层？多少参数？</h3><ul>
<li>[ ] TODO</li>
</ul>
<h3 id="MobileNetV1、MobileNetV2和MobileNetV3有什么区别"><a href="#MobileNetV1、MobileNetV2和MobileNetV3有什么区别" class="headerlink" title="MobileNetV1、MobileNetV2和MobileNetV3有什么区别"></a>MobileNetV1、MobileNetV2和MobileNetV3有什么区别</h3><p>MobileNetv1：在depthwise separable convolutions（参考Xception）方法的基础上提供了高校模型设计的两个选择：宽度因子（width multiplie）和分辨率因子（resolution multiplier）。深度可分离卷积depthwise separable convolutions（参考Xception）的本质是冗余信息更小的稀疏化表达。</p>
<p>下面介绍两幅Xception中 depthwise separable convolution的图示：</p>
<p><img src="/2020/10/19/DL%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9/Users/58341/Desktop/Deep-L/Deep-Learning-Interview-Book/docs/imgs/DLIB-0018.png" alt="image.png"></p>
<p><img src="/2020/10/19/DL%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9/Users/58341/Desktop/Deep-L/Deep-Learning-Interview-Book/docs/imgs/DLIB-0019.png" alt="image.png"></p>
<p>深度可分离卷积的过程是①用16个3×3大小的卷积核（1通道）分别与输入的16通道的数据做卷积（这里使用了16个1通道的卷积核，输入数据的每个通道用1个3×3的卷积核卷积），得到了16个通道的特征图，我们说该步操作是depthwise（逐层）的，在叠加16个特征图之前，②接着用32个1×1大小的卷积核（16通道）在这16个特征图进行卷积运算，将16个通道的信息进行融合（用1×1的卷积进行不同通道间的信息融合），我们说该步操作是pointwise（逐像素）的。这样我们可以算出整个过程使用了3×3×16+（1×1×16）×32 =656个参数。</p>
<p>注：上述描述与标准的卷积非常的不同，第一点在于使用非1x1卷积核时，是单channel的（可以说是1通道），即上一层输出的每个channel都有与之对应的卷积核。而标准的卷积过程，卷积核是多channel的。第二点在于使用1x1卷积核实现多channel的融合，并利用多个1x1卷积核生成多channel。表达的可能不是很清楚，但结合图示其实就容易明白了。</p>
<p>一般卷积核的channel也常称为深度（depth），所以叫做深度可分离，即原来为多channel组合，现在变成了单channel分离。</p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/t800ghb/article/details/78879612">深度解读谷歌MobileNet</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/chaolei3/article/details/79374563">对深度可分离卷积、分组卷积、扩张卷积、转置卷积（反卷积）的理解</a></li>
</ul>
<h3 id="MobileNetv2为什么会加shotcut？"><a href="#MobileNetv2为什么会加shotcut？" class="headerlink" title="MobileNetv2为什么会加shotcut？"></a>MobileNetv2为什么会加shotcut？</h3><ul>
<li>[ ] TODO</li>
</ul>
<h3 id="MobileNet-V2中的Residual结构最先是哪个网络提出来的？"><a href="#MobileNet-V2中的Residual结构最先是哪个网络提出来的？" class="headerlink" title="MobileNet V2中的Residual结构最先是哪个网络提出来的？"></a>MobileNet V2中的Residual结构最先是哪个网络提出来的？</h3><ul>
<li>[ ] TODO</li>
</ul>
<h2 id="ShuffleNet-系列（V1-V2-）"><a href="#ShuffleNet-系列（V1-V2-）" class="headerlink" title="ShuffleNet 系列（V1-V2++）"></a>ShuffleNet 系列（V1-V2++）</h2><ul>
<li>[ ] TODO</li>
</ul>
<h3 id="ShuffleNetV1"><a href="#ShuffleNetV1" class="headerlink" title="ShuffleNetV1"></a>ShuffleNetV1</h3><ul>
<li><p>[ ] TODO</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u011974639/article/details/79200559">轻量级网络—ShuffleNet论文解读</a></p>
</li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/29f4ec483b96">轻量级网络ShuffleNet v1</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32304419">CNN模型之ShuffleNet</a></li>
</ul>
<h3 id="ShuffleNetV2"><a href="#ShuffleNetV2" class="headerlink" title="ShuffleNetV2"></a>ShuffleNetV2</h3><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/48261931">ShuffleNetV2：轻量级CNN网络中的桂冠</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/67009992">轻量级神经网络“巡礼”（一）—— ShuffleNetV2</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/42288448">ShufflenetV2_高效网络的4条实用准则</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/51566209">ShuffNet v1 和 ShuffleNet v2</a></li>
</ul>
<h2 id="IGC-系列（V1-V3）"><a href="#IGC-系列（V1-V3）" class="headerlink" title="IGC 系列（V1-V3）"></a>IGC 系列（V1-V3）</h2><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/ZLIL9A3RS0jj8knbXP9uFQ">微软资深研究员详解基于交错组卷积的高效DNN | 公开课笔记</a></li>
</ul>
<h2 id="深度可分离网络（Depth-separable-convolution）"><a href="#深度可分离网络（Depth-separable-convolution）" class="headerlink" title="深度可分离网络（Depth separable convolution）"></a>深度可分离网络（Depth separable convolution）</h2><ul>
<li>[ ] TODO</li>
</ul>
<h2 id><a href="#" class="headerlink" title=" "></a> </h2><h1 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h1><h2 id="IoU"><a href="#IoU" class="headerlink" title="IoU"></a>IoU</h2><p>IoU（Intersection over Union），又称重叠度/交并比。</p>
<p><strong>1 NMS</strong>：当在图像中预测多个proposals、pred bboxes时，由于预测的结果间可能存在高冗余（即同一个目标可能被预测多个矩形框），因此可以过滤掉一些彼此间高重合度的结果；具体操作就是根据各个bbox的score降序排序，剔除与高score bbox有较高重合度的低score bbox，那么重合度的度量指标就是IoU；</p>
<p><strong>2 mAP</strong>：得到检测算法的预测结果后，需要对pred bbox与gt bbox一起评估检测算法的性能，涉及到的评估指标为mAP，那么当一个pred bbox与gt bbox的重合度较高（如IoU score &gt; 0.5），且分类结果也正确时，就可以认为是该pred bbox预测正确，这里也同样涉及到IoU的概念；</p>
<p>提到IoU，大家都知道怎么回事，讲起来也都头头是道，我拿两个图示意下（以下两张图都不是本人绘制）：</p>
<p><img src="https://pic2.zhimg.com/80/v2-8fb0aa2eebc1931432eb0ed92059d2c1_hd.jpg" alt="img"></p>
<p>绿框：gt bbox；</p>
<p>红框：pred bbox；</p>
<p>那么IoU的计算如下：</p>
<p><img src="https://pic2.zhimg.com/80/v2-215e95291d2e4129206da27e7f5de6e9_hd.jpg" alt="img"></p>
<p>简单点说，就是<strong>gt bbox、pred bbox交集的面积 / 二者并集的面积</strong>；</p>
<p>好了，现在理解IoU的原理和计算方法了，就应该思考如何函数实现了，这也是我写本笔记的原因；</p>
<p>有次面试实习生的时候，一位同学讲各类目标检测算法头头是道，说到自己复现某某算法的mAP高达多少多少，问完他做的各种改进后，觉得小伙子还是挺不错的；</p>
<p>后来我是想着问问mAP的概念吧，但又觉得有点太复杂，不容易一下讲清楚细节，那就问问IoU吧，结果那位小朋友像傻逼一样看着我，说就是两个bbox的交并比啊，我说那要不你写段伪代码实现下吧，既然简单的话，应该实现起来还是很快的（一般我们也都会有这么个写伪代码的面试步骤，考考动手能力和思考能力吧）；然后那位自信满满的小伙子就立马下手开始写了，一般听完题目直接写代码的面试者，有两种可能性：</p>
<p>1 确实写过类似的代码，已经知道里面有哪些坑了，直接信手拈来；</p>
<p>2 没写过类似的代码，且把问题考虑简单化了；</p>
<p>我说你不用着急写，可以先想想两个bbox出现交集的各种情况，如两个bbox如何摆放，位置，以及二者不存在交集的情况等等（看到IoU的具体代码后，你会发现虽然只有寥寥几行代码，但其实已经处理好此类情况了），然后他画了几个图，瞬间表情严肃起来，然后我继续说你还得考虑一个bbox包围另一个bbox；两bbox并不是边角相交，而是两条边相交的特殊情况等等（说到这里我觉得自己也坏坏滴，故意把人家往歪路上牵。。。但主要是看得出来他确实不熟悉IoU的实现了），他就又画了若干种情况，最后开始写代码，刚开始还ok，写了十几行，后来越加越多，草稿纸上也涂涂改改越来越夸张，脸也越胀越红；我看了下他的代码，觉得他思路还行，考虑的还挺周全的，就给了他一个提示：你有没有考虑到你列举的这些情况，有一些可以合并的？他看了下，觉得是可以合并一些情况，就删减了部分代码，稿纸上就更乱了，然后又问他：可不可以继续合并；他就又继续思考了。。。大概是后来越想越复杂，就给我说这个原理他懂的，代码也看过，但现在确实是没能写出来；然后我安慰他，说如果没写过的话，确实是会把问题考虑简单化 / 复杂化，不过我并不是专门考个题目刁难你，而是因为你一直都在做目标检测，所以以为IoU的原理、实现你应该会比较熟悉的，写起代码也应该没问题的；而且你的思路也挺好的，先考虑各种复杂情况，再慢慢合并一些情况，先从1到N，再回到1就行，只不过可能到了N，没考虑到如何再回到1了；</p>
<p>再后来，也面试过其他实习生同学，问到了IoU的实现，很可惜，好像还没有一位同学能圆满写出来的。。。当然了，可能是我有时候过于抠细节了，不利于他们的发挥吧。。。</p>
<p>好了，以上都是废话，看看如何实现吧；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># This is the python code for calculating bbox IoU,</span></span><br><span class="line"><span class="comment"># By running the script, we can get the IoU score between pred / gt bboxes</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Author: hzhumeng01 2018-10-19</span></span><br><span class="line"><span class="comment"># copyright @ netease, AI group</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function, absolute_import</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_IoU</span>(<span class="params">pred_bbox, gt_bbox</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    return iou score between pred / gt bboxes</span></span><br><span class="line"><span class="string">    :param pred_bbox: predict bbox coordinate</span></span><br><span class="line"><span class="string">    :param gt_bbox: ground truth bbox coordinate</span></span><br><span class="line"><span class="string">    :return: iou score</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># bbox should be valid, actually we should add more judgements, just ignore here...</span></span><br><span class="line">    <span class="comment"># assert ((abs(pred_bbox[2] - pred_bbox[0]) &gt; 0) and</span></span><br><span class="line">    <span class="comment">#         (abs(pred_bbox[3] - pred_bbox[1]) &gt; 0))</span></span><br><span class="line">    <span class="comment"># assert ((abs(gt_bbox[2] - gt_bbox[0]) &gt; 0) and</span></span><br><span class="line">    <span class="comment">#         (abs(gt_bbox[3] - gt_bbox[1]) &gt; 0))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># -----0---- get coordinates of inters</span></span><br><span class="line">    ixmin = max(pred_bbox[<span class="number">0</span>], gt_bbox[<span class="number">0</span>])</span><br><span class="line">    iymin = max(pred_bbox[<span class="number">1</span>], gt_bbox[<span class="number">1</span>])</span><br><span class="line">    ixmax = min(pred_bbox[<span class="number">2</span>], gt_bbox[<span class="number">2</span>])</span><br><span class="line">    iymax = min(pred_bbox[<span class="number">3</span>], gt_bbox[<span class="number">3</span>])</span><br><span class="line">    iw = np.maximum(ixmax - ixmin + <span class="number">1.</span>, <span class="number">0.</span>)</span><br><span class="line">    ih = np.maximum(iymax - iymin + <span class="number">1.</span>, <span class="number">0.</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># -----1----- intersection</span></span><br><span class="line">    inters = iw * ih</span><br><span class="line"></span><br><span class="line">    <span class="comment"># -----2----- union, uni = S1 + S2 - inters</span></span><br><span class="line">    uni = ((pred_bbox[<span class="number">2</span>] - pred_bbox[<span class="number">0</span>] + <span class="number">1.</span>) * (pred_bbox[<span class="number">3</span>] - pred_bbox[<span class="number">1</span>] + <span class="number">1.</span>) +</span><br><span class="line">           (gt_bbox[<span class="number">2</span>] - gt_bbox[<span class="number">0</span>] + <span class="number">1.</span>) * (gt_bbox[<span class="number">3</span>] - gt_bbox[<span class="number">1</span>] + <span class="number">1.</span>) -</span><br><span class="line">           inters)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># -----3----- iou</span></span><br><span class="line">    overlaps = inters / uni</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> overlaps</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_max_IoU</span>(<span class="params">pred_bboxes, gt_bbox</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    given 1 gt bbox, &gt;1 pred bboxes, return max iou score for the given gt bbox and pred_bboxes</span></span><br><span class="line"><span class="string">    :param pred_bbox: predict bboxes coordinates, we need to find the max iou score with gt bbox for these pred bboxes</span></span><br><span class="line"><span class="string">    :param gt_bbox: ground truth bbox coordinate</span></span><br><span class="line"><span class="string">    :return: max iou score</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># bbox should be valid, actually we should add more judgements, just ignore here...</span></span><br><span class="line">    <span class="comment"># assert ((abs(gt_bbox[2] - gt_bbox[0]) &gt; 0) and</span></span><br><span class="line">    <span class="comment">#         (abs(gt_bbox[3] - gt_bbox[1]) &gt; 0))</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> pred_bboxes.shape[<span class="number">0</span>] &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># -----0---- get coordinates of inters, but with multiple predict bboxes</span></span><br><span class="line">        ixmin = np.maximum(pred_bboxes[:, <span class="number">0</span>], gt_bbox[<span class="number">0</span>])</span><br><span class="line">        iymin = np.maximum(pred_bboxes[:, <span class="number">1</span>], gt_bbox[<span class="number">1</span>])</span><br><span class="line">        ixmax = np.minimum(pred_bboxes[:, <span class="number">2</span>], gt_bbox[<span class="number">2</span>])</span><br><span class="line">        iymax = np.minimum(pred_bboxes[:, <span class="number">3</span>], gt_bbox[<span class="number">3</span>])</span><br><span class="line">        iw = np.maximum(ixmax - ixmin + <span class="number">1.</span>, <span class="number">0.</span>)</span><br><span class="line">        ih = np.maximum(iymax - iymin + <span class="number">1.</span>, <span class="number">0.</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># -----1----- intersection</span></span><br><span class="line">        inters = iw * ih</span><br><span class="line"></span><br><span class="line">        <span class="comment"># -----2----- union, uni = S1 + S2 - inters</span></span><br><span class="line">        uni = ((gt_bbox[<span class="number">2</span>] - gt_bbox[<span class="number">0</span>] + <span class="number">1.</span>) * (gt_bbox[<span class="number">3</span>] - gt_bbox[<span class="number">1</span>] + <span class="number">1.</span>) +</span><br><span class="line">               (pred_bboxes[:, <span class="number">2</span>] - pred_bboxes[:, <span class="number">0</span>] + <span class="number">1.</span>) * (pred_bboxes[:, <span class="number">3</span>] - pred_bboxes[:, <span class="number">1</span>] + <span class="number">1.</span>) -</span><br><span class="line">               inters)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># -----3----- iou, get max score and max iou index</span></span><br><span class="line">        overlaps = inters / uni</span><br><span class="line">        ovmax = np.max(overlaps)</span><br><span class="line">        jmax = np.argmax(overlaps)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> overlaps, ovmax, jmax</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># test1</span></span><br><span class="line">    pred_bbox = np.array([<span class="number">50</span>, <span class="number">50</span>, <span class="number">90</span>, <span class="number">100</span>])   <span class="comment"># top-left: &lt;50, 50&gt;, bottom-down: &lt;90, 100&gt;, &lt;x-axis, y-axis&gt;</span></span><br><span class="line">    gt_bbox = np.array([<span class="number">70</span>, <span class="number">80</span>, <span class="number">120</span>, <span class="number">150</span>])</span><br><span class="line">    <span class="keyword">print</span> (get_IoU(pred_bbox, gt_bbox))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># test2</span></span><br><span class="line">    pred_bboxes = np.array([[<span class="number">15</span>, <span class="number">18</span>, <span class="number">47</span>, <span class="number">60</span>],</span><br><span class="line">                          [<span class="number">50</span>, <span class="number">50</span>, <span class="number">90</span>, <span class="number">100</span>],</span><br><span class="line">                          [<span class="number">70</span>, <span class="number">80</span>, <span class="number">120</span>, <span class="number">145</span>],</span><br><span class="line">                          [<span class="number">130</span>, <span class="number">160</span>, <span class="number">250</span>, <span class="number">280</span>],</span><br><span class="line">                          [<span class="number">25.6</span>, <span class="number">66.1</span>, <span class="number">113.3</span>, <span class="number">147.8</span>]])</span><br><span class="line">    gt_bbox = np.array([<span class="number">70</span>, <span class="number">80</span>, <span class="number">120</span>, <span class="number">150</span>])</span><br><span class="line">    <span class="keyword">print</span> (get_max_IoU(pred_bboxes, gt_bbox))</span><br></pre></td></tr></table></figure>
<p>其实计算bbox间IoU唯一的难点就在计算intersection，代码的实现很简单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ixmin = max(pred_bbox[<span class="number">0</span>], gt_bbox[<span class="number">0</span>])</span><br><span class="line">iymin = max(pred_bbox[<span class="number">1</span>], gt_bbox[<span class="number">1</span>])</span><br><span class="line">ixmax = min(pred_bbox[<span class="number">2</span>], gt_bbox[<span class="number">2</span>])</span><br><span class="line">iymax = min(pred_bbox[<span class="number">3</span>], gt_bbox[<span class="number">3</span>])</span><br><span class="line">iw = np.maximum(ixmax - ixmin + <span class="number">1.</span>, <span class="number">0.</span>)</span><br><span class="line">ih = np.maximum(iymax - iymin + <span class="number">1.</span>, <span class="number">0.</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>比较厉害的就是，以上短短六行代码就可以囊括所有pred bbox与gt bbox间的关系，不管是bboxes间相交 / 不相交，各种相交形式等等；我们在画图分析两个bbox间的关系时，会考虑各种情况，动手实践时会发现很复杂，是因为我们<strong>陷入了一种先入为主的思维</strong>，就是pred bbox与gt bbox有一个先后顺序，即我们认定了pred bbox为画图中的第一个bbox，gt bbox为第二个，这样在二者有不同位置关系时，就得考虑各种坐标判断情况，但若此时交换二者位置，其实并不影响我们计算IoU；</p>
<p>以上六行代码也印证了这个观点，直接计算两个bbox的相交边框坐标即可，若不相交得到的结果中，必有ixmax &lt; ixmin、iymax - iymin其一成立，此时iw、ih就为0了；</p>
<p>好了，以上就是IoU的计算，原理比较简单，具体分析比较复杂，实现却异常简单，但通过对问题的深入分析，也能加深我们对知识的理解；</p>
<p>代码我传到github上了，比较简单：<a target="_blank" rel="noopener" href="https://github.com/humengdoudou/object_detection_mAP/blob/master/IoU_demo.py">IoU_demo.py</a></p>
<p><strong>参考资料</strong></p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/47189358">目标检测番外篇(1)_IoU</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u014061630/article/details/82818112">目标检测之 IoU</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/70768666">Detection基础模块之（一）IoU</a></p>
</li>
</ul>
<h3 id="如何计算-mIoU？"><a href="#如何计算-mIoU？" class="headerlink" title="如何计算 mIoU？"></a>如何计算 mIoU？</h3><p>Mean Intersection over Union(MIoU，均交并比)，为语义分割的标准度量。其计算两个集合的交集和并集之比，在语义分割问题中，这两个集合为真实值（ground truth）和预测值（predicted segmentation）。这个比例可以变形为TP（交集）比上TP、FP、FN之和（并集）。在每个类上计算IoU，然后取平均。</p>
<script type="math/tex; mode=display">
MIoU=\frac{1}{k+1}\sum^{k}_{i=0}{\frac{p_{ii}}{\sum_{j=0}^{k}{p_{ij}+\sum_{j=0}^{k}{p_{ji}-p_{ii}}}}}</script><p>pij表示真实值为i，被预测为j的数量。</p>
<p><strong>直观理解</strong></p>
<p><img src="/2020/10/19/DL%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9/Users/58341/Desktop/文档们/Deep-L/Deep-Learning-Interview-Book/docs/imgs/DLIB-0020.png" alt></p>
<p>红色圆代表真实值，黄色圆代表预测值。橙色部分为两圆交集部分。</p>
<ul>
<li>MPA（Mean Pixel Accuracy，均像素精度）：计算橙色与红色圆的比例；</li>
<li>MIoU：计算两圆交集（橙色部分）与两圆并集（红色+橙色+黄色）之间的比例，理想情况下两圆重合，比例为1。</li>
</ul>
<p><strong>Tensorflow源码解析</strong></p>
<p>Tensorflow主要用<code>tf.metrics.mean_iou</code>来计算mIoU，下面解析源码：</p>
<p><strong>第一步：计算混淆矩阵</strong></p>
<p>混淆矩阵例子</p>
<p><img src="/2020/10/19/DL%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9/Users/58341/Desktop/文档们/Deep-L/Deep-Learning-Interview-Book/docs/imgs/DLIB-0021.jpg" alt="img"></p>
<p><strong>Pytorch源码解</strong></p>
<p>Pytorch基本计算思路和上面是一样的，代码很简洁，就不过多介绍了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IOUMetric</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Class to calculate mean-iou using fast_hist method</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_classes</span>):</span></span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.hist = np.zeros((num_classes, num_classes))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_fast_hist</span>(<span class="params">self, label_pred, label_true</span>):</span></span><br><span class="line">        mask = (label_true &gt;= <span class="number">0</span>) &amp; (label_true &lt; self.num_classes)</span><br><span class="line">        hist = np.bincount(</span><br><span class="line">            self.num_classes * label_true[mask].astype(int) +</span><br><span class="line">            label_pred[mask], minlength=self.num_classes ** <span class="number">2</span>).reshape(self.num_classes, self.num_classes)</span><br><span class="line">        <span class="keyword">return</span> hist</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_batch</span>(<span class="params">self, predictions, gts</span>):</span></span><br><span class="line">        <span class="keyword">for</span> lp, lt <span class="keyword">in</span> zip(predictions, gts):</span><br><span class="line">            self.hist += self._fast_hist(lp.flatten(), lt.flatten())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span>(<span class="params">self</span>):</span></span><br><span class="line">        acc = np.diag(self.hist).sum() / self.hist.sum()</span><br><span class="line">        acc_cls = np.diag(self.hist) / self.hist.sum(axis=<span class="number">1</span>)</span><br><span class="line">        acc_cls = np.nanmean(acc_cls)</span><br><span class="line">        iu = np.diag(self.hist) / (self.hist.sum(axis=<span class="number">1</span>) + self.hist.sum(axis=<span class="number">0</span>) - np.diag(self.hist))</span><br><span class="line">        mean_iu = np.nanmean(iu)</span><br><span class="line">        freq = self.hist.sum(axis=<span class="number">1</span>) / self.hist.sum()</span><br><span class="line">        fwavacc = (freq[freq &gt; <span class="number">0</span>] * iu[freq &gt; <span class="number">0</span>]).sum()</span><br><span class="line">        <span class="keyword">return</span> acc, acc_cls, iu, mean_iu, fwavacc</span><br></pre></td></tr></table></figure>
<p><strong>Python 简版实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#RT:RightTop</span></span><br><span class="line"><span class="comment">#LB:LeftBottom</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">IOU</span>(<span class="params">rectangle A, rectangleB</span>):</span></span><br><span class="line">    W = min(A.RT.x, B.RT.x) - max(A.LB.x, B.LB.x)</span><br><span class="line">    H = min(A.RT.y, B.RT.y) - max(A.LB.y, B.LB.y)</span><br><span class="line">    <span class="keyword">if</span> W &lt;= <span class="number">0</span> <span class="keyword">or</span> H &lt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    SA = (A.RT.x - A.LB.x) * (A.RT.y - A.LB.y)</span><br><span class="line">    SB = (B.RT.x - B.LB.x) * (B.RT.y - B.LB.y)</span><br><span class="line">    cross = W * H</span><br><span class="line">    <span class="keyword">return</span> cross/(SA + SB - cross)</span><br></pre></td></tr></table></figure>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/rafaelpadilla/Object-Detection-Metrics">https://github.com/rafaelpadilla/Object-Detection-Metrics</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/jiongnima/article/details/84750819">mIoU（平均交并比）计算代码与逐行解析</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/wasidennis/AdaptSegNet/blob/master/compute_iou.py">https://github.com/wasidennis/AdaptSegNet/blob/master/compute_iou.py</a></li>
<li><a target="_blank" rel="noopener" href="https://tianws.github.io/skill/2018/10/30/miou/">mIoU源码解析</a></li>
</ul>
<h2 id="mAP"><a href="#mAP" class="headerlink" title="mAP"></a>mAP</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/55575423">参考资料</a></p>
<p>mAP定义及相关概念</p>
<ul>
<li>mAP: mean Average Precision, 即各类别AP的平均值</li>
<li>AP: PR曲线下面积，后文会详细讲解</li>
<li>PR曲线: Precision-Recall曲线</li>
<li>Precision: TP / (TP + FP)</li>
<li>Recall: TP / (TP + FN)</li>
<li>TP: IoU&gt;0.5的检测框数量（同一Ground Truth只计算一次）</li>
<li>FP: IoU&lt;=0.5的检测框，或者是检测到同一个GT的多余检测框的数量</li>
<li>FN: 没有检测到的GT的数量</li>
</ul>
<p>本笔记介绍目标检测的一个基本概念：AP、mAP（mean Average Precision），做目标检测的同学想必对这个词语耳熟能详了，不管是Pascal VOC，还是COCO，甚至是人脸检测的wider face数据集，都使用到了AP、mAP的评估方式，那么AP、mAP到底是什么？如何计算的？</p>
<p>如果希望一篇笔记讲明白目标检测中的mAP，感觉自己表达能力有限，可能搞不定，但如果希望一下能明白mAP含义的，可以参照引用链接；今天主要介绍下mAP的计算方式，假定前提为已经明白了precision、recall、tp、fp等概念，当然了，不明白也没关系，下一篇介绍Pascal VOC评估工具时会再详细介绍；</p>
<p><strong>1 图像检索mAP</strong></p>
<p>那么mAP到底是什么东西，如何计算？网上已经有了很多很多资料，但其实很多感觉都讲不清楚，我看到过一个在图像检索里面介绍得最好的示意图，我们先以图像检索中的mAP为例说明，其实目标检测中mAP与之几乎一样：</p>
<p><img src="https://pic2.zhimg.com/80/v2-7e1dd60163df014ad08ea15388fedd51_hd.jpg" alt="img"></p>
<p>以上是图像检索中mAP的计算案例，简要说明下：</p>
<p>1 查询图片1在图像库中检索相似图像，假设图像库中有五张相似图像，表示为图片1、…、图片5，排名不分先后；</p>
<p>2 检索（过程略），返回了top-10图像，如上图第二行，橙色表示相似的图像，灰色为无关图像；</p>
<p>3 接下来就是precision、recall的计算过程了，结合上图比较容易理解；</p>
<p>以返回图片6的节点为例：</p>
<p>top-6中，有3张图像确实为相似图像，另三张图像为无关图像，因此precision = 3 / 6；同时，总共五张相似图像，top-6检索出来了三张，因此recall = 3 / 5；</p>
<p>4 然后计算AP，可以看右边的计算方式，可以发现是把列出来的查询率(precision)相加取平均，那么最关键的问题来了：为什么选择这几张图像的precision求平均？可惜图中并没有告诉我们原因；</p>
<p>但其实不难，一句话就是：<strong>选择每个recall区间内对应的最高precision</strong>；</p>
<p>举个栗子，以上图橙色检索案例为例，当我们只选择top-1作为检索结果返回（也即只返回一个检索结果）时，检索性能为：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">top-1：recall = 1 / 5、precision = 1 / 1；# 以下类推；</span><br><span class="line">top-2：recall = 1 / 5、precision = 1 / 2；</span><br><span class="line">top-3：recall = 2 / 5、precision = 2 / 3；</span><br><span class="line">top-4：recall = 2 / 5、precision = 2 / 4；</span><br><span class="line">top-5：recall = 2 / 5、precision = 2 / 5；</span><br><span class="line">top-6：recall = 3 / 5、precision = 3 / 6；</span><br><span class="line">top-7：recall = 3 / 5、precision = 3 / 7；</span><br><span class="line">top-8：recall = 3 / 5、precision = 3 / 8；</span><br><span class="line">top-9：recall = 4 / 5、precision = 4 / 9；</span><br><span class="line">top-10：recall = 5 / 5、precision = 5 / 10；</span><br></pre></td></tr></table></figure>
<p>结合上面清单，先找找recall = 1 / 5区间下的最高precision，对应着precision = 1 / 1；</p>
<p>同理，recall = 2 / 5区间下的最高precision，对应着precision = 2 / 3；</p>
<p>recall = 3 / 5区间下的最高precision，对应着precision = 3 / 6；依次类推；</p>
<p>这样AP = (1 / 1 + 2 / 3 + 3 / 6 + 4 / 9 + 5 / 10) / 5；</p>
<p>那么mAP是啥？计算所有检索图像返回的AP均值，对应上图就是橙、绿突图像计算AP求均值，对应红色框；</p>
<p>这样mAP就计算完毕啦~~~是不是很容易理解？目标检测的mAP也是类似操作了；</p>
<p><strong>2 目标检测中mAP计算流程</strong></p>
<p>这里面我引用的是一篇博文，以下内容大多参考该博文，做了一些小修改；</p>
<p>下面的例子也很容易理解，假设检测人脸吧，gt label表示1为人脸，0为bg，某张图像中共检出了20个pred bbox，id：1 ~ 20，并对应了confidence score，gt label也很容易获得，pred bbox与gt bbox算IoU，给定一个threshold，那么就<strong>知道该pred bbox是否为正确的预测结果了，就对应了其gt label</strong>；—— 其实下表不应该这么理解的，但我们还是先这么认为，忽略差异吧，先直捣黄龙，table 1：</p>
<p><img src="https://pic3.zhimg.com/80/v2-f3d821d5661e41f6bbeddea2a7ce4972_hd.jpg" alt="img"></p>
<p>接下来对confidence score排序，得到table 2：</p>
<p><img src="https://pic1.zhimg.com/80/v2-dbcb5bac2c1e97e151cfe756d5cc55e8_hd.jpg" alt="img"></p>
<p><em>这张表很重要，接下来的precision和recall都是依照这个表计算的﻿，那么这里的confidence score其实就和图像检索中的相似度关联上了，具体地，就是如第一节的图像检索中，虽然我们计算mAP没在乎其检索返回的先后顺序，但top1肯定是与待检索图像最相似的，对应的similarity score最高，对人脸检测而言，pred bbox的confidence score最高，也说明该bbox最有可能是人脸；</em></p>
<p>然后计算precision和recall，这两个标准的定义如下：</p>
<p><img src="https://pic1.zhimg.com/80/v2-6b533fc4b307c03992a07b08812a12e4_hd.jpg" alt="img"></p>
<p>上面的图看看就行，能理解就理解，不理解可以参照第一节图像检索的例子来理解；</p>
<p>现以返回的top-5结果为例，如table 3：</p>
<p><img src="https://pic1.zhimg.com/80/v2-30ee6334f6aa93f9d10889fa4a3d1a10_hd.jpg" alt="img"></p>
<p>在这个例子中，true positives就是指id = 4、2的pred bbox，false positives就是指id = 13、19、6的pred bbox。方框内圆圈外的元素（false negatives + true negatives）是相对于方框内的元素而言，在这个例子中，是指confidence score排在top-5之外的元素，即table 4：</p>
<p><img src="https://pic1.zhimg.com/80/v2-e01ddf90fc9862e12ae5ab0d7416bc10_hd.jpg" alt="img"></p>
<p>其中，false negatives是指id = 9、16、7、20的4个pred bbox，true negatives是指id = 1、18、5、15、10、17、12、14、8、11、3的11个pred bbox；</p>
<p>那么，这个例子中Precision = 2 / 5 = 40%，意思是对于人脸检测而言，我们选定了5 pred bbox，其中正确的有2个，即准确率为40%；Recall = 2 / 6 = 33%，意思是该图像中共有6个人脸，但是因为我们只召回了2个，所以召回率为33%；</p>
<p>实际的目标检测任务中，我们通常不满足只通过top-5来衡量一个模型的好坏，而是需要知道从top-1到top-N（N是所有pred bbox，本文中为20）对应的precision和recall；显然随着我们选定的pred bbox越来也多，recall一定会越来越高，而precision整体上会呈下降趋势；把recall当成横坐标，precision当成纵坐标，即可得到常用的precision-recall曲线，以上例子的precision-recall曲线如fig 1：</p>
<p><img src="https://pic3.zhimg.com/80/v2-46dbabe907e601580c065aa03ee1a89a_hd.jpg" alt="img"></p>
<p>以上图像如何计算的？可以参照第一节图像检索中的栗子，还是比较容易理解的吧；</p>
<p>上面的每个红点，就相当于根据table 2，按照第一节中图像检索的方式计算出来的，也可以直接参照下面的table 5，自己心里算一算；</p>
<p>那么按照<strong>选择每个recall区间内对应的最高precision</strong>的计算方案，各个recall区间内对应的top-precision，就刚好如fig 1中的绿色框位置，可以进一步结合table 5中的绿色框理解；</p>
<p>好了，那么对这张图像而言，其AP = （1 / 1 + 2 / 2 + 3 / 6 + 4 / 7 + 5 / 11 + 6 / 16）/ 6；这是针对单张图像而言，所有图像也类似方式计算，那么就可以根据所有图像上的pred bbox，采用同样的方式，就计算出了所有图像上人脸这个类的AP；因为人脸检测只有一个类，如Pascal VOC这种20类的，每类都可以计算出一个AP，那么AP_total / 20，就是mAP啦；</p>
<p>但是等等，有没有发现table 5中，计算方式好像跟我们讲的有一点不一样？我们继续看看；</p>
<p><strong>3 Pascal VOC的两套mAP评估标准</strong></p>
<p>Pascal VOC中对mAP的计算经历了两次迭代，一种是VOC07的计算标准，对应绿色框：</p>
<p>首先设定一组阈值，T = [0、0.1、0.2、…、1]，然后对于recall大于每一个阈值Ti（比如recall &gt; 0.3），我们都会在该recall区间内得到一个对应的最大precision，这样我们就计算出了11个precision；——- 这里与上两节介绍的概念是一样的，只不过上面recall的区间是参照gt label来划分的，这里是我们人为划分的11个节点；</p>
<p>AP即为这11个precision的平均值，这种方法英文叫做11-point interpolated average precision；有了一个类的AP，所有类的AP均值即为mAP；</p>
<p>另一种是VOC10的计算标准，对应白色框：</p>
<p>新的计算方法假设N个pred bbox中有M个gt bbox，那么我们会得到M个recall节点（1 / M、2 / M、…、 M / M），对于<strong>每个recall值 r，我们可以计算出对应（r’ &gt; r）的最大precision，然后对这M个precision值取平均即得到最后的AP值</strong>，计算方法如table 5：</p>
<p><img src="https://pic4.zhimg.com/80/v2-525566cf829e30dcdc4156a3ada7303f_hd.jpg" alt="img"></p>
<p>从VOC07的绿框、VOC10的白框对比可知，差异主要在recall = 3 / 6下的precision，可以发现VOC07找的top-precision是在该recall区间段内的，但<strong>VOC10相当于是向后查找的，需确保该recall阈值以后的区间内，对应的是top-precision</strong>，可知4 / 7 &gt; 3 / 6，因此使用4 / 7替换了3 / 6，其他recall阈值下的操作方式类似；</p>
<p><strong>那么代码的实操中，就得从按照recall阈值从后往前计算了，这样就可以一遍就梭哈出所有结果，如果按recall从前往后计算，就有很多重复性计算（不断地重复向后recall区间内查找top-precision），然后呢，就可以使用到动态规划的方式做了，理论结合实践啊有木有~~~</strong></p>
<p>那么VOC10下，相应的Precision-Recall曲线如fig 2，可以发现这条曲线是单调递减的，剩下的AP计算方式就与VOC07相同了：</p>
<p>这里还需要继续一点，<strong>VOC07是11点插值的AP方式，等于是卡了11个离散的点，划分10个区间来计算AP</strong>，但VOC10是是<strong>根据recall值变化的区间来计算的</strong>，在这个栗子里，recall只变化了6次，但如果recall变化很多次，如100次、1000次、9999次等，就可以认为是<strong>一种 “伪” 连续的方式计算</strong>了；</p>
<p><img src="https://pic3.zhimg.com/80/v2-f86ce8588802e5cfee2d2f09303f98d2_hd.jpg" alt="img"></p>
<p><strong>总结</strong>：</p>
<p>AP衡量的是模型在每个类别上的好坏，mAP衡量的是模型在所有类别上的好坏，得到AP后mAP的计算就变得很简单了，就是取所有类别AP的平均值。</p>
<p><strong>3 代码</strong></p>
<p>直接上代码吧，这个函数假设我们已经得到了排序好的precision、recall的list，对应上图fig 2，进一步可以参照第一节中的清单理解；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># VOC-style mAP，分为两个计算方式，之所有两个计算方式，是因为2010年后VOC更新了评估方法，因此就有了07-metric和else...</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">voc_ap</span>(<span class="params">rec, prec, use_07_metric=False</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    average precision calculations</span></span><br><span class="line"><span class="string">    [precision integrated to recall]</span></span><br><span class="line"><span class="string">    :param rec: recall list</span></span><br><span class="line"><span class="string">    :param prec: precision list</span></span><br><span class="line"><span class="string">    :param use_07_metric: 2007 metric is 11-recall-point based AP</span></span><br><span class="line"><span class="string">    :return: average precision</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> use_07_metric:</span><br><span class="line">        <span class="comment"># 11 point metric</span></span><br><span class="line">        ap = <span class="number">0.</span></span><br><span class="line">        <span class="comment"># VOC07是11点插值的AP方式，等于是卡了11个离散的点，划分10个区间来计算AP</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> np.arange(<span class="number">0.</span>, <span class="number">1.1</span>, <span class="number">0.1</span>):</span><br><span class="line">            <span class="keyword">if</span> np.sum(rec &gt;= t) == <span class="number">0</span>:</span><br><span class="line">                p = <span class="number">0</span>    <span class="comment"># recall卡的阈值到顶了，1.1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                p = np.max(prec[rec &gt;= t])   <span class="comment"># VOC07：选择每个recall区间内对应的最高precision的计算方案</span></span><br><span class="line">            ap = ap + p / <span class="number">11.</span>    <span class="comment"># 11-recall-point based AP</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># correct AP calculation</span></span><br><span class="line">        <span class="comment"># first append sentinel values at the end</span></span><br><span class="line">        mrec = np.concatenate(([<span class="number">0.</span>], rec, [<span class="number">1.</span>]))</span><br><span class="line">        mpre = np.concatenate(([<span class="number">0.</span>], prec, [<span class="number">0.</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute the precision envelope</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(mpre.size - <span class="number">1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">            mpre[i - <span class="number">1</span>] = np.maximum(mpre[i - <span class="number">1</span>], mpre[i])    <span class="comment"># 这个是不是动态规划？从后往前找之前区间内的top-precision，多么优雅的代码呀~~~</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># to calculate area under PR curve, look for points where X axis (recall) changes value</span></span><br><span class="line">        <span class="comment"># 上面的英文，可以结合着fig 2的绿框理解，一目了然</span></span><br><span class="line">        <span class="comment"># VOC10是是根据recall值变化的区间来计算的，如果recall变化很多次，就可以认为是一种 “伪” 连续的方式计算了，以下求的是recall的变化</span></span><br><span class="line">        i = np.where(mrec[<span class="number">1</span>:] != mrec[:<span class="number">-1</span>])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算AP，这个计算方式有点玄乎，是一个积分公式的简化，应该是对应的fig 2中红色曲线以下的面积，之前公式的推导我有看过，现在有点忘了，麻烦各位同学补充一下</span></span><br><span class="line">        <span class="comment"># 现在理解了，不难，公式：sum (\Delta recall) * prec，其实结合fig2和下面的图，不就是算的积分么？如果recall划分得足够细，就可以当做连续数据，然后以下公式就是积分公式，算的precision、recall下面的面积了</span></span><br><span class="line">        ap = np.sum((mrec[i + <span class="number">1</span>] - mrec[i]) * mpre[i + <span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> ap</span><br></pre></td></tr></table></figure>
<p>通常VOC10标准下计算的mAP值会高于VOC07，原因如下，我就不详细介绍了：</p>
<blockquote>
<p><strong>Interpolated average precision</strong><br>Some authors choose an alternate approximation that is called the <em>interpolated average precision</em>. Often, they still call it average precision. Instead of using <em>P(k)</em>, the precision at a retrieval cutoff of <em>k</em> images, the interpolated average precision uses:</p>
</blockquote>
<p><img src="https://pic1.zhimg.com/80/v2-5bf4a2d116d55aa4685df9a10488fce0_hd.jpg" alt="img"></p>
<blockquote>
<p>In other words, instead of using the precision that was actually observed at cutoff <em>k</em>, the interpolated average precision uses the maximum precision observed across all cutoffs with higher recall. The full equation for computing the interpolated average precision is:</p>
</blockquote>
<p><img src="https://pic3.zhimg.com/80/v2-bce2e48f2913849f4029404cfbde9616_hd.jpg" alt="img"></p>
<blockquote>
<p>Visually, here’s how the interpolated average precision compares to the approximated average precision (to show a more interesting plot, this one isn’t from the earlier example):</p>
</blockquote>
<p><img src="https://pic1.zhimg.com/80/v2-7e00ce50249def8536978cc12a5cafe0_hd.jpg" alt="img"></p>
<blockquote>
<p><em>The approximated average precision closely hugs the actually observed curve. The interpolated average precision over estimates the precision at many points and produces a higher average precision value than the approximated average precision.</em></p>
<p>Further, there are variations on where to take the samples when computing the interpolated average precision. Some take samples at a fixed 11 points from 0 to 1: {0, 0.1, 0.2, …, 0.9, 1.0}. This is called the 11-point interpolated average precision. Others sample at every <em>k</em> where the recall changes.</p>
</blockquote>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/48992451">目标检测番外篇(2)_mAP</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/53405779">目标检测中的mAP是什么含义？</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/rafaelpadilla/Object-Detection-Metrics">Object-Detection-Metrics</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/67279824">【目标检测】VOC mAP</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/60834912">白话mAP</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/60319755">Detection基础模块之（二）mAP</a></li>
</ul>
<h3 id="如何计算-mAP？"><a href="#如何计算-mAP？" class="headerlink" title="如何计算 mAP？"></a>如何计算 mAP？</h3><p>每类的AP/类别数</p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/Cartucho/mAP">https://github.com/Cartucho/mAP</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/rafaelpadilla/Object-Detection-Metrics">https://github.com/rafaelpadilla/Object-Detection-Metrics</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/67279824">【目标检测】VOC mAP</a></li>
</ul>
<h2 id="目标检测度量标准"><a href="#目标检测度量标准" class="headerlink" title="目标检测度量标准"></a>目标检测度量标准</h2><ul>
<li>mAP</li>
<li><p>FPS</p>
</li>
<li><p>[ ] TODO</p>
</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/rafaelpadilla/Object-Detection-Metrics">Object-Detection-Metrics</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/70306015">目标检测的性能评价指标</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/60794316">【目标检测】基础知识：IoU、NMS、Bounding box regression</a></li>
</ul>
<h2 id="图像分割度量标准"><a href="#图像分割度量标准" class="headerlink" title="图像分割度量标准"></a>图像分割度量标准</h2><ul>
<li>[ ] TODO</li>
<li>PA</li>
<li>MP</li>
<li>mIoU</li>
<li>FWIoU</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IOUMetric</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Class to calculate mean-iou using fast_hist method</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_classes</span>):</span></span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.hist = np.zeros((num_classes, num_classes))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_fast_hist</span>(<span class="params">self, label_pred, label_true</span>):</span></span><br><span class="line">        mask = (label_true &gt;= <span class="number">0</span>) &amp; (label_true &lt; self.num_classes)</span><br><span class="line">        hist = np.bincount(</span><br><span class="line">            self.num_classes * label_true[mask].astype(int) +</span><br><span class="line">            label_pred[mask], minlength=self.num_classes ** <span class="number">2</span>).reshape(self.num_classes, self.num_classes)</span><br><span class="line">        <span class="keyword">return</span> hist</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_batch</span>(<span class="params">self, predictions, gts</span>):</span></span><br><span class="line">        <span class="keyword">for</span> lp, lt <span class="keyword">in</span> zip(predictions, gts):</span><br><span class="line">            self.hist += self._fast_hist(lp.flatten(), lt.flatten())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span>(<span class="params">self</span>):</span></span><br><span class="line">        acc = np.diag(self.hist).sum() / self.hist.sum()</span><br><span class="line">        acc_cls = np.diag(self.hist) / self.hist.sum(axis=<span class="number">1</span>)</span><br><span class="line">        acc_cls = np.nanmean(acc_cls)</span><br><span class="line">        iu = np.diag(self.hist) / (self.hist.sum(axis=<span class="number">1</span>) + self.hist.sum(axis=<span class="number">0</span>) - np.diag(self.hist))</span><br><span class="line">        mean_iu = np.nanmean(iu)</span><br><span class="line">        freq = self.hist.sum(axis=<span class="number">1</span>) / self.hist.sum()</span><br><span class="line">        fwavacc = (freq[freq &gt; <span class="number">0</span>] * iu[freq &gt; <span class="number">0</span>]).sum()</span><br><span class="line">        <span class="keyword">return</span> acc, acc_cls, iu, mean_iu, fwavacc</span><br></pre></td></tr></table></figure>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1704.06857">《A Review on Deep Learning Techniques Applied to Semantic Segmentation》</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/38236530">图像语义分割准确率度量方法总结</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/u014593748/article/details/71698246">论文笔记 |　基于深度学习的图像语义分割技术概述之5.1度量标准</a></li>
</ul>
<h2 id="非极大值抑制NMS"><a href="#非极大值抑制NMS" class="headerlink" title="非极大值抑制NMS"></a>非极大值抑制NMS</h2><p>NMS的作用是将重复的检测框去掉</p>
<p>pytorch IoU代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># IOU计算</span></span><br><span class="line">    <span class="comment"># 假设box1维度为[N,4]   box2维度为[M,4]</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">iou</span>(<span class="params">self, box1, box2</span>):</span></span><br><span class="line">        N = box1.size(<span class="number">0</span>)</span><br><span class="line">        M = box2.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        lt = torch.max(  <span class="comment"># 左上角的点</span></span><br><span class="line">            box1[:, :<span class="number">2</span>].unsqueeze(<span class="number">1</span>).expand(N, M, <span class="number">2</span>),   <span class="comment"># [N,2]-&gt;[N,1,2]-&gt;[N,M,2]</span></span><br><span class="line">            box2[:, :<span class="number">2</span>].unsqueeze(<span class="number">0</span>).expand(N, M, <span class="number">2</span>),   <span class="comment"># [M,2]-&gt;[1,M,2]-&gt;[N,M,2]</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        rb = torch.min(</span><br><span class="line">            box1[:, <span class="number">2</span>:].unsqueeze(<span class="number">1</span>).expand(N, M, <span class="number">2</span>),</span><br><span class="line">            box2[:, <span class="number">2</span>:].unsqueeze(<span class="number">0</span>).expand(N, M, <span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        wh = rb - lt  <span class="comment"># [N,M,2]</span></span><br><span class="line">        wh[wh &lt; <span class="number">0</span>] = <span class="number">0</span>   <span class="comment"># 两个box没有重叠区域</span></span><br><span class="line">        inter = wh[:,:,<span class="number">0</span>] * wh[:,:,<span class="number">1</span>]   <span class="comment"># [N,M]</span></span><br><span class="line"></span><br><span class="line">        area1 = (box1[:,<span class="number">2</span>]-box1[:,<span class="number">0</span>]) * (box1[:,<span class="number">3</span>]-box1[:,<span class="number">1</span>])  <span class="comment"># (N,)</span></span><br><span class="line">        area2 = (box2[:,<span class="number">2</span>]-box2[:,<span class="number">0</span>]) * (box2[:,<span class="number">3</span>]-box2[:,<span class="number">1</span>])  <span class="comment"># (M,)</span></span><br><span class="line">        area1 = area1.unsqueeze(<span class="number">1</span>).expand(N,M)  <span class="comment"># (N,M)</span></span><br><span class="line">        area2 = area2.unsqueeze(<span class="number">0</span>).expand(N,M)  <span class="comment"># (N,M)</span></span><br><span class="line"></span><br><span class="line">        iou = inter / (area1+area2-inter)</span><br><span class="line">        <span class="keyword">return</span> iou</span><br></pre></td></tr></table></figure>
<p>NMS</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># NMS算法</span></span><br><span class="line">    <span class="comment"># bboxes维度为[N,4]，scores维度为[N,], 均为tensor</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">nms</span>(<span class="params">self, bboxes, scores, threshold=<span class="number">0.5</span></span>):</span></span><br><span class="line">        x1 = bboxes[:,<span class="number">0</span>]</span><br><span class="line">        y1 = bboxes[:,<span class="number">1</span>]</span><br><span class="line">        x2 = bboxes[:,<span class="number">2</span>]</span><br><span class="line">        y2 = bboxes[:,<span class="number">3</span>]</span><br><span class="line">        areas = (x2-x1)*(y2-y1)   <span class="comment"># [N,] 每个bbox的面积</span></span><br><span class="line">        _, order = scores.sort(<span class="number">0</span>, descending=<span class="literal">True</span>)    <span class="comment"># 降序排列 这里返回的是indices</span></span><br><span class="line"></span><br><span class="line">        keep = []</span><br><span class="line">        <span class="keyword">while</span> order.numel() &gt; <span class="number">0</span>:       <span class="comment"># torch.numel()返回张量元素个数</span></span><br><span class="line">            <span class="keyword">if</span> order.numel() == <span class="number">1</span>:     <span class="comment"># 保留框只剩一个</span></span><br><span class="line">                i = order.item()</span><br><span class="line">                keep.append(i)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                i = order[<span class="number">0</span>].item()    <span class="comment"># 保留scores最大的那个框box[i]</span></span><br><span class="line">                keep.append(i)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算box[i]与其余各框的IOU(思路很好)</span></span><br><span class="line">            xx1 = x1[order[<span class="number">1</span>:]].clamp(min=x1[i])   <span class="comment"># [N-1,]</span></span><br><span class="line">            yy1 = y1[order[<span class="number">1</span>:]].clamp(min=y1[i])</span><br><span class="line">            xx2 = x2[order[<span class="number">1</span>:]].clamp(max=x2[i])</span><br><span class="line">            yy2 = y2[order[<span class="number">1</span>:]].clamp(max=y2[i])</span><br><span class="line">            inter = (xx2-xx1).clamp(min=<span class="number">0</span>) * (yy2-yy1).clamp(min=<span class="number">0</span>)   <span class="comment"># [N-1,]</span></span><br><span class="line"></span><br><span class="line">            iou = inter / (areas[i]+areas[order[<span class="number">1</span>:]]-inter)  <span class="comment"># [N-1,]</span></span><br><span class="line">            idx = (iou &lt;= threshold).nonzero().squeeze() <span class="comment"># 注意此时idx为[N-1,] 而order为[N,]</span></span><br><span class="line">            <span class="keyword">if</span> idx.numel() == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            order = order[idx+<span class="number">1</span>]  <span class="comment"># 修补索引之间的差值</span></span><br><span class="line">        <span class="keyword">return</span> torch.LongTensor(keep)   <span class="comment"># Pytorch的索引值为LongTensor</span></span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201129150151.png" style="zoom:50%;"></p>
<h2 id="目标检测中的Anchor"><a href="#目标检测中的Anchor" class="headerlink" title="目标检测中的Anchor"></a>目标检测中的Anchor</h2><p>anchor技术将问题转换为<strong>“这个固定参考框中有没有认识的目标，目标框偏离参考框多远”</strong>，不再需要多尺度遍历滑窗</p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201201171632.png" style="zoom: 80%;"></p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/55824651">目标检测中的Anchor</a></li>
</ul>
<h2 id="原始图片中的ROI如何映射到到feature-map"><a href="#原始图片中的ROI如何映射到到feature-map" class="headerlink" title="原始图片中的ROI如何映射到到feature map?"></a>原始图片中的ROI如何映射到到feature map?</h2><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/24780433">https://zhuanlan.zhihu.com/p/24780433</a></li>
<li><a target="_blank" rel="noopener" href="http://www.cnblogs.com/objectDetect/p/5947169.html">http://www.cnblogs.com/objectDetect/p/5947169.html</a></li>
</ul>
<h2 id="请问Faster-R-CNN和SSD-中为什么用smooth-l1-loss，和l2有什么区别？"><a href="#请问Faster-R-CNN和SSD-中为什么用smooth-l1-loss，和l2有什么区别？" class="headerlink" title="请问Faster R-CNN和SSD 中为什么用smooth l1 loss，和l2有什么区别？"></a>请问Faster R-CNN和SSD 中为什么用smooth l1 loss，和l2有什么区别？</h2><ol>
<li>当预测框与 ground truth 差别过大时，梯度值不至于过大；</li>
<li>当预测框与 ground truth 差别很小时，梯度值足够小。</li>
</ol>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/58200555/answer/621174180">请问faster rcnn和ssd 中为什么用smooth l1 loss，和l2有什么区别？</a></li>
</ul>
<h2 id="给定5个人脸关键点和5个对齐后的点，求怎么变换的？"><a href="#给定5个人脸关键点和5个对齐后的点，求怎么变换的？" class="headerlink" title="给定5个人脸关键点和5个对齐后的点，求怎么变换的？"></a>给定5个人脸关键点和5个对齐后的点，求怎么变换的？</h2><ul>
<li>[ ] TODO</li>
</ul>
<h2 id="Bounding-boxes-回归原理-公式"><a href="#Bounding-boxes-回归原理-公式" class="headerlink" title="Bounding boxes 回归原理/公式"></a>Bounding boxes 回归原理/公式</h2><ul>
<li>[ ] TODO</li>
</ul>
<h2 id="U-Net-和-FCN的区别"><a href="#U-Net-和-FCN的区别" class="headerlink" title="U-Net 和 FCN的区别"></a>U-Net 和 FCN的区别</h2><ul>
<li>[ ] TODO</li>
</ul>
<h2 id="介绍KCF算法"><a href="#介绍KCF算法" class="headerlink" title="介绍KCF算法"></a>介绍KCF算法</h2><ul>
<li>[ ] TODO</li>
</ul>
<h2 id="介绍MobileNet-SSD算法"><a href="#介绍MobileNet-SSD算法" class="headerlink" title="介绍MobileNet-SSD算法"></a>介绍MobileNet-SSD算法</h2><ul>
<li>[ ] TODO</li>
</ul>
<h2 id="目标检测中的多尺度训练-测试？"><a href="#目标检测中的多尺度训练-测试？" class="headerlink" title="目标检测中的多尺度训练/测试？"></a>目标检测中的多尺度训练/测试？</h2><ul>
<li>[ ] TODO</li>
</ul>
<p>多尺度训练对全卷积网络有效，一般设置几种不同尺度的图片，训练时每隔一定iterations随机选取一种尺度训练。这样训练出来的模型鲁棒性强，其可以接受任意大小的图片作为输入，使用尺度小的图片测试速度会快些，但准确度低，用尺度大的图片测试速度慢，但是准确度高。</p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/271781123">目标检测中的多尺度训练/测试？</a></li>
</ul>
<h2 id="目标检测中的正负样本不平衡问题"><a href="#目标检测中的正负样本不平衡问题" class="headerlink" title="目标检测中的正负样本不平衡问题"></a>目标检测中的正负样本不平衡问题</h2><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1604.03540">OHEM</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1708.02002">Focal Loss</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.05181">GHM</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.04821">PISA</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.06373">AP-loss</a></li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/55036597">样本贡献不均：Focal Loss和 Gradient Harmonizing Mechanism</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/62314673">被忽略的Focal Loss变种</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/63954517">Soft Sampling：探索更有效的采样策略</a>：介绍了<strong>Focal Loss</strong>、<strong>GHM</strong>和<strong>PISA</strong></li>
</ul>
<h2 id="目标检测中的类别漏检问题该怎么解决？"><a href="#目标检测中的类别漏检问题该怎么解决？" class="headerlink" title="目标检测中的类别漏检问题该怎么解决？"></a>目标检测中的类别漏检问题该怎么解决？</h2><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/372208101">目标检测中的类别漏检问题该怎么解决？</a></li>
</ul>
<h2 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h2><h3 id="RPN-的损失函数"><a href="#RPN-的损失函数" class="headerlink" title="RPN 的损失函数"></a>RPN 的损失函数</h3><ul>
<li>[ ] TODO</li>
</ul>
<h3 id="RPN中的anchor-box是怎么选取的？"><a href="#RPN中的anchor-box是怎么选取的？" class="headerlink" title="RPN中的anchor box是怎么选取的？"></a>RPN中的anchor box是怎么选取的？</h3><ul>
<li>[ ] TODO</li>
</ul>
<h2 id="RoI-Pooling"><a href="#RoI-Pooling" class="headerlink" title="RoI Pooling"></a>RoI Pooling</h2><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/59692298">你真的学会RoI Pooling了吗?</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/46927880">IoUNet(5)源码 RoIPooling(1)</a>  </li>
</ul>
<h2 id="RoI-Align"><a href="#RoI-Align" class="headerlink" title="RoI Align"></a>RoI Align</h2><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/46928697">IoUNet(6) 源码 RoIAlign(1)</a></li>
</ul>
<h2 id="为什么深度学习中的图像分割要先编码再解码？"><a href="#为什么深度学习中的图像分割要先编码再解码？" class="headerlink" title="为什么深度学习中的图像分割要先编码再解码？"></a>为什么深度学习中的图像分割要先编码再解码？</h2><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/322191738">为什么深度学习中的图像分割要先编码再解码？</a></li>
</ul>
<h2 id="NMS"><a href="#NMS" class="headerlink" title="NMS"></a>NMS</h2><p>本笔记介绍目标检测的另一个基本概念：NMS（non-maximum suppression），做目标检测的同学想必对这个词语耳熟能详了；</p>
<p>在检测图像中的目标时，不可避免地会检出很多bboxes + cls scores，这些bbox之间有很多是冗余的，一个目标可能会被多个bboxes检出，如果所有bboxes都输出，就很影响体验和美观了（同一个目标输出100个bboxes，想想都后怕~~~），一种方案就是提升cls scores的阈值，减少bbox数量的输出；另一种方案就是使用NMS，将同一目标内的bboxes按照cls score + IoU阈值做筛选，剔除冗余地、低置信度的bbox；</p>
<p>可能又会问了：为什么目标检测时，会有这么多无效、冗余检测框呢？这个。。。我的理解，是因为图像中没有目标尺度、位置的先验知识，为保证对目标的高召回，就必须使用滑窗、anchor / default bbox密集采样的方式，尽管检测模型能对每个anchor / default bbox做出 cls + reg，可以一定程度上剔除误检，但没有结合检出bbox的cls score + IoU阈值做筛选，而NMS就可以做到这一点；</p>
<p><strong>1 NMS操作流程</strong></p>
<p>NMS用于剔除图像中检出的冗余bbox，标准NMS的具体做法为：</p>
<p><strong>step-1</strong>：将所有检出的output_bbox按cls score划分（如pascal voc分20个类，也即将output_bbox按照其对应的cls score划分为21个集合，1个bg类，只不过bg类就没必要做NMS而已）；</p>
<p><strong>step-2</strong>：在每个集合内根据各个bbox的cls score做降序排列，得到一个降序的list_k；</p>
<p><strong>step-3</strong>：从list_k中top1 cls score开始，计算该bbox_x与list中其他bbox_y的IoU，若IoU大于阈值T，则剔除该bbox_y，最终保留bbox_x，从list_k中取出；</p>
<p><strong>step-4</strong>：选择list_k中top2 cls score(步骤3取出top 1 bbox_x后，原list_k中的top 2就相当于现list_k中的top 1了，但如果step-3中剔除的bbox_y刚好是原list_k中的top 2，就依次找top 3即可，理解这么个意思就行)，重复step-3中的迭代操作，直至list_k中所有bbox都完成筛选；</p>
<p><strong>step-5</strong>：对每个集合的list_k，重复step-3、4中的迭代操作，直至所有list_k都完成筛选；</p>
<p>以上操作写的有点绕，不过如果理解NMS操作流程的话，再结合下图，应该还是非常好理解的；</p>
<p><img src="https://pic3.zhimg.com/80/v2-44f9d8d3f66e59e407a4edb5a02ea4ea_hd.jpg" alt="img"></p>
<p><strong>2 代码学习</strong></p>
<p><strong>2.1 test_RFB.py</strong></p>
<p>我选择了RFBNet里的代码介绍NMS，因为里面的流程基本上就是按照我说的操作进行了；</p>
<p>先看看test_RFB.py中的片段，通过以下代码可以发现，其对应着step-1、step5操作，就是说NMS操作是逐类进行的，图像中检出的所有bboxes，按照 cls 做划分，再每个类的bbox进一步做NMS；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">out = net(x)      <span class="comment"># forward pass，这里相当于将图像 x 输入RFBNet，得到了pred cls + reg</span></span><br><span class="line">boxes, scores = detector.forward(out,priors) <span class="comment"># 结合priors，将pred reg（也即预测的offsets）解码成最终的pred bbox，如果理解anchor / default bbox操作流程，这个应该很好理解的；</span></span><br><span class="line">boxes = boxes[<span class="number">0</span>]</span><br><span class="line">scores=scores[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># scale each detection back up to the image</span></span><br><span class="line">boxes *= scale   <span class="comment"># （0，1）区间坐标的bbox做尺度反正则化</span></span><br><span class="line">boxes = boxes.cpu().numpy()</span><br><span class="line">scores = scores.cpu().numpy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, num_classes):      <span class="comment"># 对每个类 j 的pred bbox单独做NMS，为什么index从1开始？因为0是bg，做NMS无意义</span></span><br><span class="line">    inds = np.where(scores[:, j] &gt; thresh)[<span class="number">0</span>]     <span class="comment"># 找到该类 j 下，所有cls score大于thresh的bbox，为什么选择大于thresh的bbox？因为score小于阈值的bbox，直接可以过滤掉，无需劳烦NMS</span></span><br><span class="line">    <span class="keyword">if</span> len(inds) == <span class="number">0</span>:    <span class="comment"># 没有满足条件的bbox，返回空，跳过；</span></span><br><span class="line">        all_boxes[j][i] = np.empty([<span class="number">0</span>, <span class="number">5</span>], dtype=np.float32)</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    c_bboxes = boxes[inds]</span><br><span class="line">    c_scores = scores[inds, j]   <span class="comment"># 找到对应类 j 下的score即可</span></span><br><span class="line">    c_dets = np.hstack((c_bboxes, c_scores[:, np.newaxis])).astype(</span><br><span class="line">        np.float32, copy=<span class="literal">False</span>)   <span class="comment"># 将满足条件的bbox + cls score的bbox通过hstack完成合体</span></span><br><span class="line"></span><br><span class="line">    keep = nms(c_dets, <span class="number">0.45</span>, force_cpu=args.cpu)    <span class="comment"># NMS，返回需保存的bbox index：keep</span></span><br><span class="line">    c_dets = c_dets[keep, :]</span><br><span class="line">    all_boxes[j][i] = c_dets     <span class="comment"># i 对应每张图像，j 对应图像中类别 j 的bbox清单</span></span><br></pre></td></tr></table></figure>
<p>介绍以上代码处理流程，<strong>两个目的</strong>：</p>
<p>1 test_RFB.py的处理流程非常清晰，也很方便我们的理解；</p>
<p>2 for j in range(1, num_classes)操作表明了，NMS是逐类进行的，也即参与NMS的bbox都属于同一类；</p>
<p><strong>2.2 py_cpu_nms.py</strong></p>
<p>代码同样来自于FRBNet，结合注释可以发现引自Fast R-CNN；</p>
<p>这个代码是最简版的nms，跟第一节中NMS处理流程一致，非常适合学习，可以作为baseline，我加了个简单的main函数做测试；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------------------------------------------------</span></span><br><span class="line"><span class="comment"># Fast R-CNN</span></span><br><span class="line"><span class="comment"># Copyright (c) 2015 Microsoft</span></span><br><span class="line"><span class="comment"># Licensed under The MIT License [see LICENSE for details]</span></span><br><span class="line"><span class="comment"># Written by Ross Girshick</span></span><br><span class="line"><span class="comment"># --------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">py_cpu_nms</span>(<span class="params">dets, thresh</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Pure Python NMS baseline.&quot;&quot;&quot;</span></span><br><span class="line">    x1 = dets[:, <span class="number">0</span>]                     <span class="comment"># pred bbox top_x</span></span><br><span class="line">    y1 = dets[:, <span class="number">1</span>]                     <span class="comment"># pred bbox top_y</span></span><br><span class="line">    x2 = dets[:, <span class="number">2</span>]                     <span class="comment"># pred bbox bottom_x</span></span><br><span class="line">    y2 = dets[:, <span class="number">3</span>]                     <span class="comment"># pred bbox bottom_y</span></span><br><span class="line">    scores = dets[:, <span class="number">4</span>]              <span class="comment"># pred bbox cls score</span></span><br><span class="line"></span><br><span class="line">    areas = (x2 - x1 + <span class="number">1</span>) * (y2 - y1 + <span class="number">1</span>)    <span class="comment"># pred bbox areas</span></span><br><span class="line">    order = scores.argsort()[::<span class="number">-1</span>]              <span class="comment"># 对pred bbox按score做降序排序，对应step-2</span></span><br><span class="line"></span><br><span class="line">    keep = []    <span class="comment"># NMS后，保留的pred bbox</span></span><br><span class="line">    <span class="keyword">while</span> order.size &gt; <span class="number">0</span>:</span><br><span class="line">        i = order[<span class="number">0</span>]          <span class="comment"># top-1 score bbox</span></span><br><span class="line">        keep.append(i)   <span class="comment"># top-1 score的话，自然就保留了</span></span><br><span class="line">        xx1 = np.maximum(x1[i], x1[order[<span class="number">1</span>:]])   <span class="comment"># top-1 bbox（score最大）与order中剩余bbox计算NMS</span></span><br><span class="line">        yy1 = np.maximum(y1[i], y1[order[<span class="number">1</span>:]])</span><br><span class="line">        xx2 = np.minimum(x2[i], x2[order[<span class="number">1</span>:]])</span><br><span class="line">        yy2 = np.minimum(y2[i], y2[order[<span class="number">1</span>:]])</span><br><span class="line"></span><br><span class="line">        w = np.maximum(<span class="number">0.0</span>, xx2 - xx1 + <span class="number">1</span>)</span><br><span class="line">        h = np.maximum(<span class="number">0.0</span>, yy2 - yy1 + <span class="number">1</span>)</span><br><span class="line">        inter = w * h</span><br><span class="line">        ovr = inter / (areas[i] + areas[order[<span class="number">1</span>:]] - inter)      <span class="comment"># 无处不在的IoU计算~~~</span></span><br><span class="line"></span><br><span class="line">        inds = np.where(ovr &lt;= thresh)[<span class="number">0</span>]     <span class="comment"># 这个操作可以对代码断点调试理解下，结合step-3，我们希望剔除所有与当前top-1 bbox IoU &gt; thresh的冗余bbox，那么保留下来的bbox，自然就是ovr &lt;= thresh的非冗余bbox，其inds保留下来，作进一步筛选</span></span><br><span class="line">        order = order[inds + <span class="number">1</span>]   <span class="comment"># 保留有效bbox，就是这轮NMS未被抑制掉的幸运儿，为什么 + 1？因为ind = 0就是这轮NMS的top-1，剩余有效bbox在IoU计算中与top-1做的计算，inds对应回原数组，自然要做 +1 的映射，接下来就是step-4的循环</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> keep    <span class="comment"># 最终NMS结果返回</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    dets = np.array([[<span class="number">100</span>,<span class="number">120</span>,<span class="number">170</span>,<span class="number">200</span>,<span class="number">0.98</span>],</span><br><span class="line">                     [<span class="number">20</span>,<span class="number">40</span>,<span class="number">80</span>,<span class="number">90</span>,<span class="number">0.99</span>],</span><br><span class="line">                     [<span class="number">20</span>,<span class="number">38</span>,<span class="number">82</span>,<span class="number">88</span>,<span class="number">0.96</span>],</span><br><span class="line">                     [<span class="number">200</span>,<span class="number">380</span>,<span class="number">282</span>,<span class="number">488</span>,<span class="number">0.9</span>],</span><br><span class="line">                     [<span class="number">19</span>,<span class="number">38</span>,<span class="number">75</span>,<span class="number">91</span>, <span class="number">0.8</span>]])</span><br><span class="line"></span><br><span class="line">    py_cpu_nms(dets, <span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>
<p><strong>2.2 bbox_utils.py</strong></p>
<p>同样是RFBNet中的nms代码，用pytorch实现的，其实和2.1小节中的NMS操作完全一致；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Original author: Francisco Massa:</span></span><br><span class="line"><span class="comment"># https://github.com/fmassa/object-detection.torch</span></span><br><span class="line"><span class="comment"># Ported to PyTorch by Max deGroot (02/01/2017)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nms</span>(<span class="params">boxes, scores, overlap=<span class="number">0.5</span>, top_k=<span class="number">200</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Apply non-maximum suppression at test time to avoid detecting too many</span></span><br><span class="line"><span class="string">    overlapping bounding boxes for a given object. ---- 这里面有一个细节，NMS仅用于测试阶段，为什么不用于训练阶段呢？可以评论留言下，我就不解释了，嘿嘿~~~</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        boxes: (tensor) The location preds for the img, Shape: [num_priors,4].</span></span><br><span class="line"><span class="string">        scores: (tensor) The class predscores for the img, Shape:[num_priors].</span></span><br><span class="line"><span class="string">        overlap: (float) The overlap thresh for suppressing unnecessary boxes.</span></span><br><span class="line"><span class="string">        top_k: (int) The Maximum number of box preds to consider.</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        The indices of the kept boxes with respect to num_priors.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    keep = torch.Tensor(scores.size(<span class="number">0</span>)).fill_(<span class="number">0</span>).long()</span><br><span class="line">    <span class="keyword">if</span> boxes.numel() == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> keep</span><br><span class="line">    x1 = boxes[:, <span class="number">0</span>]</span><br><span class="line">    y1 = boxes[:, <span class="number">1</span>]</span><br><span class="line">    x2 = boxes[:, <span class="number">2</span>]</span><br><span class="line">    y2 = boxes[:, <span class="number">3</span>]</span><br><span class="line">    area = torch.mul(x2 - x1, y2 - y1)    <span class="comment"># IoU初步准备</span></span><br><span class="line">    v, idx = scores.sort(<span class="number">0</span>)  <span class="comment"># sort in ascending order，对应step-2，不过是升序操作，非降序</span></span><br><span class="line">    <span class="comment"># I = I[v &gt;= 0.01]</span></span><br><span class="line">    idx = idx[-top_k:]  <span class="comment"># indices of the top-k largest vals，依然是升序的结果</span></span><br><span class="line">    xx1 = boxes.new()</span><br><span class="line">    yy1 = boxes.new()</span><br><span class="line">    xx2 = boxes.new()</span><br><span class="line">    yy2 = boxes.new()</span><br><span class="line">    w = boxes.new()</span><br><span class="line">    h = boxes.new()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># keep = torch.Tensor()</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> idx.numel() &gt; <span class="number">0</span>:   <span class="comment"># 对应step-4，若所有pred bbox都处理完毕，就可以结束循环啦~</span></span><br><span class="line">        i = idx[<span class="number">-1</span>]  <span class="comment"># index of current largest val，top-1 score box，因为是升序的，所有返回index = -1的最后一个元素即可</span></span><br><span class="line">        <span class="comment"># keep.append(i)</span></span><br><span class="line">        keep[count] = i</span><br><span class="line">        count += <span class="number">1</span>    <span class="comment"># 不仅记数NMS保留的bbox个数，也作为index存储bbox</span></span><br><span class="line">        <span class="keyword">if</span> idx.size(<span class="number">0</span>) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        idx = idx[:<span class="number">-1</span>]  <span class="comment"># remove kept element from view，top-1已保存，不需要了~~~</span></span><br><span class="line">        <span class="comment"># load bboxes of next highest vals</span></span><br><span class="line">        torch.index_select(x1, <span class="number">0</span>, idx, out=xx1)</span><br><span class="line">        torch.index_select(y1, <span class="number">0</span>, idx, out=yy1)</span><br><span class="line">        torch.index_select(x2, <span class="number">0</span>, idx, out=xx2)</span><br><span class="line">        torch.index_select(y2, <span class="number">0</span>, idx, out=yy2)</span><br><span class="line">        <span class="comment"># store element-wise max with next highest score</span></span><br><span class="line">        xx1 = torch.clamp(xx1, min=x1[i])   <span class="comment"># 对应 np.maximum(x1[i], x1[order[1:]]) </span></span><br><span class="line">        yy1 = torch.clamp(yy1, min=y1[i])</span><br><span class="line">        xx2 = torch.clamp(xx2, max=x2[i])</span><br><span class="line">        yy2 = torch.clamp(yy2, max=y2[i])</span><br><span class="line">        w.resize_as_(xx2)</span><br><span class="line">        h.resize_as_(yy2)</span><br><span class="line">        w = xx2 - xx1</span><br><span class="line">        h = yy2 - yy1</span><br><span class="line">        <span class="comment"># check sizes of xx1 and xx2.. after each iteration</span></span><br><span class="line">        w = torch.clamp(w, min=<span class="number">0.0</span>)    <span class="comment"># clamp函数可以去查查，类似max、mini的操作</span></span><br><span class="line">        h = torch.clamp(h, min=<span class="number">0.0</span>)</span><br><span class="line">        inter = w*h</span><br><span class="line">        <span class="comment"># IoU = i / (area(a) + area(b) - i)     </span></span><br><span class="line">        <span class="comment"># 以下两步操作做了个优化，area已经计算好了，就可以直接根据idx读取结果了，area[i]同理，避免了不必要的冗余计算</span></span><br><span class="line">        rem_areas = torch.index_select(area, <span class="number">0</span>, idx)  <span class="comment"># load remaining areas)</span></span><br><span class="line">        union = (rem_areas - inter) + area[i]     <span class="comment"># 就是area(a) + area(b) - i</span></span><br><span class="line">        IoU = inter/union  <span class="comment"># store result in iou，# IoU来啦~~~</span></span><br><span class="line">        <span class="comment"># keep only elements with an IoU &lt;= overlap</span></span><br><span class="line">        idx = idx[IoU.le(overlap)]   <span class="comment"># 这一轮NMS操作，IoU阈值小于overlap的idx，就是需要保留的bbox，其他的就直接忽略吧，并进行下一轮计算</span></span><br><span class="line">    <span class="keyword">return</span> keep, count</span><br></pre></td></tr></table></figure>
<p><strong>2.2 cpu_nms.pyx</strong></p>
<p>同样在RGBNet项目中，下面就是优化后的NNS操作，以及soft-NMS操作，我就不细讲了~~~</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------------------------------------------------</span></span><br><span class="line"><span class="comment"># Fast R-CNN</span></span><br><span class="line"><span class="comment"># Copyright (c) 2015 Microsoft</span></span><br><span class="line"><span class="comment"># Licensed under The MIT License [see LICENSE for details]</span></span><br><span class="line"><span class="comment"># Written by Ross Girshick</span></span><br><span class="line"><span class="comment"># --------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">cimport numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">cdef inline np.float32_t max(np.float32_t a, np.float32_t b):</span><br><span class="line">    <span class="keyword">return</span> a <span class="keyword">if</span> a &gt;= b <span class="keyword">else</span> b</span><br><span class="line"></span><br><span class="line">cdef inline np.float32_t min(np.float32_t a, np.float32_t b):</span><br><span class="line">    <span class="keyword">return</span> a <span class="keyword">if</span> a &lt;= b <span class="keyword">else</span> b</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cpu_nms</span>(<span class="params">np.ndarray[np.float32_t, ndim=<span class="number">2</span>] dets, np.float thresh</span>):</span></span><br><span class="line">    cdef np.ndarray[np.float32_t, ndim=<span class="number">1</span>] x1 = dets[:, <span class="number">0</span>]</span><br><span class="line">    cdef np.ndarray[np.float32_t, ndim=<span class="number">1</span>] y1 = dets[:, <span class="number">1</span>]</span><br><span class="line">    cdef np.ndarray[np.float32_t, ndim=<span class="number">1</span>] x2 = dets[:, <span class="number">2</span>]</span><br><span class="line">    cdef np.ndarray[np.float32_t, ndim=<span class="number">1</span>] y2 = dets[:, <span class="number">3</span>]</span><br><span class="line">    cdef np.ndarray[np.float32_t, ndim=<span class="number">1</span>] scores = dets[:, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">    cdef np.ndarray[np.float32_t, ndim=<span class="number">1</span>] areas = (x2 - x1 + <span class="number">1</span>) * (y2 - y1 + <span class="number">1</span>)</span><br><span class="line">    cdef np.ndarray[np.int_t, ndim=<span class="number">1</span>] order = scores.argsort()[::<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    cdef int ndets = dets.shape[<span class="number">0</span>]</span><br><span class="line">    cdef np.ndarray[np.int_t, ndim=<span class="number">1</span>] suppressed = \</span><br><span class="line">            np.zeros((ndets), dtype=np.int)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># nominal indices</span></span><br><span class="line">    cdef int _i, _j</span><br><span class="line">    <span class="comment"># sorted indices</span></span><br><span class="line">    cdef int i, j</span><br><span class="line">    <span class="comment"># temp variables for box i&#x27;s (the box currently under consideration)</span></span><br><span class="line">    cdef np.float32_t ix1, iy1, ix2, iy2, iarea</span><br><span class="line">    <span class="comment"># variables for computing overlap with box j (lower scoring box)</span></span><br><span class="line">    cdef np.float32_t xx1, yy1, xx2, yy2</span><br><span class="line">    cdef np.float32_t w, h</span><br><span class="line">    cdef np.float32_t inter, ovr</span><br><span class="line"></span><br><span class="line">    keep = []</span><br><span class="line">    <span class="keyword">for</span> _i <span class="keyword">in</span> range(ndets):</span><br><span class="line">        i = order[_i]</span><br><span class="line">        <span class="keyword">if</span> suppressed[i] == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        keep.append(i)</span><br><span class="line">        ix1 = x1[i]</span><br><span class="line">        iy1 = y1[i]</span><br><span class="line">        ix2 = x2[i]</span><br><span class="line">        iy2 = y2[i]</span><br><span class="line">        iarea = areas[i]</span><br><span class="line">        <span class="keyword">for</span> _j <span class="keyword">in</span> range(_i + <span class="number">1</span>, ndets):</span><br><span class="line">            j = order[_j]</span><br><span class="line">            <span class="keyword">if</span> suppressed[j] == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            xx1 = max(ix1, x1[j])</span><br><span class="line">            yy1 = max(iy1, y1[j])</span><br><span class="line">            xx2 = min(ix2, x2[j])</span><br><span class="line">            yy2 = min(iy2, y2[j])</span><br><span class="line">            w = max(<span class="number">0.0</span>, xx2 - xx1 + <span class="number">1</span>)</span><br><span class="line">            h = max(<span class="number">0.0</span>, yy2 - yy1 + <span class="number">1</span>)</span><br><span class="line">            inter = w * h</span><br><span class="line">            ovr = inter / (iarea + areas[j] - inter)</span><br><span class="line">            <span class="keyword">if</span> ovr &gt;= thresh:</span><br><span class="line">                suppressed[j] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> keep</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cpu_soft_nms</span>(<span class="params">np.ndarray[float, ndim=<span class="number">2</span>] boxes, float sigma=<span class="number">0.5</span>, float Nt=<span class="number">0.3</span>, float threshold=<span class="number">0.001</span>, unsigned int method=<span class="number">0</span></span>):</span></span><br><span class="line">    cdef unsigned int N = boxes.shape[<span class="number">0</span>]</span><br><span class="line">    cdef float iw, ih, box_area</span><br><span class="line">    cdef float ua</span><br><span class="line">    cdef int pos = <span class="number">0</span></span><br><span class="line">    cdef float maxscore = <span class="number">0</span></span><br><span class="line">    cdef int maxpos = <span class="number">0</span></span><br><span class="line">    cdef float x1,x2,y1,y2,tx1,tx2,ty1,ty2,ts,area,weight,ov</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">        maxscore = boxes[i, <span class="number">4</span>]</span><br><span class="line">        maxpos = i</span><br><span class="line"></span><br><span class="line">        tx1 = boxes[i,<span class="number">0</span>]</span><br><span class="line">        ty1 = boxes[i,<span class="number">1</span>]</span><br><span class="line">        tx2 = boxes[i,<span class="number">2</span>]</span><br><span class="line">        ty2 = boxes[i,<span class="number">3</span>]</span><br><span class="line">        ts = boxes[i,<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">        pos = i + <span class="number">1</span></span><br><span class="line">	<span class="comment"># get max box</span></span><br><span class="line">        <span class="keyword">while</span> pos &lt; N:</span><br><span class="line">            <span class="keyword">if</span> maxscore &lt; boxes[pos, <span class="number">4</span>]:</span><br><span class="line">                maxscore = boxes[pos, <span class="number">4</span>]</span><br><span class="line">                maxpos = pos</span><br><span class="line">            pos = pos + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># add max box as a detection </span></span><br><span class="line">        boxes[i,<span class="number">0</span>] = boxes[maxpos,<span class="number">0</span>]</span><br><span class="line">        boxes[i,<span class="number">1</span>] = boxes[maxpos,<span class="number">1</span>]</span><br><span class="line">        boxes[i,<span class="number">2</span>] = boxes[maxpos,<span class="number">2</span>]</span><br><span class="line">        boxes[i,<span class="number">3</span>] = boxes[maxpos,<span class="number">3</span>]</span><br><span class="line">        boxes[i,<span class="number">4</span>] = boxes[maxpos,<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">	<span class="comment"># swap ith box with position of max box</span></span><br><span class="line">        boxes[maxpos,<span class="number">0</span>] = tx1</span><br><span class="line">        boxes[maxpos,<span class="number">1</span>] = ty1</span><br><span class="line">        boxes[maxpos,<span class="number">2</span>] = tx2</span><br><span class="line">        boxes[maxpos,<span class="number">3</span>] = ty2</span><br><span class="line">        boxes[maxpos,<span class="number">4</span>] = ts</span><br><span class="line"></span><br><span class="line">        tx1 = boxes[i,<span class="number">0</span>]</span><br><span class="line">        ty1 = boxes[i,<span class="number">1</span>]</span><br><span class="line">        tx2 = boxes[i,<span class="number">2</span>]</span><br><span class="line">        ty2 = boxes[i,<span class="number">3</span>]</span><br><span class="line">        ts = boxes[i,<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">        pos = i + <span class="number">1</span></span><br><span class="line">	<span class="comment"># NMS iterations, note that N changes if detection boxes fall below threshold</span></span><br><span class="line">        <span class="keyword">while</span> pos &lt; N:</span><br><span class="line">            x1 = boxes[pos, <span class="number">0</span>]</span><br><span class="line">            y1 = boxes[pos, <span class="number">1</span>]</span><br><span class="line">            x2 = boxes[pos, <span class="number">2</span>]</span><br><span class="line">            y2 = boxes[pos, <span class="number">3</span>]</span><br><span class="line">            s = boxes[pos, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">            area = (x2 - x1 + <span class="number">1</span>) * (y2 - y1 + <span class="number">1</span>)</span><br><span class="line">            iw = (min(tx2, x2) - max(tx1, x1) + <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> iw &gt; <span class="number">0</span>:</span><br><span class="line">                ih = (min(ty2, y2) - max(ty1, y1) + <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">if</span> ih &gt; <span class="number">0</span>:</span><br><span class="line">                    ua = float((tx2 - tx1 + <span class="number">1</span>) * (ty2 - ty1 + <span class="number">1</span>) + area - iw * ih)</span><br><span class="line">                    ov = iw * ih / ua <span class="comment">#iou between max box and detection box</span></span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> method == <span class="number">1</span>: <span class="comment"># linear</span></span><br><span class="line">                        <span class="keyword">if</span> ov &gt; Nt: </span><br><span class="line">                            weight = <span class="number">1</span> - ov</span><br><span class="line">                        <span class="keyword">else</span>:</span><br><span class="line">                            weight = <span class="number">1</span></span><br><span class="line">                    <span class="keyword">elif</span> method == <span class="number">2</span>: <span class="comment"># gaussian</span></span><br><span class="line">                        weight = np.exp(-(ov * ov)/sigma)</span><br><span class="line">                    <span class="keyword">else</span>: <span class="comment"># original NMS</span></span><br><span class="line">                        <span class="keyword">if</span> ov &gt; Nt: </span><br><span class="line">                            weight = <span class="number">0</span></span><br><span class="line">                        <span class="keyword">else</span>:</span><br><span class="line">                            weight = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                    boxes[pos, <span class="number">4</span>] = weight*boxes[pos, <span class="number">4</span>]</span><br><span class="line">		    </span><br><span class="line">		    <span class="comment"># if box score falls below threshold, discard the box by swapping with last box</span></span><br><span class="line">		    <span class="comment"># update N</span></span><br><span class="line">                    <span class="keyword">if</span> boxes[pos, <span class="number">4</span>] &lt; threshold:</span><br><span class="line">                        boxes[pos,<span class="number">0</span>] = boxes[N<span class="number">-1</span>, <span class="number">0</span>]</span><br><span class="line">                        boxes[pos,<span class="number">1</span>] = boxes[N<span class="number">-1</span>, <span class="number">1</span>]</span><br><span class="line">                        boxes[pos,<span class="number">2</span>] = boxes[N<span class="number">-1</span>, <span class="number">2</span>]</span><br><span class="line">                        boxes[pos,<span class="number">3</span>] = boxes[N<span class="number">-1</span>, <span class="number">3</span>]</span><br><span class="line">                        boxes[pos,<span class="number">4</span>] = boxes[N<span class="number">-1</span>, <span class="number">4</span>]</span><br><span class="line">                        N = N - <span class="number">1</span></span><br><span class="line">                        pos = pos - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            pos = pos + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    keep = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(N)]</span><br><span class="line">    <span class="keyword">return</span> keep</span><br></pre></td></tr></table></figure>
<p><strong>参考代码：</strong></p>
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/ruinmessi/RFBNet">https://github.com/ruinmessi/RFBNet</a>：RFBNet</p>
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/rbgirshick/py-faster-rcnn">https://github.com/rbgirshick/py-faster-rcnn</a>：学习一百遍都不为过的faster rcnn</p>
<p>NMS_demo.py：<a target="_blank" rel="noopener" href="https://github.com/humengdoudou/object_detection_mAP/blob/master/NMS_demo.py">https://github.com/humengdoudou/object_detection_mAP/blob/master/NMS_demo.py</a></p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/49481833">目标检测番外篇(3)_NMS</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/64423753">浅谈NMS的多种实现</a></li>
</ul>
<h2 id="NMS及其变体"><a href="#NMS及其变体" class="headerlink" title="NMS及其变体"></a>NMS及其变体</h2><ul>
<li><p>NMS</p>
</li>
<li><p>Soft-NMS</p>
</li>
<li>Softer-NMS</li>
<li>IoU-guided NMS</li>
<li>ConvNMS</li>
<li>Pure NMS</li>
<li>Yes-Net</li>
<li>LNMS</li>
<li>INMS</li>
<li>Polygon NMS</li>
<li>MNMS</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/70771042">Detection基础模块之（三）NMS及变体</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/28129034">NMS也能玩出花样来……</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/50126479">目标检测之非极大值抑制(NMS)各种变体</a></li>
</ul>
<h2 id="R-CNN-系列"><a href="#R-CNN-系列" class="headerlink" title="R-CNN 系列"></a>R-CNN 系列</h2><h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h3><ul>
<li>[ ] TODO</li>
</ul>
<h3 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h3><ul>
<li>[ ] TODO</li>
</ul>
<h3 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h3><ul>
<li>[ ] TODO</li>
</ul>
<h4 id="Faster-R-CNN-为什么用smooth-l1-loss，和l2有什么区别？"><a href="#Faster-R-CNN-为什么用smooth-l1-loss，和l2有什么区别？" class="headerlink" title="Faster R-CNN 为什么用smooth l1 loss，和l2有什么区别？"></a>Faster R-CNN 为什么用smooth l1 loss，和l2有什么区别？</h4><h2 id="SSD-算法"><a href="#SSD-算法" class="headerlink" title="SSD 算法"></a>SSD 算法</h2><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/65484308">SSD 论文原文完整翻译</a></li>
</ul>
<h2 id="YOLO系列（V1-V3）"><a href="#YOLO系列（V1-V3）" class="headerlink" title="YOLO系列（V1-V3）"></a>YOLO系列（V1-V3）</h2><h3 id="YOLOV1"><a href="#YOLOV1" class="headerlink" title="YOLOV1"></a>YOLOV1</h3><ul>
<li>[ ] TODO</li>
</ul>
<h3 id="YOLOv2算法"><a href="#YOLOv2算法" class="headerlink" title="YOLOv2算法"></a>YOLOv2算法</h3><ul>
<li>[ ] TODO</li>
</ul>
<h3 id="YOLOv3算法"><a href="#YOLOv3算法" class="headerlink" title="YOLOv3算法"></a>YOLOv3算法</h3><ul>
<li>[ ] TODO</li>
</ul>
<h3 id="YOLOv1-YOLOv2-YOLOv3的发展"><a href="#YOLOv1-YOLOv2-YOLOv3的发展" class="headerlink" title="YOLOv1 YOLOv2 YOLOv3的发展"></a>YOLOv1 YOLOv2 YOLOv3的发展</h3><ul>
<li>[ ] TODO</li>
</ul>
<h3 id="YOLOv2和YOLOv3的损失函数区别"><a href="#YOLOv2和YOLOv3的损失函数区别" class="headerlink" title="YOLOv2和YOLOv3的损失函数区别"></a>YOLOv2和YOLOv3的损失函数区别</h3><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/hancoder/article/details/87994678">YOLOv1，YOLOv2，YOLOv3解读</a></li>
</ul>
<h2 id="RetinaNet（Focal-loss）"><a href="#RetinaNet（Focal-loss）" class="headerlink" title="RetinaNet（Focal loss）"></a>RetinaNet（Focal loss）</h2><p>《Focal Loss for Dense Object Detection》</p>
<ul>
<li>arXiv：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1708.02002">https://arxiv.org/abs/1708.02002</a></li>
</ul>
<p>清华大学孔涛博士在知乎上这么写道：</p>
<p>目标的检测和定位中一个很困难的问题是，如何从数以万计的候选窗口中挑选包含目标物的物体。只有候选窗口足够多，才能保证模型的 Recall。</p>
<p>目前，目标检测框架主要有两种：</p>
<p>一种是 one-stage ，例如 YOLO、SSD 等，这一类方法速度很快，但识别精度没有 two-stage 的高，其中一个很重要的原因是，利用一个分类器很难既把负样本抑制掉，又把目标分类好。</p>
<p>另外一种目标检测框架是 two-stage ，以 Faster RCNN 为代表，这一类方法识别准确度和定位精度都很高，但存在着计算效率低，资源占用大的问题。</p>
<p>Focal Loss 从优化函数的角度上来解决这个问题，实验结果非常 solid，很赞的工作。</p>
<p>何恺明团队提出了用 Focal Loss 函数来训练。</p>
<p>因为，他在训练过程中发现，类别失衡是影响 one-stage 检测器准确度的主要原因。那么，如果能将“类别失衡”这个因素解决掉，one-stage 不就能达到比较高的识别精度了吗？</p>
<p>于是在研究中，何恺明团队采用 Focal Loss 函数来消除“类别失衡”这个主要障碍。</p>
<p>结果怎样呢？</p>
<p>为了评估该损失的有效性，该团队设计并训练了一个简单的密集目标检测器—RetinaNet。试验结果证明，当使用 Focal Loss 训练时，RetinaNet 不仅能赶上 one-stage 检测器的检测速度，而且还在准确度上超越了当前所有最先进的 two-stage 检测器。</p>
<p><strong>参考</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/63581984">如何评价Kaiming的Focal Loss for Dense Object Detection？</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/28442066">首发 | 何恺明团队提出 Focal Loss，目标检测精度高达39.1AP，打破现有记录</a></li>
</ul>
<h2 id="FPN-特征金字塔网络"><a href="#FPN-特征金字塔网络" class="headerlink" title="FPN 特征金字塔网络"></a>FPN 特征金字塔网络</h2><ul>
<li>[ ] TODO</li>
</ul>
<h2 id="Faster-R-CNN的RPN网络"><a href="#Faster-R-CNN的RPN网络" class="headerlink" title="Faster R-CNN的RPN网络"></a>Faster R-CNN的RPN网络</h2><p>RPN结构说明： </p>
<p>1) 从基础网络提取的第五卷积层特征进入RPN后分为两个分支，其中一个分支进行针对feature map（上图conv-5-3共有512个feature-map）的每一个位置预测共（9*4=36）个参数，其中9代表的是每一个位置预设的9种形状的anchor-box，4对应的是每一个anchor-box的预测值（该预测值表示的是预设anchor-box到ground-truth-box之间的变换参数），上图中指向rpn-bbox-pred层的箭头上面的数字36即是代表了上述的36个参数，所以rpn-bbox-pred层的feature-map数量是36，而每一张feature-map的形状（大小）实际上跟conv5-3一模一样的；</p>
<p>2) 另一分支预测该anchor-box所框定的区域属于前景和背景的概率（网上很对博客说的是，指代该点属于前景背景的概率，那样是不对的，不然怎么会有18个feature-map输出呢？否则2个就足够了），前景背景的真值给定是根据当前像素（anchor-box中心）是否在ground-truth-box内；</p>
<p>3) 上图RPN-data(python)运算框内所进行的操作是读取图像信息（原始宽高），groun-truth boxes的信息（bounding-box的位置，形状，类别）等，作好相应的转换，输入到下面的层当中。</p>
<p>4) 要注意的是RPN内部有两个loss层，一个是BBox的loss,该loss通过减小ground-truth-box与预测的anchor-box之间的差异来进行参数学习，从而使RPN网络中的权重能够学习到预测box的能力。实现细节是每一个位置的anchor-box与ground-truth里面的box进行比较，选择IOU最大的一个作为该anchor-box的真值，若没有，则将之class设为背景（概率值0，否则1），这样背景的anchor-box的损失函数中每个box乘以其class的概率后就不会对bbox的损失函数造成影响。另一个loss是class-loss,该处的loss是指代的前景背景并不是实际的框中物体类别，它的存在可以使得在最后生成roi时能快速过滤掉预测值是背景的box。也可实现bbox的预测函数不受影响，使得anchor-box能（专注于）正确的学习前景框的预测，正如前所述。所以，综合来讲，整个RPN的作用就是替代了以前的selective-search方法，因为网络内的运算都是可GPU加速的，所以一下子提升了ROI生成的速度。可以将RPN理解为一个预测前景背景，并将前景框定的一个网络，并进行单独的训练，实际上论文里面就有一个分阶段训练的训练策略，实际上就是这个原因。</p>
<p>5) 最后经过非极大值抑制，RPN层产生的输出是一系列的ROI-data，它通过ROI的相对映射关系，将conv5-3中的特征已经存入ROI-data中，以供后面的分类网使用。</p>
<p>另外两个loss层的说明：<br>也许你注意到了，最后还有两个loss层，这里的class-loss指代的不再是前景背景loss，而是真正的类别loss了，这个应该就很好理解了。而bbox-loss则是因为rpn提取的只是前景背景的预测，往往很粗糙，这里其实是通过ROI-pooling后加上两层全连接实现更精细的box修正（这里其实是我猜的）。<br>ROI-Pooing的作用是为了将不同大小的Roi映射（重采样）成统一的大小输入到全连接层去。</p>
<p>以上。</p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/mllearnertj/article/details/53709766">Faster-Rcnn中RPN（Region Proposal Network）的理解</a></li>
</ul>
<h2 id="ROI-Pooling、ROI-Align和ROI-Warping对比"><a href="#ROI-Pooling、ROI-Align和ROI-Warping对比" class="headerlink" title="ROI Pooling、ROI Align和ROI Warping对比"></a>ROI Pooling、ROI Align和ROI Warping对比</h2><ul>
<li>[ ] TODO</li>
</ul>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/lanyuxuan100/article/details/71124596">Mask-RCNN中的ROIAlign, ROIPooling及ROIWarp对比</a></li>
</ul>
<h2 id="DeepLab系列（V1-V3-）"><a href="#DeepLab系列（V1-V3-）" class="headerlink" title="DeepLab系列（V1-V3+）"></a>DeepLab系列（V1-V3+）</h2><ul>
<li>[ ] TODO</li>
</ul>
<h2 id="U-Net神经网络为什么会在医学图像分割表现好？"><a href="#U-Net神经网络为什么会在医学图像分割表现好？" class="headerlink" title="U-Net神经网络为什么会在医学图像分割表现好？"></a>U-Net神经网络为什么会在医学图像分割表现好？</h2><p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/269914775">U-Net神经网络为什么会在医学图像分割表现好？</a></li>
</ul>
<h2 id="Scene-Parsing和Semantic-Segmentation有什么不同"><a href="#Scene-Parsing和Semantic-Segmentation有什么不同" class="headerlink" title="Scene Parsing和Semantic Segmentation有什么不同?"></a>Scene Parsing和Semantic Segmentation有什么不同?</h2><p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/57726518">Scene Parsing和Semantic Segmentation有什么不同?</a></li>
</ul>
<h2 id="CenterNet"><a href="#CenterNet" class="headerlink" title="CenterNet"></a>CenterNet</h2><p>CornerNet介绍</p>
<h3 id="CornerPooling是怎么做的？"><a href="#CornerPooling是怎么做的？" class="headerlink" title="CornerPooling是怎么做的？"></a>CornerPooling是怎么做的？</h3><ul>
<li>[ ] TODO</li>
</ul>
<h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><ul>
<li>[ ] 目标检测方向</li>
<li>[ ] 图像分割方向</li>
<li>[ ] 目标跟踪方向</li>
<li>[ ] 人脸（检测&amp;识别&amp;关键点）</li>
<li>[ ] OCR方向</li>
<li>[ ] SLAM方向</li>
<li>[ ] 超分辨率</li>
<li>[ ] 医疗影响方向</li>
<li>[ ] Re-ID</li>
</ul>
<h1 id="深度学习Tricks"><a href="#深度学习Tricks" class="headerlink" title="深度学习Tricks"></a>深度学习Tricks</h1><h3 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h3><p>由于好的深度学习模型都需要很大的数据集，所以为了加强模型的效果，在原有的数据集上，采用<strong>数据增强</strong>的方法进行数据集“<strong>扩充</strong>”，在现在的深度学习模型训练中，data augmentation几乎是必备的。</p>
<ul>
<li><p>常见的augmentation的方法有<strong>随机旋转、 水平/垂直翻转、随即剪裁、颜色抖动等</strong>，需要注意的是，一般旋转和剪裁是同时进行的。</p>
<p>pytorch中的augmentation<a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1528683">参考资料</a></p>
</li>
<li><p><strong>Fancy PCA</strong></p>
</li>
</ul>
<h3 id="Pre-Processing"><a href="#Pre-Processing" class="headerlink" title="Pre-Processing"></a>Pre-Processing</h3><p>对一个传入网络的巨大训练集来说，首先需要对数据进行预处理。</p>
<ul>
<li>zero-center归一化</li>
</ul>
<p>这种归一化对于不同的输入特征有不同的尺度(或单位)时，才有意义应用这种预处理，如果是图像(像素范围为0-255)则不用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X -= np.mean(X, axis = <span class="number">0</span>) <span class="comment"># zero-center</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X /= np.std(X, axis = <span class="number">0</span>) <span class="comment"># normalize</span></span><br></pre></td></tr></table></figure>
<ul>
<li>PCA Whitening</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X -= np.mean(X, axis = <span class="number">0</span>) <span class="comment"># zero-center</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cov = np.dot(X.T, X) / X.shape[<span class="number">0</span>] <span class="comment"># compute the covariance matrix</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>U,S,V = np.linalg.svd(cov) <span class="comment"># compute the SVD factorization of the data covariance matrix</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Xrot = np.dot(X, U) <span class="comment"># decorrelate the data</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Xwhite = Xrot / np.sqrt(S + <span class="number">1e-5</span>) <span class="comment"># divide by the eigenvalues (which are square roots of the singular values)</span></span><br></pre></td></tr></table></figure>
<p>这个方法的缺点是会增加数据的噪声，因为使输入具有相同的大小会延伸特征的维度。</p>
<h3 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h3><ul>
<li>all zero initialization</li>
</ul>
<p>如果神经元的权重初始化为相同，则它们之间就没有不对称的来源。</p>
<ul>
<li>small random initialization</li>
</ul>
<p>为了避免all zero带来的问题，并且将权重值初始化为靠近0</p>
<p>满足$weight \in 0.01\times N(0,1)$的高斯分布。</p>
<ul>
<li>Calibrating the variance</li>
</ul>
<p>上面的random initialization会随着输入的数量增多而方差变大，为了解决这个问题，可以将初始化权重除以输入神经元的数量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>w = np.random.randn(n) / sqrt(n) <span class="comment"># calibrating the variances with 1/sqrt(n)</span></span><br></pre></td></tr></table></figure>
<p>通过校准神经元的方差进行的先前初始化未考虑ReLU。专门针对ReLUs进行了初始化，得出结论，网络中神经元的方差应为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>w = np.random.randn(n) * sqrt(<span class="number">2.0</span>/n) <span class="comment"># current recommendation</span></span><br></pre></td></tr></table></figure>
<h4 id="During-Training"><a href="#During-Training" class="headerlink" title="During Training"></a>During Training</h4><ul>
<li><strong>Filters and pooling size</strong></li>
</ul>
<p>关于卷积核的尺寸和池化尺寸，卷积核一般采用3x3，stride=1的尺寸，可以保证特征图的输入大小不变，而池化采用的池化核尺寸为2x2。</p>
<ul>
<li><strong>Learning Rate</strong></li>
</ul>
<p>一般按照batch size大小来确定LR，从0.1开始，然后/2，/5进行递减，并且在固定的epoch进行decay</p>
<p><strong>关于batch size的大小与LR</strong></p>
<p>使用更大的batch size会导致减缓训练进度。对于凸问题，收敛速度会随着batch size的增加而降低。也就是说，在相同的epoch下，使用更大的batch size可能会导致验证集accuracy更低。</p>
<p>所以有一些trick来解决batch size增大的问题</p>
<ol>
<li><strong>Linear scaling learning rate</strong></li>
</ol>
<p>当我们选择初始学习率为0.1，batch size为256时，那么当我们将batch size增大至b时，就需要将初始学习率增加曾0.1×b/256</p>
<ol>
<li><strong>Learning rate warmup</strong></li>
</ol>
<p>选择若干个epoch进行warmup逐渐将学习率增加到指定的初始学习率</p>
<ol>
<li><strong>Zero $\gamma $</strong></li>
</ol>
<p>将batch normalization的两个参数都初始化为0</p>
<ol>
<li><strong>No bias decay</strong></li>
</ol>
<p>为了避免过拟合，对于权重weight和偏差bias，我们通常会使用weight decay。但在这里，仅对weight使用decay，而不对bias使用decay。</p>
<ul>
<li><strong>Fine-tune on pre-trained models</strong></li>
</ul>
<p>一个很好的适应新的数据集的方法是利用新的数据集在预训练的模型上进行fine-tune，需要依据新数据集的大小和与预训练数据集的相似程度来进行微调。</p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201221003245.png" alt></p>
<h3 id="Activation-Functions"><a href="#Activation-Functions" class="headerlink" title="Activation Functions"></a>Activation Functions</h3><h3 id="Regularizations"><a href="#Regularizations" class="headerlink" title="Regularizations"></a>Regularizations</h3><h3 id="Insight-from-Figures"><a href="#Insight-from-Figures" class="headerlink" title="Insight from Figures"></a>Insight from Figures</h3><h3 id="Ensemble"><a href="#Ensemble" class="headerlink" title="Ensemble"></a>Ensemble</h3><p>可以将多个模型的训练结果进行融合，主要应用在比赛中</p>
<p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201221004521.png" alt></p>
<h3 id="Mixup"><a href="#Mixup" class="headerlink" title="Mixup"></a>Mixup</h3><p>在mixup中，每次随机采样两个样本 $(x_i,y_i)$和 $(x_2,y_2)$，然后通过加权线性插值生成新的样本进行训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i,(images,target) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">    <span class="comment"># 1.input output</span></span><br><span class="line">    images = images.cuda(non_blocking=<span class="literal">True</span>)</span><br><span class="line">    target = torch.from_numpy(np.array(target)).float().cuda(non_blocking=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.mixup</span></span><br><span class="line">    alpha=config.alpha</span><br><span class="line">    lam = np.random.beta(alpha,alpha)</span><br><span class="line">    index = torch.randperm(images.size(<span class="number">0</span>)).cuda()</span><br><span class="line">    inputs = lam*images + (<span class="number">1</span>-lam)*images[index,:]</span><br><span class="line">    targets_a, targets_b = target, target[index]</span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    loss = lam * criterion(outputs, targets_a) + (<span class="number">1</span> - lam) * criterion(outputs, targets_b)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3.backward</span></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># reset gradient</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()        <span class="comment"># update parameters of net</span></span><br></pre></td></tr></table></figure>
<p>关于<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/149964631">beta分布</a></p>
<h3 id="label-smoothing"><a href="#label-smoothing" class="headerlink" title="label smoothing"></a>label smoothing</h3><p><img src="https://gitee.com/browallia/tuchuang/raw/master/img/20201221152951.png" alt></p>
<ol>
<li>有正则化的效果</li>
<li>Label Smoothing起到的作用实际上是抑制了feature norm，此时softmax prob不能到达$(1-\alpha)$，loss曲面上不再存在平缓区域，处处都有较大的梯度指向各个类中心，所以特征会更加聚拢。</li>
</ol>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">BROWALLIA</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://www.browallia.top/2020/10/19/DL%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9/">https://www.browallia.top/2020/10/19/DL%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.browallia.top" target="_blank">Viva La Vida</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87/">面试准备</a></div><div class="post_share"><div class="social-share" data-image="https://gitee.com/browallia/tuchuang/raw/master/img/cover.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/zfb.png" target="_blank"><img class="post-qr-code-img" src="/img/zfb.png" alt="Alipay"/></a><div class="post-qr-code-desc">Alipay</div></li></ul></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/10/29/Knowledge-Distillation/"><img class="prev-cover" src="https://gitee.com/browallia/tuchuang/raw/master/img/KD.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Knowledge Distillation</div></div></a></div><div class="next-post pull-right"><a href="/2020/06/03/DETR-note/"><img class="next-cover" src="https://gitee.com/browallia/tuchuang/raw/master/img/DETR.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">DETR-note</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></article></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2021 By BROWALLIA</div><div class="footer_custom_text">早安，打工人！</div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  var script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const initData = {
      el: '#vcomment',
      appId: 'N9FvFdatw3iiNqYayrrAfRhJ-gzGzoHsz',
      appKey: 'oE1wULzwlDDcdjA0kAbxd3k0',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'en',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
    }

    if (true) { 
      initData.requiredFields= ('nick,mail'.split(','))
    }

    const valine = new Valine(initData)
  }

  if (typeof Valine === 'function') initValine() 
  else $.getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js', initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.querySelector('#vcomment'),loadValine)
  else setTimeout(() => loadValine(), 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>