<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>DETR-note | Viva La Vida</title><meta name="keywords" content="目标检测"><meta name="author" content="BROWALLIA"><meta name="copyright" content="BROWALLIA"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="Transformer和AttentionTransformerTransformer本质上是一个Encoder-Decoder的结构  在paper中，encoder和decoder都由6个block组成，编码器的输出作为解码器的输入。 encoders每个block都是独立的单元，不会share weights。每个block又由以下的两部分组成(self-attention和FFN)  在">
<meta property="og:type" content="article">
<meta property="og:title" content="DETR-note">
<meta property="og:url" content="https://www.browallia.top/2020/06/03/DETR-note/index.html">
<meta property="og:site_name" content="Viva La Vida">
<meta property="og:description" content="Transformer和AttentionTransformerTransformer本质上是一个Encoder-Decoder的结构  在paper中，encoder和decoder都由6个block组成，编码器的输出作为解码器的输入。 encoders每个block都是独立的单元，不会share weights。每个block又由以下的两部分组成(self-attention和FFN)  在">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gitee.com/browallia/tuchuang/raw/master/img/DETR.jpg">
<meta property="article:published_time" content="2020-06-03T13:30:48.000Z">
<meta property="article:modified_time" content="2020-12-13T17:17:34.967Z">
<meta property="article:author" content="BROWALLIA">
<meta property="article:tag" content="目标检测">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/browallia/tuchuang/raw/master/img/DETR.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://www.browallia.top/2020/06/03/DETR-note/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin="crossorigin"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="content"/><meta name="baidu-site-verification" content="content"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6df37eb9bea6fc6acfb352324998b672";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-D99NED65DC"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-D99NED65DC');
</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.2.0',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-12-14 01:17:34'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {
  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }

  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Viva La Vida" type="application/atom+xml">
</head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/dbs.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">27</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">16</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> POI</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> Me</span></a></div></div></div></div><div id="body-wrap"><div id="web_bg"></div><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer%E5%92%8CAttention"><span class="toc-number">1.</span> <span class="toc-text">Transformer和Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer"><span class="toc-number">1.0.1.</span> <span class="toc-text">Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">1.0.1.1.</span> <span class="toc-text">编码器</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#self-attention"><span class="toc-number">1.0.1.1.1.</span> <span class="toc-text">self-attention</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#multi-head%E6%9C%BA%E5%88%B6"><span class="toc-number">1.0.1.1.2.</span> <span class="toc-text">multi-head机制</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Position-Encoding"><span class="toc-number">1.0.1.1.3.</span> <span class="toc-text">Position Encoding</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Residuals"><span class="toc-number">1.0.1.1.4.</span> <span class="toc-text">Residuals</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="toc-number">1.0.1.2.</span> <span class="toc-text">解码器</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DETR"><span class="toc-number">1.1.</span> <span class="toc-text">DETR</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E5%88%86%E5%9B%BE%E5%8C%B9%E9%85%8D%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-bipartite-matiching-loss"><span class="toc-number">1.1.1.</span> <span class="toc-text">二分图匹配损失函数(bipartite matiching loss)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DETR%E7%BB%93%E6%9E%84"><span class="toc-number">1.1.2.</span> <span class="toc-text">DETR结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81"><span class="toc-number">1.1.3.</span> <span class="toc-text">实现代码</span></a></li></ol></li></ol></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://gitee.com/browallia/tuchuang/raw/master/img/DETR.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Viva La Vida</a></span><span id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> POI</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> Me</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">DETR-note</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-06-03T13:30:48.000Z" title="发表于 2020-06-03 21:30:48">2020-06-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2020-12-13T17:17:34.967Z" title="更新于 2020-12-14 01:17:34">2020-12-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">1.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>4分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><a id="more"></a>
<h1 id="Transformer和Attention"><a href="#Transformer和Attention" class="headerlink" title="Transformer和Attention"></a>Transformer和Attention</h1><h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><p>Transformer本质上是一个Encoder-Decoder的结构</p>
<p><img src="/2020/06/03/DETR-note/1.png" style="zoom: 67%;"></p>
<p>在paper中，encoder和decoder都由6个block组成，编码器的输出作为解码器的输入。</p>
<p>encoders每个block都是独立的单元，不会share weights。每个block又由以下的两部分组成(self-attention和FFN)</p>
<p><img src="/2020/06/03/DETR-note/2.png" style="zoom:67%;"></p>
<p>在decoder部分每个block也和encoder相似，只是在两层之间加了一个attention层(类似于seq2seq的attention机制，可以让decoder更加关注input相关的部分)。</p>
<h4 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h4><p><img src="/2020/06/03/DETR-note/3.png" style="zoom:67%;"></p>
<p>编码器接受单词的词嵌入列表作为输入(列表中每个向量为512-d)。单词在通过self-attention层时会依赖单词之间的关系，但是在通过Feed Forward时没有这些依赖关系，可以并行进行。</p>
<h5 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a>self-attention</h5><p>类似于RNN中的隐藏层，self-attention的作用是将其他单词的信息融入到正在处理的单词中。</p>
<p>PS： ”<code>The animal didn&#39;t cross the street because it was too tired</code>” 在处理it时会将前文the animal等信息加到it中。</p>
<p><strong>三个向量</strong></p>
<p>self-attention的<strong>第一步</strong>就是从输入的词嵌入向量列表生成三个向量(64-d)</p>
<p><img src="/2020/06/03/DETR-note/4.png" style="zoom:67%;"></p>
<hr>
<p><strong>query vector</strong>：与其他单词的key vector相乘计算得分<br><strong>key vector</strong>：与其他单词query vector相乘代表对其他单词在语义上的影响<br><strong>value vector</strong>：</p>
<p> <strong>第二步</strong>是对输入的单词进行得分计算，得分决定了这个词在句子中有多重视其他部分。</p>
<p>得分的计算是由其他单词的key vector和该单词的query vector进行点积计算。</p>
<p><strong>第三步</strong>是对每个得分除以$\sqrt{d_k}$然后进行softmax可以得出每个位置的单词对该位置的贡献。</p>
<p><strong>第四步</strong>是将每个单词的value vector与求出来的softmax权重相乘，可以关注语义上联系很强的单词。</p>
<p><strong>第五步</strong>是将所有带权重的value vector进行求和，当作self-attention的输出。</p>
<p><img src="/2020/06/03/DETR-note/5.png" style="zoom:50%;"></p>
<p>在实际的运算中是以矩阵来进行运算的。</p>
<p><img src="/2020/06/03/DETR-note/6.png" style="zoom:50%;"></p>
<h5 id="multi-head机制"><a href="#multi-head机制" class="headerlink" title="multi-head机制"></a>multi-head机制</h5><ol>
<li>多头注意力扩展了模型专注于不同位置的能力</li>
<li>可以将每个词嵌入投射到不同的子空间</li>
</ol>
<p>n个注意力头会产生n个z输出，通过$W^0$与拼接好的输出进行相乘得到最后融合多个注意力头的Z</p>
<p><img src="/2020/06/03/DETR-note/7.png" style="zoom:50%;"></p>
<p>self-attention的总体计算过程</p>
<p><img src="/2020/06/03/DETR-note/8.png" style="zoom:67%;"></p>
<h5 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h5><p>在词嵌入中，将每个单词的位置编码(向量)加入到嵌入向量中，描述单词的输入顺序。</p>
<p><img src="/2020/06/03/DETR-note/9.png" style="zoom:67%;"><img src="/2020/06/03/DETR-note/10.png" alt></p>
<p>(Positional Embedding左半部分通过正弦函数求出，右半部分通过余弦函数求出)</p>
<h5 id="Residuals"><a href="#Residuals" class="headerlink" title="Residuals"></a>Residuals</h5><p><img src="/2020/06/03/DETR-note/10.png" style="zoom:67%;"></p>
<h4 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h4><p> <img src="https://jalammar.github.io/images/t/transformer_decoding_2.gif" alt="img"> </p>
<p>编码器通过处理输入序列开启工作。顶端编码器的输出之后会变转化为一个包含向量K（键向量）和V（值向量）的注意力向量集 。这些向量将被每个解码器用于自身的“编码-解码注意力层”，而这些层可以帮助解码器关注输入序列哪些位置合适 。</p>
<p> 这个“编码-解码注意力层”工作方式基本就像多头自注意力层一样，只不过它是通过在它下面的层来创造查询矩阵，并且从编码器的输出中取得键/值矩阵。 </p>
<p><strong>整体结构</strong></p>
<p><img src="/2020/06/03/DETR-note/11.png" style="zoom:80%;"></p>
<h2 id="DETR"><a href="#DETR" class="headerlink" title="DETR"></a>DETR</h2><h3 id="二分图匹配损失函数-bipartite-matiching-loss"><a href="#二分图匹配损失函数-bipartite-matiching-loss" class="headerlink" title="二分图匹配损失函数(bipartite matiching loss)"></a>二分图匹配损失函数(bipartite matiching loss)</h3><p>图片经过CNN提取特征输入Transformer模型，输出N个固定prediction box(class, bbox)格式。GT的bbox也以(class,bbox)的形式存在，并且补齐N个$(\emptyset,*)$ bbox。</p>
<p>通过最佳匹配算法(匈牙利算法)来确定GT的最佳匹配框，然后可以计算损失函数。</p>
<p>将输出的bbox与GT的bbox对应起来，寻找一个最佳的对应关系，使得loss最小。(这样做的好处是可以将多个输出相同object的框选择一个最优的输出bbox与GT标注框对应，迫使模型学习输出更多不同object的bbox，并且匈牙利算法会对预测的object数大于GT的object数进行惩罚)</p>
<h3 id="DETR结构"><a href="#DETR结构" class="headerlink" title="DETR结构"></a>DETR结构</h3><p><img src="/2020/06/03/DETR-note/12.png" alt></p>
<ol>
<li><p>将图像经过CNN提取的特征与object的位置positional encoding结合送入transformer encoder中。</p>
<p>(由于transformer只接受序列化输入，所以将(C,H,W) flatten 得到(C, HXW)的序列化特征)</p>
</li>
<li><p>将encoder的输出特征传入decoder(相当于一个特征映射的过程，encoder能够学习更多的其他位置的特征)，decoder的输入是object queries，object queries首先是n个随机变量，经过decoder融合encoder输出的图像信息得出n个合适的bbox输出（n个object queries可以当作n个不同的人从n个不同的角度对图像进行观测，<strong>注意</strong>图像不同的地方，<strong>是需要学习的</strong>）</p>
</li>
</ol>
<p><strong>具体结构</strong></p>
<p><img src="/2020/06/03/DETR-note/13.png" style="zoom:80%;"></p>
<h3 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h3><p><strong>pytorch</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> resnet50</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DETR</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_classes, hidden_dim, nheads,</span></span></span><br><span class="line"><span class="function"><span class="params">num_encoder_layers, num_decoder_layers</span>):</span></span><br><span class="line">	super().__init__()</span><br><span class="line"><span class="comment"># We take only convolutional layers from ResNet-50 model</span></span><br><span class="line">	self.backbone = nn.Sequential(*list(resnet50(pretrained=<span class="literal">True</span>).children())[:<span class="number">-2</span>])</span><br><span class="line">	self.conv = nn.Conv2d(<span class="number">2048</span>, hidden_dim, <span class="number">1</span>)</span><br><span class="line">    self.transformer = nn.Transformer(hidden_dim, nheads,</span><br><span class="line">	num_encoder_layers, num_decoder_layers)</span><br><span class="line">	self.linear_class = nn.Linear(hidden_dim, num_classes + <span class="number">1</span>)</span><br><span class="line">	self.linear_bbox = nn.Linear(hidden_dim, <span class="number">4</span>)</span><br><span class="line">	self.query_pos = nn.Parameter(torch.rand(<span class="number">100</span>, hidden_dim))</span><br><span class="line">	self.row_embed = nn.Parameter(torch.rand(<span class="number">50</span>, hidden_dim // <span class="number">2</span>))</span><br><span class="line">	self.col_embed = nn.Parameter(torch.rand(<span class="number">50</span>, hidden_dim // <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">		x = self.backbone(inputs)</span><br><span class="line">		h = self.conv(x)</span><br><span class="line">		H, W = h.shape[<span class="number">-2</span>:]</span><br><span class="line">		pos = torch.cat([</span><br><span class="line">		self.col_embed[:W].unsqueeze(<span class="number">0</span>).repeat(H, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">		self.row_embed[:H].unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, W, <span class="number">1</span>),</span><br><span class="line">			], dim=<span class="number">-1</span>).flatten(<span class="number">0</span>, <span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">		h = self.transformer(pos + h.flatten(<span class="number">2</span>).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>),</span><br><span class="line">		self.query_pos.unsqueeze(<span class="number">1</span>))</span><br><span class="line">		<span class="keyword">return</span> self.linear_class(h), self.linear_bbox(h).sigmoid()</span><br><span class="line"></span><br><span class="line">detr = DETR(num_classes=<span class="number">91</span>, hidden_dim=<span class="number">256</span>, nheads=<span class="number">8</span>, num_encoder_layers=<span class="number">6</span>, num_decoder_layers=<span class="number">6</span>)</span><br><span class="line">detr.eval()</span><br><span class="line">inputs = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">800</span>, <span class="number">1200</span>)</span><br><span class="line">logits, bboxes = detr(inputs)</span><br></pre></td></tr></table></figure>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">BROWALLIA</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://www.browallia.top/2020/06/03/DETR-note/">https://www.browallia.top/2020/06/03/DETR-note/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.browallia.top" target="_blank">Viva La Vida</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></div><div class="post_share"><div class="social-share" data-image="https://gitee.com/browallia/tuchuang/raw/master/img/DETR.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/zfb.png" target="_blank"><img class="post-qr-code-img" src="/img/zfb.png" alt="Alipay"/></a><div class="post-qr-code-desc">Alipay</div></li></ul></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/10/19/DL%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9/"><img class="prev-cover" src="https://gitee.com/browallia/tuchuang/raw/master/img/cover.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">DL面试知识点</div></div></a></div><div class="next-post pull-right"><a href="/2020/05/28/PolarMask-note/"><img class="next-cover" src="/img/post1.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">PolarMask-note</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/05/06/FPN-note/" title="FPN-note"><img class="cover" src="/img/post1.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-05-06</div><div class="title">FPN-note</div></div></a></div><div><a href="/2020/04/11/Faster-RCNN-笔记/" title="Faster_RCNN 笔记"><img class="cover" src="/img/post1.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-04-11</div><div class="title">Faster_RCNN 笔记</div></div></a></div><div><a href="/2020/05/12/Retinanet-focal-loss/" title="Retinanet&focal-loss"><img class="cover" src="/img/post1.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-05-12</div><div class="title">Retinanet&focal-loss</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></article></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2021 By BROWALLIA</div><div class="footer_custom_text">早安，打工人！</div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  var script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const initData = {
      el: '#vcomment',
      appId: 'N9FvFdatw3iiNqYayrrAfRhJ-gzGzoHsz',
      appKey: 'oE1wULzwlDDcdjA0kAbxd3k0',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'en',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
    }

    if (true) { 
      initData.requiredFields= ('nick,mail'.split(','))
    }

    const valine = new Valine(initData)
  }

  if (typeof Valine === 'function') initValine() 
  else $.getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js', initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.querySelector('#vcomment'),loadValine)
  else setTimeout(() => loadValine(), 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>